{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import actorcritic as ac\n",
    "import run_experiment as expt\n",
    "import environment as gw\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'load_model': False, 'load_dir': '', 'architecture': 'B', 'input_dims': 400, 'action_dims': 4, 'hidden_types': ['linear', 'linear', 'linear', 'linear', 'linear', 'linear'], 'hidden_dims': [1000, 2000, 2000, 2000, 1000, 1000], 'freeze_w': False, 'rfsize': 5, 'gamma': 0.98, 'eta': 0.0005, 'use_EC': False}\n"
     ]
    }
   ],
   "source": [
    "ep = expt.basic_env_params()\n",
    "env = gw.GridWorld(rows=ep.rows, cols=ep.columns, env_type=ep.env_type,\n",
    "                           rewards={ep.reward_location: ep.reward_mag},\n",
    "                           step_penalization=ep.penalty,\n",
    "                           rho=ep.rho,\n",
    "                           actionlist=ep.actionlist,\n",
    "                           rewarded_action=ep.rewarded_action)\n",
    "agent_parameters = expt.basic_agent_params(env)\n",
    "agent_parameters.input_dims = env.nstates\n",
    "agent_parameters.hidden_types = ['linear', 'linear', 'linear', 'linear', 'linear', 'linear']\n",
    "agent_parameters.hidden_dims = [1000, 2000, 2000, 2000, 1000, 1000]\n",
    "\n",
    "print(agent_parameters.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ac.ActorCritic(agent_parameters.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "valueshape torch.Size([1, 1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1]) torch.Size([1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annik/Dropbox/LINC Lab Documents/Code/MEMRL/episodic_control/actorcritic/__init__.py:281: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1, 1, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_losses.append(F.smooth_l1_loss(value, return_bs))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0321592dade6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_SR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mp_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsi_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mp_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/LINC Lab Documents/Code/MEMRL/episodic_control/actorcritic/__init__.py\u001b[0m in \u001b[0;36mfinish_trial\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_SR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                         \u001b[0mp_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsi_loss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m                         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpsi_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = 100\n",
    "NUM_EVENTS = 20\n",
    "for trial in range(NUM_TRIALS):\n",
    "    # reset environment, choose new starting location for agent\n",
    "    env.resetEnvironment()\n",
    "    # clear hidden layer cache if using lstm or gru cells\n",
    "    agent.reinit_hid()\n",
    "    reward_sum = 0\n",
    "    \n",
    "    for event in range(NUM_EVENTS):\n",
    "        # get state observation\n",
    "        observation = torch.Tensor(np.expand_dims(env.get_observation(), axis=0))\n",
    "        # pass observation through network\n",
    "        if agent.use_SR:\n",
    "            policy_, value_, phi_, psi_ = agent(observation)\n",
    "            agent.saved_phi.append(phi_)\n",
    "            agent.saved_psi.append(psi_)\n",
    "        else:\n",
    "            policy_, value_, psi_ = agent(observation)\n",
    "\n",
    "        # linear activity\n",
    "        choice = agent.select_action(policy_, value_)\n",
    "        \n",
    "        # select action from policy\n",
    "        action = env.action_list[choice][0]\n",
    "\n",
    "        # take a step in the environment\n",
    "        s_1d, reward, isdone = env.move(action)\n",
    "\n",
    "        agent.saved_rewards.append(reward)\n",
    "        reward_sum += reward\n",
    "\n",
    "        if isdone:\n",
    "            if agent.use_SR:\n",
    "                agent.saved_phi.append(phi_)\n",
    "                agent.saved_psi.append(psi_)\n",
    "            encountered_reward = True\n",
    "            break\n",
    "\n",
    "    if agent.use_SR:\n",
    "        p_loss, v_loss, psi_loss = agent.finish_trial()\n",
    "    else:\n",
    "        p_loss, v_loss = agent.finish_trial()\n",
    "\n",
    "    if trial == 0 or trial%print_freq==0 or trial == NUM_TRIALS - 1:\n",
    "        print(f\"{trial}: {self.reward_sum} ({time.time() - t}s / Running Av: {running_rwdavg}\")\n",
    "        t = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
