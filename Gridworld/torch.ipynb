{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import random\n",
    "\n",
    "import torch \n",
    "from torch import autograd, optim, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "from utils import *\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "saveplots = False\n",
    "\n",
    "if mazetype=='none':\n",
    "    pathvar = './valplots/grid/{}/'.format(obs_rho)\n",
    "    figpath = './figures/grid/{}/'.format(obs_rho)\n",
    "else: \n",
    "    pathvar = './valplots/{}/'.format(mazetype)\n",
    "    figpath = './figures/{}/'.format(mazetype)\n",
    "\n",
    "if not os.path.exists(figpath):\n",
    "    os.makedirs(figpath)\n",
    "if saveplots:\n",
    "    if not os.path.exists(pathvar):\n",
    "        os.makedirs(pathvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Agent \n",
    "class AC_Net(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super(AC_Net, self).__init__()\n",
    "        # dims is a list of dimensions of each layer [dimension_of_input, dim_of_hiddenlayer1, ... dim_of_hiddenlayerN, dim_of_output]\n",
    "        self.layers = dims\n",
    "        if len(dims)>2: \n",
    "            self.hidden = []\n",
    "            for i in range(len(dims)-2):\n",
    "                self.hidden.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            self.h_layers = nn.ModuleList(self.hidden)\n",
    "            \n",
    "        self.p_in = nn.Linear(dims[-2],dims[-1])\n",
    "        self.v_in = nn.Linear(dims[-2],1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(self.layers)>2:\n",
    "            for i in range(len(self.hidden)):\n",
    "                x = F.sigmoid(self.hidden[i](x))\n",
    "        pol = F.softmax(self.p_in(x))\n",
    "        val = self.v_in(x)\n",
    "\n",
    "        return pol, val\n",
    "    \n",
    "    \n",
    "def finish_episode(episode_data):\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    value_loss = 0\n",
    "    rewards = []\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + man_gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    for (action, value), r in zip(saved_actions, rewards):\n",
    "        reward = r - value.data[0,0]\n",
    "        action.reinforce(reward)\n",
    "        value_loss += F.smooth_l1_loss(value, Variable(torch.Tensor([r])))\n",
    "    optimizer.zero_grad()\n",
    "    final_nodes = [value_loss] + list(map(lambda p: p.action, saved_actions))\n",
    "    gradients = [torch.ones(1)] + [None] * len(saved_actions)\n",
    "    autograd.backward(final_nodes, gradients)\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAFDCAYAAABlQfaWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1MSgSlITgypJTQyqJDUxqJLU5P8AbiyRL0ZSjLQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81cd07fa50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#========================\n",
    "# Environment Parameters\n",
    "#======================== \n",
    "height = 15\n",
    "width = 15\n",
    "\n",
    "mazetype = 'none'\n",
    "#obstacle density\n",
    "obs_rho = 0\n",
    "\n",
    "#place cells\n",
    "place_cells = 700\n",
    "#place cell full width half max (must be <1)\n",
    "fwhm = .2\n",
    "\n",
    "#make environment\n",
    "maze = gridworld([height, width],rho=obs_rho,num_pc=place_cells, pc_fwhm=fwhm, maze_type=mazetype)\n",
    "\n",
    "make_env_plots(maze,1,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=====================\n",
    "# Training Parameters\n",
    "#=====================\n",
    "#trial parameters\n",
    "NUM_TRIALS = 100000\n",
    "\n",
    "NUM_EVENTS = 300\n",
    "\n",
    "discount_factor = 0.98\n",
    "port_shift = 'none'\n",
    "\n",
    "#agent parameters\n",
    "dims = [len(maze.net_state[0]),30, len(maze.actionlist)]\n",
    "model = AC_Net(dims)\n",
    "\n",
    "#learning parameters\n",
    "\n",
    "eta = 1e-2 #gradient descent learning rate\n",
    "opt = optim.SGD(model.parameters(), lr = eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Single run of NUM_TRIALS each with NUM_EVENTS\n",
    "print_freq = 1./10\n",
    "\n",
    "#steps_rwd = []\n",
    "# trialtime = [] #track how many steps into trial before done == True\n",
    "\n",
    "total_reward = [] #track total reward achieved in trial \n",
    "total_loss = [[],[]]\n",
    "\n",
    "# data frames for value and policy maps\n",
    "val_maps = []\n",
    "p_field = np.zeros((height, width), dtype=[('action_taken', 'i4'),('taken_prob', 'f4'), ('likely_action', 'i4'),('likely_prob', 'f4'), ('timestep', 'i4')])\n",
    "\n",
    "# record current time before beginning of trial\n",
    "print strftime(\"%a, %d %b %Y %H:%M:%S +0000\", time.localtime())\n",
    "blocktime = time.time()\n",
    "\n",
    "# loop over total number of trials \n",
    "for i in xrange(NUM_TRIALS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        _ep_buffer = []\n",
    "        reward_sum = 0\n",
    "\n",
    "        # reset the environment, initialize agent position and reset value map\n",
    "        maze.start_trial()\n",
    "        maze.value_map = maze.init_value_map\n",
    "        \n",
    "        # get inital state\n",
    "        state = autograd.Variable(torch.FloatTensor(maze.net_state))\n",
    "        \n",
    "        for j in xrange(NUM_EVENTS): \n",
    "            # get policy and value estimate in order to select next action\n",
    "            policy_, value_ = model(state)\n",
    "            \n",
    "            policy = policy_.data[0]\n",
    "            val = value_.data[0]\n",
    "            \n",
    "            maze.value_map[maze.cur_state[1]][maze.cur_state[0]] = val\n",
    "\n",
    "            # select action\n",
    "            choice = np.random.choice(np.arange(len(policy)), 1, p=list(policy))[0]\n",
    "            action = maze.actionlist[choice]\n",
    "            choice_prob = policy[choice]\n",
    "\n",
    "            p_field[maze.cur_state[1]][maze.cur_state[0]] = (choice, list(policy)[choice], list(policy).index(max(list(policy))), max(list(policy)), i)\n",
    "            \n",
    "            # get new state of the environment and reward from action \n",
    "            if j < NUM_EVENTS:\n",
    "                next_state = maze.move(action)\n",
    "\n",
    "            rwd = maze.rwd\n",
    "\n",
    "            # store buffer of agents experiences so that later we can calulate returns/etc. backwards through time\n",
    "            _ep_buffer.append([state,choice,rwd,next_state, val, choice_prob])\n",
    "\n",
    "            # update state\n",
    "            state = autograd.Variable(torch.FloatTensor(next_state))\n",
    "\n",
    "            reward_sum += rwd\n",
    "            \n",
    "            #if maze.done == True: \n",
    "            #    trialtime.append(j)\n",
    "            #    break\n",
    "            #elif j == NUM_EVENTS-1:\n",
    "            #    trialtime.append(NUM_EVENTS)\n",
    "        # make data storage useable type \n",
    "        _ep_buffer = np.array(_ep_buffer)\n",
    "        # compute returns\n",
    "        _returns = torch.FloatTensor(discount_rwds(_ep_buffer[:,2], gamma=discount_factor))\n",
    "        _values = torch.FloatTensor(_ep_buffer[:,4])\n",
    "        rpe = _returns - _values\n",
    "        \n",
    "        responsible_outputs = torch.FloatTensor(_ep_buffer[:,5])\n",
    "        \n",
    "        # LOSS FUNCTIONS\n",
    "        critic_loss = 0.5*(rpe).pow(2).mean()\n",
    "        actor_loss = (torch.log(responsible_outputs)*rpe).mean()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        critic_loss.backward()\n",
    "        actor_loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        total_loss[0].append(actor_loss)\n",
    "        total_loss[1].append(critic_loss)\n",
    "        \n",
    "        #####\n",
    "        ##### TODO: UPDATE WEIGHTS \n",
    "        #####\n",
    "        \n",
    "        total_reward.append(reward_sum)\n",
    "        val_maps.append(maze.value_map.copy())\n",
    "        if saveplots:\n",
    "            if (i%100 == 0):\n",
    "                plt.clf()\n",
    "                current_cmap = plt.cm.get_cmap()\n",
    "                current_cmap.set_bad(color='white')\n",
    "                plt.imshow(maze.value_map.copy(), vmin = 0, vmax=40, cmap = 'jet', interpolation='none')\n",
    "\n",
    "                plt.annotate('*', np.add(maze.rwd_loc, (0, -0)), color='w')\n",
    "                plt.title('{}'.format(i))\n",
    "                #plt.gca().invert_yaxis()\n",
    "                plt.colorbar()\n",
    "                plt.savefig(pathvar+str(i),format='png')\n",
    "\n",
    "        # print reward measure\n",
    "        if i==1 or i%(print_freq*NUM_TRIALS)==0 or i == NUM_TRIALS-1: \n",
    "            print \"Trial {0} finished in {1:.3f}. Total reward = {2} (Avg {3:.3f})\".format(i, time.time()-start_time, reward_sum, float(reward_sum)/float(NUM_EVENTS)),\n",
    "            print \"Block took {0:.3f}\".format(time.time()-blocktime)\n",
    "            blocktime = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[0.15292039513587952, 0.10390134900808334, 0.13969439268112183, 0.2419632077217102, 0.18850110471248627, 0.17301954329013824]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    pGrad_buf = sess.run(myAgent.policy_vars)\n",
    "    vGrad_buf = sess.run(myAgent.value_vars)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(total_reward, 'b-', label='Total Reward')\n",
    "#plt.axhline(y=NUM_EVENTS*(0.983), color='r', linestyle='-', label='Minimum \\nPefect Score')\n",
    "plt.legend(loc=0)\n",
    "plt.ylim([0,NUM_EVENTS])\n",
    "plt.savefig(figpath+'{}_reward.svg'.format(mazetype), format = 'svg')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(total_loss[0], 'b', label = 'Actor Loss')\n",
    "plt.title('gamma = {}'.format(discount_factor))\n",
    "plt.plot(total_loss[1], 'r', label= 'Critic Loss')\n",
    "plt.legend(loc=0)\n",
    "plt.savefig(figpath+'{}_loss.svg'.format(mazetype), format = 'svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotrows = 4\n",
    "plotcols = 5\n",
    "fig, axes = plt.subplots(nrows=plotrows, ncols=plotcols, sharex=True, sharey =True)\n",
    "items = np.linspace(0, len(val_maps)-1, plotrows*plotcols)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    data = val_maps[int(items[i])]\n",
    "    #im = ax.pcolor(data, cmap= 'Spectral_r', vmin=np.nanmin(data), vmax=np.nanmax(data))\n",
    "    im = ax.pcolor(data, cmap= 'Spectral_r', vmin=0, vmax=20)\n",
    "    im.cmap.set_under('white')\n",
    "    ax.axis('off')\n",
    "    ax.annotate('*', np.add(maze.rwd_loc, (-0, 1.5)), color='k')\n",
    "    ax.set_title('{}'.format(int(items[i])))\n",
    "\n",
    "axes[0,0].invert_yaxis()\n",
    "\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "if mazetype == 'none':\n",
    "    plt.savefig('./figures/grid_obs{}_valuemap.svg'.format(obs_rho), format='svg')\n",
    "else: \n",
    "    plt.savefig('./figures/{}_valuemap.svg'.format(mazetype), format='svg')\n",
    "plt.show()\n",
    "\n",
    "print np.nanmax(val_maps)\n",
    "\n",
    "data = val_maps[-1]\n",
    "print len(val_maps)\n",
    "plt.imshow(data, vmin=np.nanmin(data), vmax=np.nanmax(data), cmap='jet', interpolation='none')\n",
    "#plt.imshow(data, vmin=1.32, vmax=1.5, cmap='jet', interpolation='none')\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = val_maps[-1].copy()\n",
    "data[np.where(data>0)] = 0\n",
    "\n",
    "## Plot actual choice\n",
    "fig = plt.figure()\n",
    "\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cmx.ScalarMappable(norm = cNorm, cmap=cmap)\n",
    "\n",
    "\n",
    "ax1  = fig.add_axes([0.04, 0, 0.4, 0.85]) # [left, bottom, width, height]\n",
    "ax2   = fig.add_axes([0.47, 0, 0.4, 0.85]) # [left, bottom, width, height]\n",
    "axc = fig.add_axes([0.89, 0.125, 0.05, 0.6])\n",
    "\n",
    "cb1 = mpl.colorbar.ColorbarBase(axc, cmap=cmap, norm=cNorm)\n",
    "\n",
    "ax1.imshow(data, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax1.add_patch(patches.Circle(maze.rwd_loc, 0.35, fc='w'))\n",
    "\n",
    "ax2.imshow(data, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax2.add_patch(patches.Circle(maze.rwd_loc, 0.35, fc='w'))\n",
    "\n",
    "\n",
    "# p_field indicies\n",
    "# 0 - choice, \n",
    "# 1 - list(tfprob)[choice], \n",
    "# 2 - list(tfprob).index(max(list(tfprob))),\n",
    "# 3 - max(list(tfprob)), \n",
    "# 4 - i)\n",
    "\n",
    "for i in range(0, p_field.shape[0]):\n",
    "    for j in range(0, p_field.shape[1]):\n",
    "        dx1, dy1, head_w1, head_l1 = make_arrows(p_field[i][j][0], p_field[i][j][1]) \n",
    "        if (dx1, dy1) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal1 = scalarMap.to_rgba(p_field[i][j][1])\n",
    "            ax1.arrow(j, i, dx1, dy1, head_width =0.3, head_length =0.2, color=colorVal1, alpha = 1 - ((len(val_maps)-p_field[i][j][4])/len(val_maps)))\n",
    "            \n",
    "        dx2, dy2, head_w2, head_l2 = make_arrows(p_field[i][j][2], p_field[i][j][3])\n",
    "        if (dx2, dy2) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal2 = scalarMap.to_rgba(p_field[i][j][3])\n",
    "            ax2.arrow(j, i, dx2, dy2, head_width =0.3, head_length =0.2, color=colorVal2, alpha = 1 - ((len(val_maps)-p_field[i][j][4])/len(val_maps)))\n",
    "            \n",
    "ax1.set_title(\"Chosen Action\")\n",
    "ax2.set_title(\"Most likely choice\")\n",
    "\n",
    "plt.savefig('./figures/{}choice_field.svg'.format(mazetype),format ='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_gif(mypath, mazetype):\n",
    "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    intfiles = [int(f) for f in onlyfiles]\n",
    "    intfiles.sort()\n",
    "    if mazetype =='none':\n",
    "        gifname = './figures/gifs/grid{}.gif'.format(obs_rho)\n",
    "    else: \n",
    "        gifname = './figures/gifs/{}.gif'.format(mazetype)\n",
    "    \n",
    "\n",
    "    with imageio.get_writer(gifname, mode='I', duration=0.5) as writer:\n",
    "                for filename in intfiles:\n",
    "                    image = imageio.imread(mypath+str(filename))\n",
    "                    writer.append_data(image)\n",
    "    print \"Gif file saved at \", gifname\n",
    "\n",
    "if saveplots:\n",
    "    make_gif(pathvar, mazetype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = autograd.Variable(torch.FloatTensor(maze.net_state))\n",
    "policy, value = model(input)\n",
    "print len(policy.data[0])\n",
    "print list(policy.data[0])\n",
    "action_choice = np.random.choice(np.arange(len(policy.data[0])), p=list(policy.data[0]))\n",
    "print action_choice\n",
    "opt = optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "rpe = return_holder - value_holder\n",
    "critic_loss = 0.5*(rpe).pow(2).mean()\n",
    "actor_loss = (torch.log(responsible_outputs)*rpe).mean()\n",
    "'''\n",
    "Loss function - pol_loss / val_loss \n",
    "            \n",
    "    self.return_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "    self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "\n",
    "    self.indexes = tf.range(0, tf.shape(self.policy)[0]) * tf.shape(self.policy)[1] + self.action_holder\n",
    "    self.responsible_outputs = tf.gather(tf.reshape(self.policy, [-1]), self.indexes)\n",
    "\n",
    "    # calculate reward prediction errors\n",
    "    self.rpe = self.return_holder - self.value\n",
    "\n",
    "    self.actor_loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.rpe)\n",
    "    self.critic_loss = 0.5 * tf.reduce_mean(tf.square(self.rpe))\n",
    "\n",
    "\n",
    "pol_loss = -(log (responsible outputs) *(return-value)).mean()\n",
    "val_losss = 0.5* ((return - value).pow(2)).mean()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = np.array([[1],[2],[3],[4],[5],[6],[7]])\n",
    "y = np.repeat(z, 4, axis = 1 )\n",
    "y[:,2] = 8\n",
    "w = np.random.choice(np.arange(4), len(z))\n",
    "print 'w', w\n",
    "\n",
    "x = torch.FloatTensor(y)\n",
    "x = torch.rand((7,4))\n",
    "print 'x', x\n",
    "print np.array([x[ind, w[ind]] for ind,i in enumerate(w)])\n",
    "print (x.mean(dim=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
