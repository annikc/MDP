{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import imageio\n",
    "\n",
    "# for more info check http://pytorch.org/docs/master/torch.html\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Need to change from TF to torch initialization\n",
    "# Functions for making tensorflow graph objects for each layer\n",
    "def add_layer(namescope, inputs, in_size, out_size, activation_function=None):\n",
    "    inputs = tf.cast(inputs, tf.float32)\n",
    "    with tf.name_scope(namescope):\n",
    "        #w = tf.random_normal([in_size, out_size], mean=0, stddev=0.000001)\n",
    "        w = tf.zeros([in_size, out_size])\n",
    "        Weights = tf.Variable(w, dtype=tf.float32, name='weights')\n",
    "        biases = tf.Variable(tf.zeros([out_size]), dtype=tf.float32, name='biases')\n",
    "        Wx_plus_b = tf.matmul(inputs , Weights)+biases\n",
    "        if activation_function is None :\n",
    "            output = Wx_plus_b\n",
    "        else:\n",
    "            output = activation_function(Wx_plus_b)\n",
    "            \n",
    "    return output \n",
    "\n",
    "# Define Agent \n",
    "class agent():\n",
    "    def __init__(self, lr, dims):\n",
    "        # lr is learning rate\n",
    "        # dims is a list of dimensions of each layer [dimension_of_input, dim_of_hiddenlayer1, ... dim_of_hiddenlayerN, dim_of_output]\n",
    "        in_dim = dims[0]\n",
    "        out_dim = dims[-1]\n",
    "        \n",
    "        #These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
    "        with tf.name_scope(\"Input\"):\n",
    "            self.input_layer = tf.placeholder(dtype = tf.float32, shape=[None, in_dim], name = 'state')\n",
    "        \n",
    "        # make hidden layers only if specified in dims list\n",
    "        if len(dims)>2:\n",
    "            hidden_layers = []\n",
    "            for i in range(len(dims)-2):\n",
    "                layername = \"H_\"+str(i+1)\n",
    "                if i == 0:\n",
    "                    hiddenlayer = add_layer(layername, self.input_layer, dims[i], dims[i+1], activation_function=tf.nn.sigmoid)\n",
    "                else:\n",
    "                    hiddenlayer = add_layer(layername, hidden_layers[-1], dims[i], dims[i+1], activation_function=tf.nn.sigmoid)\n",
    "                hidden_layers.append(hiddenlayer)\n",
    "\n",
    "            self.policy = add_layer(\"Policy\", hidden_layers[-1], dims[-2], out_dim, activation_function = tf.nn.softmax)\n",
    "            self.value  = add_layer(\"Value\", hidden_layers[-1], dims[-2], 1, activation_function=None)\n",
    "        # if dims only has input/output dimension, connect output layer directly to input\n",
    "        # using this right now for testing\n",
    "        else:\n",
    "            self.policy = add_layer(\"Policy\", self.input_layer, in_dim, out_dim, activation_function = tf.nn.softmax)\n",
    "            self.value  = add_layer(\"Value\", self.input_layer, in_dim, 1, activation_function=None)\n",
    "        \n",
    "        \n",
    "        # make variable lists to specify to tensorflow which variables to update with which gradients\n",
    "        self.policy_vars = tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Layer1\")+tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Layer2\")+tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Policy\")\n",
    "        self.value_vars = tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Layer1\")+tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Layer2\")+tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Value\")\n",
    "        #list of all variables, not used -- may take out later\n",
    "        self.tvars = tf.trainable_variables()\n",
    "        \n",
    "        #The next six lines establish the training proceedure. We feed the return and chosen action into the network\n",
    "        #to compute the loss, and use it to update the network.\n",
    "        with tf.name_scope(\"Loss\"):\n",
    "            self.return_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "            self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "\n",
    "            self.indexes = tf.range(0, tf.shape(self.policy)[0]) * tf.shape(self.policy)[1] + self.action_holder\n",
    "            self.responsible_outputs = tf.gather(tf.reshape(self.policy, [-1]), self.indexes)\n",
    "            # calculate reward prediction errors\n",
    "            self.rpe = self.return_holder - self.value\n",
    "            \n",
    "            self.actor_loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.rpe)\n",
    "            self.critic_loss = 0.5 * tf.reduce_mean(tf.square(self.rpe))\n",
    "\n",
    "        with tf.name_scope(\"Training\"):\n",
    "            # make placeholders to store gradients -- maybe a clunky way to do this but will become useful later\n",
    "            # rather than feeding gradients directly to the apply gradients operation\n",
    "            self.p_gradient_holders = []\n",
    "            self.v_gradient_holders = []\n",
    "            for idx,var in enumerate(self.policy_vars):\n",
    "                placeholder = tf.placeholder(tf.float32,name=str(idx)+'_p_holder')\n",
    "                self.p_gradient_holders.append(placeholder)\n",
    "\n",
    "            for idx,var in enumerate(self.value_vars):\n",
    "                placeholder = tf.placeholder(tf.float32,name=str(idx)+'_v_holder')\n",
    "                self.v_gradient_holders.append(placeholder)\n",
    "            \n",
    "            # specifiy which training we are going to do for policy\n",
    "            pol_trainer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "            \n",
    "            # calculate gradients for policy loss only on policy variables\n",
    "            self.get_pol_grads = tf.gradients(self.actor_loss, self.policy_vars)\n",
    "            # op for updating policy gradients\n",
    "            self.update_pol = pol_trainer.apply_gradients(zip(self.p_gradient_holders, self.policy_vars))\n",
    "            \n",
    "            # specifiy which training we are going to do for value\n",
    "            val_trainer = tf.train.GradientDescentOptimizer(learning_rate=0.001*lr)\n",
    "            # calulate gradients for value loss\n",
    "            self.get_val_grads = tf.gradients(self.critic_loss, self.value_vars)\n",
    "            # op for updating value gradients\n",
    "            self.update_val = val_trainer.apply_gradients(zip(self.v_gradient_holders, self.value_vars)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#========================\n",
    "# Environment Parameters\n",
    "#======================== \n",
    "height = 10\n",
    "width = 10\n",
    "\n",
    "mazetype = 'room'\n",
    "#obstacle density\n",
    "obs_rho = 0.2\n",
    "\n",
    "#place cells\n",
    "place_cells = 1000\n",
    "#place cell full width half max (must be <1)\n",
    "fwhm = 0.25\n",
    "\n",
    "#make environment\n",
    "maze = gridworld([height, width],rho=obs_rho,num_pc=place_cells, pc_fwhm=fwhm, maze_type=mazetype)\n",
    "## plots\n",
    "plot_grid(maze)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print maze.cur_state\n",
    "print maze.start_loc\n",
    "\n",
    "\n",
    "plt.scatter(maze.pcs.x, maze.pcs.y, c=maze.pcs.activity(maze.cur_state), cmap = 'jet', vmin = 0, vmax =1, s = 100)#a.field_width)\n",
    "plt.ylim([1,0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "#plt.pcolor(maze.pcs.activity(maze.cur_state).reshape(1,maze.pcs.activity(maze.cur_state).shape[0]), vmin=0, vmax=1, cmap='jet')\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=====================\n",
    "# Training Parameters\n",
    "#=====================\n",
    "#trial parameters\n",
    "NUM_TRIALS = 6000\n",
    "NUM_EVENTS = 300\n",
    "\n",
    "discount_factor = 0.98\n",
    "port_shift = 'none'\n",
    "\n",
    "#gradient descent learning rate\n",
    "eta = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Single run of NUM_TRIALS each with NUM_EVENTS\n",
    "tf.reset_default_graph()\n",
    "\n",
    "dims = [len(maze.net_state[0]),len(maze.actionlist)]\n",
    "\n",
    "myAgent = agent(lr=eta, dims=dims)\n",
    "#tf.summary.FileWriter('./outputs/maze3/', graph=tf.get_default_graph())\n",
    "\n",
    "print_freq = 1./10\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saveplots = False\n",
    "if saveplots:\n",
    "    if mazetype=='none':\n",
    "        pathvar = './valplots/grid/{}/'.format(obs_rho)\n",
    "    else: \n",
    "        pathvar = './valplots/{}/'.format(mazetype)\n",
    "    if not os.path.exists(pathvar):\n",
    "        os.makedirs(pathvar)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    steps_rwd = []\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    pGrad_buf = sess.run(myAgent.policy_vars)\n",
    "    vGrad_buf = sess.run(myAgent.value_vars)\n",
    "    \n",
    "    total_reward = []\n",
    "    trialtime = []\n",
    "    total_loss = [[],[]]\n",
    "\n",
    "    blocktime = time.time()\n",
    "    val_maps = []\n",
    "    \n",
    "    print strftime(\"%a, %d %b %Y %H:%M:%S +0000\", time.localtime())\n",
    "    for i in xrange(NUM_TRIALS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        _ep_buffer = []\n",
    "        reward_sum = 0\n",
    "\n",
    "        # reset gradient buffers\n",
    "        for ix,grad in enumerate(pGrad_buf):\n",
    "            pGrad_buf[ix] = grad * 0\n",
    "        for ix,grad in enumerate(vGrad_buf):\n",
    "            vGrad_buf[ix] = grad * 0\n",
    "\n",
    "        # reset the environment\n",
    "        maze.start_trial()\n",
    "        maze.value_map = maze.init_value_map\n",
    "        \n",
    "        state = maze.net_state\n",
    "        \n",
    "        for j in xrange(NUM_EVENTS): \n",
    "            # get policy and value estimate in order to select next action\n",
    "            tfprob_, val_ = sess.run([myAgent.policy, myAgent.value], feed_dict={myAgent.input_layer:state})\n",
    "\n",
    "            # process tensorflow lists to get right shape\n",
    "            tfprob = tfprob_[0]\n",
    "            val = val_[0][0]\n",
    "            #print maze.cur_state, val, \"###\"\n",
    "            maze.value_map[maze.cur_state[1]][maze.cur_state[0]] = val\n",
    "\n",
    "            # select action\n",
    "            choice = np.random.choice(np.arange(len(tfprob)), 1, p=tfprob)[0]\n",
    "            action = maze.actionlist[choice]\n",
    "\n",
    "            # get new state of the environment and reward from action \n",
    "            if j < NUM_EVENTS:\n",
    "                next_state = maze.move(action)\n",
    "\n",
    "            rwd = maze.rwd\n",
    "\n",
    "            # store buffer of agents experiences so that later we can calulate returns/etc. backwards through time\n",
    "            _ep_buffer.append([state,choice,rwd,next_state, val])\n",
    "\n",
    "            # update state\n",
    "            state = next_state\n",
    "\n",
    "            reward_sum += rwd\n",
    "            \n",
    "            #if maze.done == True: \n",
    "            #    trialtime.append(j)\n",
    "            #    break\n",
    "            #elif j == NUM_EVENTS-1:\n",
    "            #    trialtime.append(NUM_EVENTS)\n",
    "        # make data storage useable type \n",
    "        _ep_buffer = np.array(_ep_buffer)\n",
    "        # compute returns\n",
    "        _returns = discount_rwds(_ep_buffer[:,2], gamma=discount_factor)\n",
    "\n",
    "        feed_dict = {myAgent.return_holder:_returns, myAgent.action_holder:_ep_buffer[:,1], myAgent.input_layer:np.vstack(_ep_buffer[:,0])}\n",
    "\n",
    "        # calculate gradients using the information stored in the episode buffer\n",
    "        # computed returns (backward through time)\n",
    "        # which actions were taken gives which policy unit was responsible (so grads are computed properly)\n",
    "        # pass states agent was in at each timestep through the network again to recompute the value and policy (for gradients computation in tf)\n",
    "        a_loss, c_loss = sess.run([myAgent.actor_loss, myAgent.critic_loss], feed_dict=feed_dict)\n",
    "        p_grads, v_grads = sess.run([myAgent.get_pol_grads, myAgent.get_val_grads], feed_dict=feed_dict)\n",
    "        \n",
    "        total_loss[0].append(a_loss)\n",
    "        total_loss[1].append(c_loss)\n",
    "        \n",
    "        # store gradients in gradient buffers -- not necessary for the current formulation but will be more flexible for later\n",
    "        # sorry for the additional complication \n",
    "        for idx, grad in enumerate(p_grads):\n",
    "            pGrad_buf[idx] += grad\n",
    "\n",
    "        for idx, grad in enumerate(v_grads):\n",
    "            vGrad_buf[idx] += grad\n",
    "\n",
    "        feed_dict = dict(zip(myAgent.p_gradient_holders, pGrad_buf)+zip(myAgent.v_gradient_holders, vGrad_buf))\n",
    "\n",
    "        # run gradient update operations \n",
    "        _, __ = sess.run([myAgent.update_pol, myAgent.update_val], feed_dict = feed_dict)\n",
    "\n",
    "        total_reward.append(reward_sum)\n",
    "        val_maps.append(maze.value_map.copy())\n",
    "        if saveplots:\n",
    "            if (i%100 == 0):\n",
    "                plt.clf()\n",
    "                current_cmap = plt.cm.get_cmap()\n",
    "                current_cmap.set_bad(color='white')\n",
    "                plt.imshow(maze.value_map.copy(), vmin = 0, vmax=36, cmap = 'jet', interpolation='none')\n",
    "\n",
    "                plt.annotate('*', np.add(maze.rwd_loc, (0, -0)), color='w')\n",
    "                plt.title('{}'.format(i))\n",
    "                #plt.gca().invert_yaxis()\n",
    "                plt.colorbar()\n",
    "                plt.savefig(pathvar+str(i),format='png')\n",
    "\n",
    "        # print reward measure\n",
    "        if i==1 or i%(print_freq*NUM_TRIALS)==0 or i == NUM_TRIALS-1: \n",
    "            print \"Trial {0} finished in {1:.3f}. Total reward = {2} (Avg {3:.3f})\".format(i, time.time()-start_time, reward_sum, float(reward_sum)/float(NUM_EVENTS)),\n",
    "            print \"Block took {0:.3f}\".format(time.time()-blocktime)\n",
    "            blocktime = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if mazetype =='none':\n",
    "    mypath = './valplots/grid/{}/'.format(obs_rho)\n",
    "else:\n",
    "    mypath = './valplots/{}/'.format(mazetype)\n",
    "if not os.path.exists(mypath):\n",
    "    os.makedirs(mypath)\n",
    "\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "intfiles = [int(f) for f in onlyfiles]\n",
    "intfiles.sort()\n",
    "if mazetype =='none':\n",
    "    gifname = './valplots/gifs/grid{}.gif'.format(obs_rho)\n",
    "else: \n",
    "    gifname = './valplots/gifs/{}.gif'.format(mazetype)\n",
    "\n",
    "with imageio.get_writer(gifname, mode='I', duration=0.5) as writer:\n",
    "            for filename in intfiles:\n",
    "                image = imageio.imread(mypath+str(filename))\n",
    "                writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(total_reward, 'b-')\n",
    "plt.axhline(y=NUM_EVENTS*(0.983), color='r', linestyle='-', label='Minimum \\nPefect Score')\n",
    "#plt.xlim([0,1000])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(total_loss[0], 'b')\n",
    "plt.title('gamma = {}'.format(discount_factor))\n",
    "plt.plot(total_loss[1], 'r')\n",
    "#plt.xlim([5000,6000])\n",
    "#plt.ylim([0,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotrows = 4\n",
    "plotcols = 5\n",
    "fig, axes = plt.subplots(nrows=plotrows, ncols=plotcols, sharex=True, sharey =True)\n",
    "items = np.linspace(0, len(val_maps)-1, plotrows*plotcols)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    data = val_maps[int(items[i])]\n",
    "    im = ax.imshow(data, vmin = 0, cmap= 'jet', interpolation='None')\n",
    "    ax.annotate('*', np.add(maze.rwd_loc, (-.5, 1)), color='w')\n",
    "    ax.set_title('{}'.format(int(items[i])))\n",
    "\n",
    "\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "plt.savefig('./valplots/obs{}_valuemap'.format(obs_rho), format='svg')\n",
    "plt.show()\n",
    "\n",
    "print np.nanmax(val_maps)\n",
    "\n",
    "data = val_maps[-1]- val_maps[3157]\n",
    "plt.imshow(data, vmin=0, vmax=30, cmap='jet', interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions for making tensorflow graph objects for each layer\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    inputs = tf.cast(inputs, tf.float32)\n",
    "    with tf.name_scope(namescope):\n",
    "        #w = tf.random_normal([in_size, out_size], mean=0, stddev=0.000001)\n",
    "        w = tf.zeros([in_size, out_size])\n",
    "        Weights = tf.Variable(w, dtype=tf.float32, name='weights')\n",
    "        biases = tf.Variable(tf.zeros([out_size]), dtype=tf.float32, name='biases')\n",
    "        Wx_plus_b = tf.matmul(inputs , Weights)+biases\n",
    "        if activation_function is None :\n",
    "            output = Wx_plus_b\n",
    "        else:\n",
    "            output = activation_function(Wx_plus_b)\n",
    "            \n",
    "    return output   \n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        dims = [500, 200, 200, 6]\n",
    "        in_dim = dims[0]\n",
    "        if len(dims) >2: \n",
    "            hidden_dims = []\n",
    "            for i in range(len(dims-2)):\n",
    "                hidden_dims.append(dims[1+i])\n",
    "        out_dim = dims[-1]\n",
    "\n",
    "        self.layers = []\n",
    "        for i in range(len(dims)):\n",
    "        self.layers.append()\n",
    "        \n",
    "        self.layer1 = nn.Linear(in_dim)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Variables for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Variable of input data to the Module and it produces\n",
    "    # a Variable of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Variables containing the predicted and true\n",
    "    # values of y, and the loss function returns a Variable containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Variables with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Variable, so\n",
    "    # we can access its data and gradients like we did before.\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dims = [500, 200, 200, 6]\n",
    "in_dim = dims[0]\n",
    "if len(dims)>2: \n",
    "    hidden_dims = []\n",
    "    for i in range(len(dims)-2):\n",
    "        hidden_dims.append(dims[1+i])\n",
    "out_dim = dims[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
