{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "#import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "from torch import autograd, optim, nn\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import env_utils as eu\n",
    "import MF as au\n",
    "import EC as ec\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1e79aa9110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make environment\n",
    "maze = eu.gridworld([eu.height, eu.width],\n",
    "                    rho        = eu.obs_rho,\n",
    "                    num_pc     = eu.place_cells, \n",
    "                    pc_fwhm    = eu.fwhm, \n",
    "                    maze_type  = eu.mazetype, \n",
    "                    port_shift = eu.portshift)\n",
    "#show environment\n",
    "eu.make_env_plots(maze,1,0)\n",
    "\n",
    "## test out gridworld wrapper. \n",
    "env = eu.gymworld(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================\n",
    "# Training Parameters\n",
    "#=====================\n",
    "#trial parameters\n",
    "NUM_TRIALS = 100\n",
    "NUM_EVENTS = 300\n",
    "\n",
    "discount_factor = 0.98\n",
    "\n",
    "#agent parameters\n",
    "dims = [env.observation_space.shape[0],100,50,env.action_space.n]\n",
    "model = au.AC_Net(dims)\n",
    "\n",
    "#learning parameters\n",
    "eta = 5e-4 #gradient descent learning rate\n",
    "opt = au.optim.Adam(model.parameters(), lr = eta)\n",
    "\n",
    "# memory module parameters\n",
    "EC__ = ec.ep_mem(model,50)\n",
    "#EC__.reset_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frames for value and policy maps\n",
    "val_maps = []\n",
    "\n",
    "# record current time before beginning of trial\n",
    "print \"Run started: \", strftime(\"%a, %d %b %Y %H:%M:%S +0000\", time.localtime())\n",
    "runtime= time.time()\n",
    "blocktime = time.time()\n",
    "\n",
    "#reset agent policy/value maps\n",
    "value_map = maze.empty_map\n",
    "\n",
    "t_check = time.time()\n",
    "timestamp = 1\n",
    "EC__.reset_cache()\n",
    "## variables for memory confidence\n",
    "time_since_last_reward = 0\n",
    "no_r= True\n",
    "cs_max = 0 \n",
    "confidence_tracker = []\n",
    "reward=0\n",
    "#==================================\n",
    "# Run Trial\n",
    "#==================================\n",
    "for trial in xrange(NUM_TRIALS):\n",
    "    trial_start = time.time()\n",
    "    init_state = env.reset()\n",
    "    \n",
    "    state = Variable(torch.FloatTensor(init_state))\n",
    "    reward_sum = 0\n",
    "     \n",
    "    v_last = 0\n",
    "    for event in xrange(NUM_EVENTS):\n",
    "        #pass state through network --> get policy and value [****]\n",
    "        \n",
    "        #compute memory confidence score [EC.mem_confidence()]\n",
    "        \n",
    "        #make composite policy using MF and EC policies [EC.recall_mem()]\n",
    "            # if EC cache is too small or similarity of memories is not good, \n",
    "            # composite policy == model policy\n",
    "            \n",
    "        #pick action based on composite policy\n",
    "        \n",
    "        #step based on selected action\n",
    "        \n",
    "        #compute eligibility trace/rpe approximation\n",
    "        \n",
    "        #add event to memory cache\n",
    "        \n",
    "        #--- AT end of trial, update weights \n",
    "        \n",
    "        \n",
    "        policy_, value_ = model(state)\n",
    "        conf,cs_max,time_since_last_reward = EC__.mem_confidence(reward,cs_max,time_since_last_reward,no_r)\n",
    "        policy_ = EC__.recall_mem(tuple(state.data[0]),policy_.data[0],conf)\n",
    "        choice, policy, value = au.select_action(model,policy_, value_)\n",
    "        choice_prob = policy[choice[0]]\n",
    "        \n",
    "        if event < NUM_EVENTS: \n",
    "            next_state, reward, done, info = env.step(choice[0])\n",
    "            \n",
    "        model.rewards.append(reward)\n",
    "        # update state\n",
    "        state = Variable(torch.FloatTensor(next_state))\n",
    "\n",
    "        delta = reward - value - v_last\n",
    "        \n",
    "        ### is it supposed to be current state?? \n",
    "        EC__.add_mem(tuple(next_state[0]), choice, delta, timestamp)\n",
    "        reward_sum += reward\n",
    "        v_last = value\n",
    "        timestamp += 1\n",
    "        \n",
    "    value_map = au.generate_values(maze, model)\n",
    "    p_loss, v_loss = au.finish_trial(model, discount_factor,opt)\n",
    "    total_loss[0].append(p_loss.data[0])\n",
    "    total_loss[1].append(v_loss.data[0])\n",
    "    total_reward.append(reward_sum)\n",
    "    val_maps.append(value_map.copy())\n",
    "    \n",
    "    # print reward measure\n",
    "    if trial%(print_freq*NUM_TRIALS)==0 or trial == NUM_TRIALS-1: \n",
    "        print \"Trial {0} finished w total reward = {1} (Avg {2:.3f})\".format(trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)\n",
    "        blocktime = time.time() \n",
    "    \n",
    "print \"Run took {0:.3f}\".format(time.time()-runtime)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pol_maps = []\n",
    "\n",
    "# record current time before beginning of trial\n",
    "print \"Run started: \", strftime(\"%a, %d %b %Y %H:%M:%S +0000\", time.localtime())\n",
    "runtime= time.time()\n",
    "blocktime = time.time()\n",
    "\n",
    "#reset agent policy/value maps\n",
    "value_map = maze.empty_map\n",
    "policy_map  = np.zeros((height, width), dtype=[('action_taken', 'i4'),\n",
    "                                            ('taken_prob', 'f4'), \n",
    "                                            ('likely_action', 'i4'),\n",
    "                                            ('likely_prob', 'f4')])\n",
    "t_check = time.time()\n",
    "timestamp = 1\n",
    "EC__.reset_cache()\n",
    "## variables for memory confidence\n",
    "time_since_last_reward = 0\n",
    "no_r= True\n",
    "cs_max = 0 \n",
    "confidence_tracker = []\n",
    "reward=0\n",
    "#==================================\n",
    "# Run Trial\n",
    "#==================================\n",
    "for trial in xrange(NUM_TRIALS):\n",
    "    trial_start = time.time()\n",
    "    init_state = env.reset()\n",
    "    \n",
    "    state = Variable(torch.FloatTensor(init_state))\n",
    "    \n",
    "    \n",
    "    \n",
    "    reward_sum = 0\n",
    "     \n",
    "    v_last = 0\n",
    "    for event in xrange(NUM_EVENTS):\n",
    "        policy_, value_ = model(state)\n",
    "        conf,cs_max,time_since_last_reward = EC__.mem_confidence(reward,cs_max,time_since_last_reward,no_r)\n",
    "        policy_ = EC__.recall_mem(tuple(state.data[0]),policy_.data[0],conf)\n",
    "        choice, policy, value = au.select_action(model,policy_, value_)\n",
    "        choice_prob = policy[choice[0]]\n",
    "        \n",
    "        if event < NUM_EVENTS: \n",
    "            next_state, reward, done, info = env.step(choice[0])\n",
    "            \n",
    "        model.rewards.append(reward)\n",
    "        # update state\n",
    "        state = Variable(torch.FloatTensor(next_state))\n",
    "\n",
    "        delta = reward - value - v_last\n",
    "        \n",
    "        ### is it supposed to be current state?? \n",
    "        EC__.add_mem(tuple(next_state[0]), choice, delta, timestamp)\n",
    "        reward_sum += reward\n",
    "        v_last = value\n",
    "        timestamp += 1\n",
    "        \n",
    "    value_map = au.generate_values(maze, model)\n",
    "    p_loss, v_loss = au.finish_trial(model, discount_factor,opt)\n",
    "    total_loss[0].append(p_loss.data[0])\n",
    "    total_loss[1].append(v_loss.data[0])\n",
    "    total_reward.append(reward_sum)\n",
    "    val_maps.append(value_map.copy())\n",
    "    \n",
    "    # print reward measure\n",
    "    if trial%(print_freq*NUM_TRIALS)==0 or trial == NUM_TRIALS-1: \n",
    "        print \"Trial {0} finished w total reward = {1} (Avg {2:.3f})\".format(trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)\n",
    "        blocktime = time.time() \n",
    "    \n",
    "print \"Run took {0:.3f}\".format(time.time()-runtime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = Variable(torch.FloatTensor(maze.mk_state(state=(2,2))))\n",
    "choice, policy, value = au.select_action(model,state)\n",
    "print value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(0)\n",
    "plt.plot(np.arange(len(total_loss[0])), total_loss[0], label='ploss')\n",
    "plt.plot(total_loss[1], label='vloss')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(np.arange(len(total_loss[0])),total_reward, label='rwd')\n",
    "plt.xlabel(\"Number of Trials\")\n",
    "plt.ylabel(\"Total Reward per Trial\")\n",
    "#plt.xlim([0,14000])\n",
    "plt.ylim([0,305])\n",
    "#plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eu.print_value_maps(maze,val_maps)\n",
    "print maze.rwd_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = val_maps[-1].copy()\n",
    "data[np.where(data>0)] = 0\n",
    "\n",
    "## Plot actual choice\n",
    "fig = plt.figure()\n",
    "\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cmx.ScalarMappable(norm = cNorm, cmap=cmap)\n",
    "\n",
    "\n",
    "ax1  = fig.add_axes([0.04, 0, 0.4, 0.85]) # [left, bottom, width, height]\n",
    "ax2   = fig.add_axes([0.47, 0, 0.4, 0.85]) # [left, bottom, width, height]\n",
    "axc = fig.add_axes([0.89, 0.125, 0.05, 0.6])\n",
    "\n",
    "cb1 = mpl.colorbar.ColorbarBase(axc, cmap=cmap, norm=cNorm)\n",
    "\n",
    "ax1.imshow(data, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax1.add_patch(patches.Circle(maze.rwd_loc, 0.35, fc='w'))\n",
    "\n",
    "ax2.imshow(data, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax2.add_patch(patches.Circle(maze.rwd_loc, 0.35, fc='w'))\n",
    "\n",
    "\n",
    "# p_field indicies\n",
    "# 0 - choice, \n",
    "# 1 - list(tfprob)[choice], \n",
    "# 2 - list(tfprob).index(max(list(tfprob))),\n",
    "# 3 - max(list(tfprob)), \n",
    "# 4 - i)\n",
    "\n",
    "for i in range(0, p_field.shape[0]):\n",
    "    for j in range(0, p_field.shape[1]):\n",
    "        dx1, dy1, head_w1, head_l1 = make_arrows(p_field[i][j][0], p_field[i][j][1]) \n",
    "        if (dx1, dy1) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal1 = scalarMap.to_rgba(p_field[i][j][1])\n",
    "            ax1.arrow(j, i, dx1, dy1, head_width =0.3, head_length =0.2, color=colorVal1, alpha = 1 - ((len(val_maps)-p_field[i][j][4])/len(val_maps)))\n",
    "            \n",
    "        dx2, dy2, head_w2, head_l2 = make_arrows(p_field[i][j][2], p_field[i][j][3])\n",
    "        if (dx2, dy2) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal2 = scalarMap.to_rgba(p_field[i][j][3])\n",
    "            ax2.arrow(j, i, dx2, dy2, head_width =0.3, head_length =0.2, color=colorVal2, alpha = 1 - ((len(val_maps)-p_field[i][j][4])/len(val_maps)))\n",
    "            \n",
    "ax1.set_title(\"Chosen Action\")\n",
    "ax2.set_title(\"Most likely choice\")\n",
    "\n",
    "plt.savefig('./figures/{}choice_field.svg'.format(mazetype),format ='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_dict[mazetype] = total_reward\n",
    "for i in reward_dict.keys():\n",
    "    avgs[i] = eu.running_mean(reward_dict[i], 100)\n",
    "    \n",
    "print reward_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(reward_dict.keys())\n",
    "colours = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "plt.figure(1)\n",
    "for num,i in enumerate(reward_dict.keys()):\n",
    "    #plt.plot(np.arange(len(total_reward)),reward_dict[i], colours[num],alpha=0.2*(num+1), label=i)\n",
    "    plt.plot(np.arange(len(total_reward)), avgs[i], colours[num], label=i)\n",
    "plt.xlabel(\"Number of Trials\")\n",
    "plt.ylabel(\"Total Reward per Trial\")\n",
    "#plt.xlim([0,14000])\n",
    "plt.ylim([0,305])\n",
    "plt.legend(loc=0)\n",
    "plt.savefig('average_rewards.svg', format ='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(eu)\n",
    "eu.print_value_maps(maze,val_maps,maps=845)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(eu)\n",
    "eu.print_value_maps(maze,val_maps,maps=14000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make fake memory dictionary for testing\n",
    "num_inputs = 5 #706\n",
    "num_actions = 6\n",
    "cache_limit = 50\n",
    "\n",
    "d = {}\n",
    "for i in xrange(cache_limit):\n",
    "    key = tuple(np.random.randn(num_inputs))\n",
    "    action = np.zeros(num_actions)\n",
    "    action[np.random.choice(np.arange(num_actions))] = 1\n",
    "    delta = np.random.randn(1)*100\n",
    "    timestamp = i\n",
    "    d[key] = action*delta, i\n",
    "    \n",
    "# \n",
    "memory_envelope = 1 \n",
    "def make_pval(cur_time, mem_times,deltas):\n",
    "    if type(mem_times) == list:\n",
    "        pvals = []\n",
    "        for i in xrange(len(mem_times)):\n",
    "            p = ((cur_time - mem_times[i])/cur_time)*deltas[i]\n",
    "            pval = 1-1/np.cosh(p/memory_envelope)\n",
    "            pvals.append(pval)\n",
    "    else:\n",
    "        print \"Error in retrieving persistence values\"\n",
    "    return pvals\n",
    "\n",
    "print(make_pval(100, [10,13,1,45], np.arange(4)))\n",
    "test = [v[1] for k, v in d.items()]\n",
    "print d[d.keys()[np.argmin(test)]][1]\n",
    "print \"## \", d.keys()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 interactive value map plot\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(3)\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(maze.x)\n",
    "Y = np.arange(maze.y)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "print X,Y\n",
    "Z = val_maps[50]\n",
    "\n",
    "# Plot the surface.\n",
    "zlimz= [np.nanmin(val_maps), np.nanmax(val_maps)]\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.Spectral_r, vmin=zlimz[0], vmax = zlimz[1])\n",
    "#a = ax.plot_surface()\n",
    "\n",
    "# Customize the z axis.\n",
    "ax.set_zlim(zlimz[0], zlimz[1])\n",
    "ax.invert_xaxis()\n",
    "#ax.axis('equal')\n",
    "\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "a = np.linspace(0,100,100)\n",
    "test_rewards= [] \n",
    "for _ in xrange(500):\n",
    "    test_rewards.append(np.random.choice([0,1],1,p=[.9, 0.1])[0])\n",
    "\n",
    "print test_rewards\n",
    "\n",
    "\n",
    "num_test_states = 300\n",
    "for i in xrange(num_test_states):\n",
    "    state = env.reset()\n",
    "    key = tuple(Variable(torch.FloatTensor(state)).data.numpy()[0][0:10])\n",
    "    EC__.add_mem(key, np.random.randint(6), np.random.randn(1), np.random.randint(100)+1)\n",
    "tester = tuple(Variable(torch.FloatTensor(env.reset())).data.numpy()[0][0:10])\n",
    "\n",
    "current_delta = 5\n",
    "current_action = 4\n",
    "recall1 = EC__.recall_mem(tester, current_action, current_delta, 0)\n",
    "recall2 = EC__.recall_mem(tester, current_action, current_delta, 0.5)\n",
    "recall3 = EC__.recall_mem(tester, current_action, current_delta, 1)\n",
    "\n",
    "\n",
    "plt.bar(np.arange(6)-.2, recall1, label='0', width= 0.2, color='r')\n",
    "plt.bar(np.arange(6), recall2, label='.5', width = 0.2, color='b')\n",
    "plt.bar(np.arange(6)+.2, recall3, label='1', width = 0.2,color='g')\n",
    "plt.ylim([0,1])\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "\n",
    "###########################\n",
    "\n",
    "time_since_last_reward = 0\n",
    "no_r= True\n",
    "cs_max = 0 \n",
    "\n",
    "record = []\n",
    "for rwd in test_rewards:\n",
    "    if rwd == 1: \n",
    "        no_r = False\n",
    "    cs, cs_max, time_since_last_reward = EC__.mem_confidence(rwd, cs_max, time_since_last_reward,no_r)\n",
    "    record.append(cs)\n",
    "plt.plot(record, 'r')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
