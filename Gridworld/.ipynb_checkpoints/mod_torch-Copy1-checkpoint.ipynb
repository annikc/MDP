{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import random\n",
    "\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "from torch import autograd, optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "from utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up network\n",
    "class AC_Net(nn.Module):\n",
    "    def __init__(self, dims):        \n",
    "        super(AC_Net, self).__init__()\n",
    "        # dims is a list of dimensions of each layer [dimension_of_input, dim_of_hiddenlayer1, ... dim_of_hiddenlayerN, dim_of_output]\n",
    "        self.layers = dims\n",
    "        if len(dims)>2: \n",
    "            self.hidden = []\n",
    "            for i in range(len(dims)-2):\n",
    "                self.hidden.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            self.h_layers = nn.ModuleList(self.hidden)\n",
    "            \n",
    "        self.p_in = nn.Linear(dims[-2],dims[-1])\n",
    "        self.v_in = nn.Linear(dims[-2],1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.001)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(self.layers)>2:\n",
    "            for i in range(len(self.hidden)):\n",
    "                x = F.relu(self.hidden[i](x))\n",
    "        pol = F.softmax(self.p_in(x),dim=1)\n",
    "        val = self.v_in(x)\n",
    "\n",
    "        return pol, val\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "def select_action(state):\n",
    "    policy_, value_ = model(state)\n",
    "    a = Categorical(policy_)\n",
    "    action = a.sample()\n",
    "    model.saved_actions.append(SavedAction(a.log_prob(action), value_))\n",
    "    return action.data[0], policy_.data[0], value_.data[0]\n",
    "\n",
    "\n",
    "def finish_trial():\n",
    "    R = 0\n",
    "    returns_ = discount_rwds(np.asarray(model.rewards), gamma=discount_factor)\n",
    "    saved_actions = model.saved_actions\n",
    "    \n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    \n",
    "    returns_ = torch.Tensor(returns_)\n",
    "    #returns_ = (returns_ - returns_.mean()) / (returns_.std() + np.finfo(np.float32).eps)\n",
    "    for (log_prob, value), r in zip(saved_actions, returns_):\n",
    "        rpe = r - value.data[0, 0]\n",
    "        policy_losses.append(-log_prob * rpe)\n",
    "        value_losses.append(F.smooth_l1_loss(value, Variable(torch.Tensor([r]))))\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.cat(policy_losses).sum() + torch.cat(value_losses).sum()\n",
    "    loss.backward(retain_graph=False)\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'zip' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a6a9361fe5e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#make environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmaze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgridworld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs_rho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_pc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplace_cells\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpc_fwhm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfwhm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaze_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmazetype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmake_env_plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/LINC Lab Documents/Research/MDP/Gridworld/utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gridSize, showgrid, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrwd_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0museable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                         \u001b[0mrwd_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0museable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrwd_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0museable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrwd_choice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'zip' has no len()"
     ]
    }
   ],
   "source": [
    "#========================\n",
    "# Environment Parameters\n",
    "#======================== \n",
    "height = 15\n",
    "width = 15\n",
    "\n",
    "mazetype = 'none'\n",
    "#obstacle density\n",
    "obs_rho = 0\n",
    "\n",
    "#place cells\n",
    "place_cells = 700\n",
    "#place cell full width half max (must be <1)\n",
    "fwhm = .2\n",
    "\n",
    "#make environment\n",
    "maze = gridworld([height, width],rho=obs_rho,num_pc=place_cells, pc_fwhm=fwhm, maze_type=mazetype)\n",
    "\n",
    "make_env_plots(maze,1,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==================================\n",
    "# Set up Data Recording Structures\n",
    "#==================================\n",
    "print_freq = 1./10\n",
    "total_reward = [] #track total reward achieved in trial \n",
    "total_loss = [[],[]] #[actor loss, critic loss]\n",
    "\n",
    "# data frames for value and policy maps\n",
    "val_maps = []\n",
    "pol_maps = np.zeros((height, width), dtype=[('action_taken', 'i4'),('taken_prob', 'f4'), ('likely_action', 'i4'),('likely_prob', 'f4'), ('timestep', 'i4')])\n",
    "\n",
    "# record current time before beginning of trial\n",
    "print(\"Run started: \", strftime(\"%a, %d %b %Y %H:%M:%S +0000\", time.localtime()))\n",
    "runtime= time.time()\n",
    "blocktime = time.time()\n",
    "for trial in xrange(NUM_TRIALS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    reward_sum = 0\n",
    "\n",
    "    # reset the environment, initialize agent position and reset value map\n",
    "    maze.start_trial()\n",
    "    maze.value_map = maze.init_value_map\n",
    "\n",
    "    # get inital state\n",
    "    state = Variable(torch.FloatTensor(maze.net_state))\n",
    "    for event in xrange(NUM_EVENTS):\n",
    "        # get policy and value estimate in order to select next action\n",
    "        choice, policy, value = select_action(model,state)\n",
    "\n",
    "        # select action\n",
    "        action = maze.actionlist[choice]\n",
    "        choice_prob = policy[choice]\n",
    "\n",
    "        maze.value_map[maze.cur_state[1]][maze.cur_state[0]] = value\n",
    "        pol_maps[maze.cur_state[1]][maze.cur_state[0]] = (choice, list(policy)[choice], list(policy).index(max(list(policy))), max(list(policy)), trial)\n",
    "\n",
    "        # get new state of the environment and reward from action \n",
    "        if event < NUM_EVENTS:\n",
    "            next_state = maze.move(action)\n",
    "            \n",
    "        model.rewards.append(maze.rwd)\n",
    "        # update state\n",
    "        state = autograd.Variable(torch.FloatTensor(next_state))\n",
    "\n",
    "        reward_sum += maze.rwd\n",
    "    \n",
    "    total_reward.append(reward_sum)\n",
    "    \n",
    "    finish_trial(model, discount_factor,optimizer)\n",
    "    val_maps.append(maze.value_map.copy())\n",
    "    # print reward measure\n",
    "    if trial==1 or trial%(print_freq*NUM_TRIALS)==0 or trial == NUM_TRIALS-1: \n",
    "        print (\"Trial {0} finished w total reward = {1} (Avg {2:.3f})\".format(trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime))\n",
    "        blocktime = time.time() \n",
    "    \n",
    "print(\"Run took {0:.3f}\".format(time.time()-runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(total_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_maps.append(maze.value_map.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.nanmax(val_maps)\n",
    "\n",
    "plotrows = 4\n",
    "plotcols = 5\n",
    "fig, axes = plt.subplots(nrows=plotrows, ncols=plotcols, sharex=True, sharey =True)\n",
    "items = np.linspace(0, len(val_maps)-1, plotrows*plotcols)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    data = val_maps[int(items[i])]\n",
    "    #im = ax.pcolor(data, cmap= 'Spectral_r', vmin=np.nanmin(data), vmax=np.nanmax(data))\n",
    "    im = ax.pcolor(data, cmap= 'Spectral_r', vmin=0, vmax=50)\n",
    "    im.cmap.set_under('white')\n",
    "    ax.axis('off')\n",
    "    ax.annotate('*', np.add(maze.rwd_loc, (-0, 1.5)), color='k')\n",
    "    ax.set_title('{}'.format(int(items[i])))\n",
    "\n",
    "axes[0,0].invert_yaxis()\n",
    "\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "if mazetype == 'none':\n",
    "    plt.savefig('./figures/grid_obs{}_valuemap.svg'.format(obs_rho), format='svg')\n",
    "else: \n",
    "    plt.savefig('./figures/{}_valuemap.svg'.format(mazetype), format='svg')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = val_maps[-1]\n",
    "plt.imshow(data, vmin=np.nanmin(data), vmax=np.nanmax(data), cmap='jet', interpolation='none')\n",
    "#plt.imshow(data, vmin=1.32, vmax=1.5, cmap='jet', interpolation='none')\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs, state_value = model(Variable(state))\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "    return action.data[0]\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    rewards = []\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    for (log_prob, value), r in zip(saved_actions, rewards):\n",
    "        reward = r - value.data[0, 0]\n",
    "        policy_losses.append(-log_prob * reward)\n",
    "        value_losses.append(F.smooth_l1_loss(value, Variable(torch.Tensor([r]))))\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.cat(policy_losses).sum() + torch.cat(value_losses).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state = env.reset()\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if args.render:\n",
    "                env.render()\n",
    "            model.rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = running_reward * 0.99 + t * 0.01\n",
    "        finish_episode()\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "                i_episode, t, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up network\n",
    "n_in = 500\n",
    "n_out = 6\n",
    "dims = [n_in, n_out]\n",
    "\n",
    "in_layer = Variable(torch.rand(n_in))\n",
    "W = nn.Linear(n_in, n_out)\n",
    "policy = F.softmax(W(in_layer))\n",
    "print(out_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
