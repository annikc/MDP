{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "%matplotlib inline \n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAFDCAYAAABlQfaWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0cd97fa850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#========================\n",
    "# Environment Parameters\n",
    "#======================== \n",
    "height = 15\n",
    "width = 15\n",
    "\n",
    "mazetype = 'none'\n",
    "#obstacle density\n",
    "obs_rho = 0\n",
    "\n",
    "#place cells\n",
    "place_cells = 700\n",
    "#place cell full width half max (must be <1)\n",
    "fwhm = .2\n",
    "\n",
    "#make environment\n",
    "maze = gridworld([height, width],rho=obs_rho,num_pc=place_cells, pc_fwhm=fwhm, maze_type=mazetype)\n",
    "\n",
    "make_env_plots(maze,1,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1)\n",
    "                         \n",
    "                         \n",
    "                         \n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Scale(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "# This is based on the code from gym.\n",
    "screen_width = 600\n",
    "\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2, 0, 1))  # transpose into torch order (CHW)\n",
    "    # Strip off the top and bottom of the screen\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescare, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).type(Tensor)\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "\n",
    "model = DQN()\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "#=====================\n",
    "# Training Parameters\n",
    "#=====================\n",
    "#trial parameters\n",
    "NUM_TRIALS = 1000\n",
    "\n",
    "NUM_EVENTS = 300\n",
    "\n",
    "discount_factor = 0.98\n",
    "port_shift = 'none'\n",
    "\n",
    "\n",
    "#gradient descent learning rate\n",
    "eta = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single run of NUM_TRIALS each with NUM_EVENTS\n",
    "tf.reset_default_graph()\n",
    "\n",
    "dims = [len(maze.net_state[0]),len(maze.actionlist)]\n",
    "\n",
    "myAgent = agent(lr=eta, dims=dims)\n",
    "#tf.summary.FileWriter('./outputs/maze3/', graph=tf.get_default_graph())\n",
    "\n",
    "print_freq = 1./10\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    steps_rwd = []\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    pGrad_buf = sess.run(myAgent.policy_vars)\n",
    "    vGrad_buf = sess.run(myAgent.value_vars)\n",
    "    \n",
    "    total_reward = []\n",
    "    trialtime = []\n",
    "    total_loss = [[],[]]\n",
    "\n",
    "    blocktime = time.time()\n",
    "    val_maps = []\n",
    "    \n",
    "    p_field = np.zeros((height, width), dtype=[('action_taken', 'i4'),('taken_prob', 'f4'), ('likely_action', 'i4'),('likely_prob', 'f4'), ('timestep', 'i4')])\n",
    "    \n",
    "    print strftime(\"%a, %d %b %Y %H:%M:%S +0000\", time.localtime())\n",
    "    for i in xrange(NUM_TRIALS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        _ep_buffer = []\n",
    "        reward_sum = 0\n",
    "\n",
    "        # reset gradient buffers\n",
    "        for ix,grad in enumerate(pGrad_buf):\n",
    "            pGrad_buf[ix] = grad * 0\n",
    "        for ix,grad in enumerate(vGrad_buf):\n",
    "            vGrad_buf[ix] = grad * 0\n",
    "\n",
    "        # reset the environment\n",
    "        maze.start_trial()\n",
    "        maze.value_map = maze.init_value_map\n",
    "        \n",
    "        state = maze.net_state\n",
    "        \n",
    "        for j in xrange(NUM_EVENTS): \n",
    "            # get policy and value estimate in order to select next action\n",
    "            tfprob_, val_ = sess.run([myAgent.policy, myAgent.value], feed_dict={myAgent.input_layer:state})\n",
    "\n",
    "            # process tensorflow lists to get right shape\n",
    "            tfprob = tfprob_[0]\n",
    "            val = val_[0][0]\n",
    "            #print maze.cur_state, val, \"###\"\n",
    "            maze.value_map[maze.cur_state[1]][maze.cur_state[0]] = val\n",
    "\n",
    "            # select action\n",
    "            choice = np.random.choice(np.arange(len(tfprob)), 1, p=tfprob)[0]\n",
    "            action = maze.actionlist[choice]\n",
    "\n",
    "\n",
    "            p_field[maze.cur_state[1]][maze.cur_state[0]] = (choice, list(tfprob)[choice], list(tfprob).index(max(list(tfprob))), max(list(tfprob)), i)\n",
    "            \n",
    "            # get new state of the environment and reward from action \n",
    "            if j < NUM_EVENTS:\n",
    "                next_state = maze.move(action)\n",
    "\n",
    "            rwd = maze.rwd\n",
    "\n",
    "            # store buffer of agents experiences so that later we can calulate returns/etc. backwards through time\n",
    "            _ep_buffer.append([state,choice,rwd,next_state, val])\n",
    "\n",
    "            # update state\n",
    "            state = next_state\n",
    "\n",
    "            reward_sum += rwd\n",
    "            \n",
    "            #if maze.done == True: \n",
    "            #    trialtime.append(j)\n",
    "            #    break\n",
    "            #elif j == NUM_EVENTS-1:\n",
    "            #    trialtime.append(NUM_EVENTS)\n",
    "        # make data storage useable type \n",
    "        _ep_buffer = np.array(_ep_buffer)\n",
    "        # compute returns\n",
    "        _returns = discount_rwds(_ep_buffer[:,2], gamma=discount_factor)\n",
    "\n",
    "        feed_dict = {myAgent.return_holder:_returns, myAgent.action_holder:_ep_buffer[:,1], myAgent.input_layer:np.vstack(_ep_buffer[:,0])}\n",
    "\n",
    "        # calculate gradients using the information stored in the episode buffer\n",
    "        # computed returns (backward through time)\n",
    "        # which actions were taken gives which policy unit was responsible (so grads are computed properly)\n",
    "        # pass states agent was in at each timestep through the network again to recompute the value and policy (for gradients computation in tf)\n",
    "        a_loss, c_loss = sess.run([myAgent.actor_loss, myAgent.critic_loss], feed_dict=feed_dict)\n",
    "        p_grads, v_grads = sess.run([myAgent.get_pol_grads, myAgent.get_val_grads], feed_dict=feed_dict)\n",
    "        \n",
    "        total_loss[0].append(a_loss)\n",
    "        total_loss[1].append(c_loss)\n",
    "        \n",
    "        # store gradients in gradient buffers -- not necessary for the current formulation but will be more flexible for later\n",
    "        # sorry for the additional complication \n",
    "        for idx, grad in enumerate(p_grads):\n",
    "            pGrad_buf[idx] += grad\n",
    "\n",
    "        for idx, grad in enumerate(v_grads):\n",
    "            vGrad_buf[idx] += grad\n",
    "\n",
    "        feed_dict = dict(zip(myAgent.p_gradient_holders, pGrad_buf)+zip(myAgent.v_gradient_holders, vGrad_buf))\n",
    "\n",
    "        # run gradient update operations \n",
    "        _, __ = sess.run([myAgent.update_pol, myAgent.update_val], feed_dict = feed_dict)\n",
    "\n",
    "        total_reward.append(reward_sum)\n",
    "        val_maps.append(maze.value_map.copy())\n",
    "        if saveplots:\n",
    "            if (i%100 == 0):\n",
    "                plt.clf()\n",
    "                current_cmap = plt.cm.get_cmap()\n",
    "                current_cmap.set_bad(color='white')\n",
    "                plt.imshow(maze.value_map.copy(), vmin = 0, vmax=40, cmap = 'jet', interpolation='none')\n",
    "\n",
    "                plt.annotate('*', np.add(maze.rwd_loc, (0, -0)), color='w')\n",
    "                plt.title('{}'.format(i))\n",
    "                #plt.gca().invert_yaxis()\n",
    "                plt.colorbar()\n",
    "                plt.savefig(pathvar+str(i),format='png')\n",
    "\n",
    "        # print reward measure\n",
    "        if i==1 or i%(print_freq*NUM_TRIALS)==0 or i == NUM_TRIALS-1: \n",
    "            print \"Trial {0} finished in {1:.3f}. Total reward = {2} (Avg {3:.3f})\".format(i, time.time()-start_time, reward_sum, float(reward_sum)/float(NUM_EVENTS)),\n",
    "            print \"Block took {0:.3f}\".format(time.time()-blocktime)\n",
    "            blocktime = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(total_reward, 'b-', label='Total Reward')\n",
    "#plt.axhline(y=NUM_EVENTS*(0.983), color='r', linestyle='-', label='Minimum \\nPefect Score')\n",
    "plt.legend(loc=0)\n",
    "plt.ylim([0,NUM_EVENTS])\n",
    "plt.savefig(figpath+'{}_reward.svg'.format(mazetype), format = 'svg')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(total_loss[0], 'b', label = 'Actor Loss')\n",
    "plt.title('gamma = {}'.format(discount_factor))\n",
    "plt.plot(total_loss[1], 'r', label= 'Critic Loss')\n",
    "plt.legend(loc=0)\n",
    "plt.savefig(figpath+'{}_loss.svg'.format(mazetype), format = 'svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotrows = 4\n",
    "plotcols = 5\n",
    "fig, axes = plt.subplots(nrows=plotrows, ncols=plotcols, sharex=True, sharey =True)\n",
    "items = np.linspace(0, len(val_maps)-1, plotrows*plotcols)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    data = val_maps[int(items[i])]\n",
    "    #im = ax.pcolor(data, cmap= 'Spectral_r', vmin=np.nanmin(data), vmax=np.nanmax(data))\n",
    "    im = ax.pcolor(data, cmap= 'Spectral_r', vmin=0, vmax=45)\n",
    "    im.cmap.set_under('white')\n",
    "    ax.axis('off')\n",
    "    ax.annotate('*', np.add(maze.rwd_loc, (-0, 1.5)), color='k')\n",
    "    ax.set_title('{}'.format(int(items[i])))\n",
    "\n",
    "axes[0,0].invert_yaxis()\n",
    "\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "if mazetype == 'none':\n",
    "    plt.savefig('./figures/grid_obs{}_valuemap.svg'.format(obs_rho), format='svg')\n",
    "else: \n",
    "    plt.savefig('./figures/{}_valuemap.svg'.format(mazetype), format='svg')\n",
    "plt.show()\n",
    "\n",
    "print np.nanmax(val_maps)\n",
    "\n",
    "data = val_maps[-1]\n",
    "print len(val_maps)\n",
    "plt.imshow(data, vmin=np.nanmin(data), vmax=np.nanmax(data)+0.0001, cmap='jet', interpolation='none')\n",
    "#plt.imshow(data, vmin=1.32, vmax=1.5, cmap='jet', interpolation='none')\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = val_maps[-1].copy()\n",
    "data[np.where(data>0)] = 0\n",
    "\n",
    "## Plot actual choice\n",
    "fig = plt.figure()\n",
    "\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cmx.ScalarMappable(norm = cNorm, cmap=cmap)\n",
    "\n",
    "\n",
    "ax1  = fig.add_axes([0.04, 0, 0.4, 0.85]) # [left, bottom, width, height]\n",
    "ax2   = fig.add_axes([0.47, 0, 0.4, 0.85]) # [left, bottom, width, height]\n",
    "axc = fig.add_axes([0.89, 0.125, 0.05, 0.6])\n",
    "\n",
    "cb1 = mpl.colorbar.ColorbarBase(axc, cmap=cmap, norm=cNorm)\n",
    "\n",
    "ax1.imshow(data, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax1.add_patch(patches.Circle(maze.rwd_loc, 0.35, fc='w'))\n",
    "\n",
    "ax2.imshow(data, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax2.add_patch(patches.Circle(maze.rwd_loc, 0.35, fc='w'))\n",
    "\n",
    "\n",
    "# p_field indicies\n",
    "# 0 - choice, \n",
    "# 1 - list(tfprob)[choice], \n",
    "# 2 - list(tfprob).index(max(list(tfprob))),\n",
    "# 3 - max(list(tfprob)), \n",
    "# 4 - i)\n",
    "\n",
    "for i in range(0, p_field.shape[0]):\n",
    "    for j in range(0, p_field.shape[1]):\n",
    "        dx1, dy1, head_w1, head_l1 = make_arrows(p_field[i][j][0], p_field[i][j][1]) \n",
    "        if (dx1, dy1) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal1 = scalarMap.to_rgba(p_field[i][j][1])\n",
    "            ax1.arrow(j, i, dx1, dy1, head_width =0.3, head_length =0.2, color=colorVal1, alpha = 1 - ((len(val_maps)-p_field[i][j][4])/len(val_maps)))\n",
    "            \n",
    "        dx2, dy2, head_w2, head_l2 = make_arrows(p_field[i][j][2], p_field[i][j][3])\n",
    "        if (dx2, dy2) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal2 = scalarMap.to_rgba(p_field[i][j][3])\n",
    "            ax2.arrow(j, i, dx2, dy2, head_width =0.3, head_length =0.2, color=colorVal2, alpha = 1 - ((len(val_maps)-p_field[i][j][4])/len(val_maps)))\n",
    "            \n",
    "ax1.set_title(\"Chosen Action\")\n",
    "ax2.set_title(\"Most likely choice\")\n",
    "\n",
    "plt.savefig('./figures/{}choice_field.svg'.format(mazetype),format ='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_gif(mypath, mazetype):\n",
    "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    intfiles = [int(f) for f in onlyfiles]\n",
    "    intfiles.sort()\n",
    "    if mazetype =='none':\n",
    "        gifname = './figures/gifs/grid{}.gif'.format(obs_rho)\n",
    "    else: \n",
    "        gifname = './figures/gifs/{}.gif'.format(mazetype)\n",
    "    \n",
    "\n",
    "    with imageio.get_writer(gifname, mode='I', duration=0.5) as writer:\n",
    "                for filename in intfiles:\n",
    "                    image = imageio.imread(mypath+str(filename))\n",
    "                    writer.append_data(image)\n",
    "    print \"Gif file saved at \", gifname\n",
    "\n",
    "if saveplots:\n",
    "    make_gif(pathvar, mazetype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "### Clean up this garbage heap\n",
    "saveplots = False\n",
    "\n",
    "if mazetype=='none':\n",
    "    pathvar = './valplots/grid/{}/'.format(obs_rho)\n",
    "    figpath = './figures/grid/{}/'.format(obs_rho)\n",
    "else: \n",
    "    pathvar = './valplots/{}/'.format(mazetype)\n",
    "    figpath = './figures/{}/'.format(mazetype)\n",
    "\n",
    "if not os.path.exists(figpath):\n",
    "    os.makedirs(figpath)\n",
    "if saveplots:\n",
    "    if not os.path.exists(pathvar):\n",
    "        os.makedirs(pathvar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
