{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from importlib import reload\n",
    "from modules import * \n",
    "import csv\n",
    "fig_savedir = '../data/figures/'\n",
    "\n",
    "grid_params = {\n",
    "    'y_height':   10,\n",
    "    'x_width':    10,\n",
    "    'walls':      False,\n",
    "    'rho':        0,\n",
    "    'maze_type':  'none',\n",
    "    'port_shift': 'none'\n",
    "}\n",
    "\n",
    "\n",
    "agent_params = {\n",
    "    'load_model':   False,\n",
    "    'load_dir':     '../data/outputs/gridworld/MF{}{}training.pt'.format(grid_params['x_width'],grid_params['y_height']),\n",
    "    'action_dims':  6, #=len(maze.actionlist)\n",
    "    'lin_dims':     500,\n",
    "    'batch_size':   1,\n",
    "    'gamma':        0.98, #discount factor\n",
    "    'eta':          5e-4,\n",
    "    'temperature':  1,\n",
    "    'use_EC':       False,\n",
    "    'cachelim':     100, # memory limit should be ~75% of #actions x #states\n",
    "    'state_type':   'conv'\n",
    "}\n",
    "\n",
    "run_dict = {\n",
    "    'NUM_EVENTS':   150,\n",
    "    'NUM_TRIALS':   5000,\n",
    "    'print_freq':   1/10,\n",
    "    'total_loss':   [[],[]],\n",
    "    'total_reward': [],\n",
    "    'val_maps':     [],\n",
    "    'policies':     [{},{}],\n",
    "    'deltas':       [],\n",
    "    'spots':        [],\n",
    "    'vls':          []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 5)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make environment\n",
    "maze = eu.gridworld(grid_params)\n",
    "maze.set_rwd([(int(grid_params['y_height']/2),int(grid_params['x_width']/2))])\n",
    "env = eu.gymworld(maze) # openAI-like wrapper \n",
    "\n",
    "#update agent params dictionary with layer sizes appropriate for environment \n",
    "agent_params = sg.gen_input(maze, agent_params)\n",
    "MF,opt = ac.make_agent(agent_params, freeze=False)\n",
    "gp.make_env_plots(maze,env=True)\n",
    "\n",
    "\n",
    "agent_params['cachelim'] = int(0.5*np.prod(maze.grid.shape))\n",
    "\n",
    "EC = ec.ep_mem(MF,agent_params['cachelim']) \n",
    "print(maze.rwd_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function for runs with episodic mem and without -- take use_EC as a param\n",
    "# assume just for conv inputs \n",
    "def run_trials(run_dict, use_EC, **kwargs):\n",
    "    save_data  = kwargs.get('save', True)\n",
    "    NUM_TRIALS = run_dict['NUM_TRIALS']\n",
    "    NUM_EVENTS = run_dict['NUM_EVENTS']\n",
    "    \n",
    "    blocktime = time.time()\n",
    "    \n",
    "    if use_EC:\n",
    "        EC.reset_cache()\n",
    "        memory_buffer = [[],[],[]] # [timestamp, state_t, a_t, readable_state]\n",
    "        run_dict['total_loss']   = [[],[]]\n",
    "        run_dict['total_reward'] = []\n",
    "        run_dict['track_cs']     = [[],[]]\n",
    "        run_dict['rpe']          = np.zeros(maze.grid.shape)\n",
    "        \n",
    "        reward    = 0\n",
    "        timestamp = 0\n",
    "        tslr      = np.nan_to_num(np.inf)\n",
    "        \n",
    "        for trial in range(NUM_TRIALS):\n",
    "            trialstart_stamp = timestamp\n",
    "\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "\n",
    "            env.reset() \n",
    "            \n",
    "            state = torch.Tensor(sg.get_frame(maze))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "\n",
    "            for event in range(NUM_EVENTS):\n",
    "                if trial is not 0:\n",
    "                    #compute confidence in MFC\n",
    "                    if event in [0,1]:\n",
    "                        MF_cs = EC.make_pvals(tslr,envelope=10)\n",
    "                    else: \n",
    "                        MF_cs = EC.make_pvals(tslr,envelope=10, pol_id = pol_flag, mfc=MF_cs)\n",
    "                    # pass state through EC module\n",
    "                \n",
    "                policy_, value_, lin_act_ = MF(state, temperature = 1)\n",
    "                lin_act = tuple(np.round(lin_act_.data[0].numpy(),4))\n",
    "                \n",
    "                if trial == 0: \n",
    "                    choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "                else:\n",
    "                    if event is not 0:\n",
    "                        ec_pol = torch.from_numpy(EC.recall_mem(lin_act, timestamp, env=150))\n",
    "                        candidate_policies = [policy_, ec_pol]\n",
    "                        pol_choice = np.random.choice([0,1], p=[MF_cs, 1-MF_cs])\n",
    "                        pol = candidate_policies[pol_choice]\n",
    "                        if pol_choice == 0:\n",
    "                            pol_flag = 'MF'\n",
    "                        else:\n",
    "                            pol_flag = 'EC'\n",
    "                        choice, policy, value = ac.select_ec_action(MF, policy_, value_, pol)\n",
    "                    else:\n",
    "                        choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "                \n",
    "                memory_buffer[3].append(maze.cur_state)\n",
    "                \n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "                if event is not 0:\n",
    "                    if reward == 1:\n",
    "                        tslr = 0\n",
    "                    else:\n",
    "                        tslr += 1\n",
    "                    \n",
    "                    run_dict['track_cs'][0].append(tslr)\n",
    "                    run_dict['track_cs'][1].append(MF_cs)\n",
    "                \n",
    "                MF.rewards.append(reward)\n",
    "                \n",
    "                memory_buffer[0].append(timestamp)\n",
    "                memory_buffer[1].append(lin_act)\n",
    "                memory_buffer[2].append(choice)\n",
    "                \n",
    "                EC.add_mem(add_mem_dict)#add event to memory cache\n",
    "                \n",
    "                # because we need to include batch size of 1 \n",
    "                state = torch.Tensor(sg.get_frame(maze))\n",
    "                reward_sum += reward\n",
    "\n",
    "                v_last = value\n",
    "                timestamp += 1\n",
    "\n",
    "            p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "            \n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                #run_dict['total_loss'][0].append(p_loss.data[0])\n",
    "                #run_dict['total_loss'][1].append(v_loss.data[0])\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "\n",
    "            if trial ==0 or trial%10==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()\n",
    "\n",
    "    else:\n",
    "        run_dict['rpe'] = np.zeros(maze.grid.shape)\n",
    "        run_dict['total_loss'] =  [[],[]]\n",
    "        run_dict['total_reward'] = []\n",
    "        for trial in range(NUM_TRIALS):\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "            track_deltas = []\n",
    "            track_spots  = []\n",
    "            visited_locs = []\n",
    "\n",
    "            env.reset() \n",
    "            state = torch.Tensor(sg.get_frame(maze))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "\n",
    "            for event in range(NUM_EVENTS):\n",
    "                policy_, value_ = MF(state, agent_params['temperature'])[0:2]\n",
    "                choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "\n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "                MF.rewards.append(reward)\n",
    "                #compute eligibility trace/rpe approximation\n",
    "                delta = reward + agent_params['gamma']*value - v_last  \n",
    "                state = torch.Tensor(sg.get_frame(maze))\n",
    "                run_dict['rpe'][maze.cur_state[1]][maze.cur_state[0]] = delta\n",
    "                \n",
    "                reward_sum += reward\n",
    "                v_last = value\n",
    "\n",
    "            p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "            \n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                run_dict['total_loss'][0].append(p_loss.item())\n",
    "                run_dict['total_loss'][1].append(v_loss.item())\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "\n",
    "            if trial ==0 or trial%100==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3996c9019818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#false = mf only, true = w ec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-989fafd4c3e1>\u001b[0m in \u001b[0;36mrun_trials\u001b[0;34m(run_dict, use_EC, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mtimestamp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mp_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msave_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/LINC Lab Documents/Code/MEMRL/rl_network/actorcritic.py\u001b[0m in \u001b[0;36mfinish_trial\u001b[0;34m(model, discount_factor, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;31m#total_loss = p_loss + v_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "run_trials(run_dict, True) #false = mf only, true = w ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      " 0\n",
      "[13:00:53]  Trial 1 TotRew = 0 (0.109s)\n",
      "[13:01:08]  Trial 101 TotRew = 0 (14.913s)\n",
      "[13:01:22]  Trial 201 TotRew = 0 (14.224s)\n",
      "[13:01:36]  Trial 301 TotRew = 0 (14.204s)\n",
      "[13:01:51]  Trial 401 TotRew = 0 (14.841s)\n",
      "[13:02:05]  Trial 501 TotRew = 0 (14.007s)\n",
      "[13:02:18]  Trial 601 TotRew = 0 (12.898s)\n",
      "[13:02:35]  Trial 701 TotRew = 0 (16.662s)\n",
      "[13:03:03]  Trial 801 TotRew = 0 (28.466s)\n",
      "[13:03:17]  Trial 901 TotRew = 0 (13.593s)\n",
      "[13:03:31]  Trial 1001 TotRew = 0 (13.813s)\n",
      "[13:03:45]  Trial 1101 TotRew = 0 (14.316s)\n",
      "[13:03:59]  Trial 1201 TotRew = 0 (14.339s)\n",
      "[13:04:14]  Trial 1301 TotRew = 0 (14.385s)\n",
      "[13:04:28]  Trial 1401 TotRew = 0 (14.500s)\n",
      "[13:04:42]  Trial 1501 TotRew = 0 (14.053s)\n",
      "[13:04:56]  Trial 1601 TotRew = 0 (13.856s)\n",
      "[13:05:11]  Trial 1701 TotRew = 0 (14.739s)\n",
      "[13:05:25]  Trial 1801 TotRew = 0 (14.571s)\n",
      "[13:05:39]  Trial 1901 TotRew = 0 (13.521s)\n",
      "[13:05:52]  Trial 2001 TotRew = 0 (13.289s)\n",
      "[13:06:06]  Trial 2101 TotRew = 0 (13.309s)\n",
      "[13:06:19]  Trial 2201 TotRew = 0 (13.255s)\n",
      "[13:06:33]  Trial 2301 TotRew = 0 (14.044s)\n",
      "[13:06:47]  Trial 2401 TotRew = 0 (14.318s)\n",
      "[13:07:01]  Trial 2501 TotRew = 0 (13.447s)\n",
      "[13:07:14]  Trial 2601 TotRew = 0 (13.754s)\n",
      "[13:07:29]  Trial 2701 TotRew = 0 (14.743s)\n",
      "[13:07:43]  Trial 2801 TotRew = 0 (13.524s)\n",
      "[13:07:57]  Trial 2901 TotRew = 0 (13.957s)\n",
      "[13:08:11]  Trial 3001 TotRew = 0 (14.371s)\n",
      "[13:08:27]  Trial 3101 TotRew = 0 (15.790s)\n",
      "[13:08:41]  Trial 3201 TotRew = 0 (14.656s)\n",
      "[13:08:55]  Trial 3301 TotRew = 0 (13.227s)\n",
      "[13:09:08]  Trial 3401 TotRew = 0 (13.269s)\n",
      "[13:09:21]  Trial 3501 TotRew = 0 (13.573s)\n",
      "[13:09:36]  Trial 3601 TotRew = 0 (14.995s)\n",
      "[13:09:50]  Trial 3701 TotRew = 0 (13.443s)\n",
      "[13:10:03]  Trial 3801 TotRew = 0 (13.359s)\n",
      "[13:10:16]  Trial 3901 TotRew = 0 (13.017s)\n",
      "[13:10:30]  Trial 4001 TotRew = 0 (13.466s)\n",
      "[13:10:44]  Trial 4101 TotRew = 0 (13.769s)\n",
      "[13:10:57]  Trial 4201 TotRew = 0 (13.654s)\n",
      "[13:11:12]  Trial 4301 TotRew = 0 (14.471s)\n",
      "[13:11:25]  Trial 4401 TotRew = 0 (13.359s)\n",
      "[13:11:38]  Trial 4501 TotRew = 0 (12.879s)\n",
      "[13:11:51]  Trial 4601 TotRew = 0 (13.186s)\n",
      "[13:12:04]  Trial 4701 TotRew = 0 (13.097s)\n",
      "[13:12:18]  Trial 4801 TotRew = 0 (13.715s)\n",
      "[13:12:33]  Trial 4901 TotRew = 0 (14.835s)\n",
      "[13:12:46]  Trial 5000 TotRew = 0 (13.543s)\n",
      "-----\n",
      " 1\n",
      "[13:12:46]  Trial 1 TotRew = 0 (0.157s)\n",
      "[13:13:00]  Trial 101 TotRew = 0 (13.904s)\n",
      "[13:13:14]  Trial 201 TotRew = 0 (13.695s)\n",
      "[13:13:28]  Trial 301 TotRew = 0 (14.284s)\n",
      "[13:13:43]  Trial 401 TotRew = 0 (14.495s)\n",
      "[13:13:58]  Trial 501 TotRew = 0 (14.915s)\n",
      "[13:14:12]  Trial 601 TotRew = 0 (14.487s)\n",
      "[13:14:27]  Trial 701 TotRew = 0 (15.015s)\n",
      "[13:14:42]  Trial 801 TotRew = 0 (14.935s)\n",
      "[13:14:57]  Trial 901 TotRew = 0 (15.087s)\n",
      "[13:15:12]  Trial 1001 TotRew = 0 (14.778s)\n",
      "[13:15:27]  Trial 1101 TotRew = 0 (14.974s)\n",
      "[13:15:41]  Trial 1201 TotRew = 0 (14.245s)\n",
      "[13:15:56]  Trial 1301 TotRew = 0 (14.732s)\n",
      "[13:16:11]  Trial 1401 TotRew = 0 (15.484s)\n",
      "[13:16:26]  Trial 1501 TotRew = 0 (14.255s)\n",
      "[13:16:41]  Trial 1601 TotRew = 0 (15.651s)\n",
      "[13:16:55]  Trial 1701 TotRew = 0 (13.847s)\n",
      "[13:17:10]  Trial 1801 TotRew = 0 (14.845s)\n",
      "[13:17:24]  Trial 1901 TotRew = 0 (13.956s)\n",
      "[13:17:38]  Trial 2001 TotRew = 0 (13.618s)\n",
      "[13:17:51]  Trial 2101 TotRew = 1 (13.420s)\n",
      "[13:18:04]  Trial 2201 TotRew = 2 (13.396s)\n",
      "[13:18:18]  Trial 2301 TotRew = 0 (13.480s)\n",
      "[13:18:33]  Trial 2401 TotRew = 0 (14.834s)\n",
      "[13:18:46]  Trial 2501 TotRew = 0 (13.694s)\n",
      "[13:19:01]  Trial 2601 TotRew = 0 (14.976s)\n",
      "[13:19:17]  Trial 2701 TotRew = 0 (15.183s)\n",
      "[13:19:31]  Trial 2801 TotRew = 0 (14.012s)\n",
      "[13:19:45]  Trial 2901 TotRew = 0 (13.934s)\n",
      "[13:19:59]  Trial 3001 TotRew = 0 (14.551s)\n",
      "[13:20:13]  Trial 3101 TotRew = 0 (13.821s)\n",
      "[13:20:27]  Trial 3201 TotRew = 0 (14.396s)\n",
      "[13:20:42]  Trial 3301 TotRew = 0 (14.343s)\n",
      "[13:20:56]  Trial 3401 TotRew = 1 (14.191s)\n",
      "[13:21:10]  Trial 3501 TotRew = 0 (14.501s)\n",
      "[13:21:24]  Trial 3601 TotRew = 0 (13.345s)\n",
      "[13:21:37]  Trial 3701 TotRew = 1 (13.006s)\n",
      "[13:21:50]  Trial 3801 TotRew = 0 (13.077s)\n",
      "[13:22:04]  Trial 3901 TotRew = 0 (13.794s)\n",
      "[13:22:19]  Trial 4001 TotRew = 0 (15.373s)\n",
      "[13:22:33]  Trial 4101 TotRew = 0 (14.394s)\n",
      "[13:22:48]  Trial 4201 TotRew = 0 (14.129s)\n",
      "[13:23:02]  Trial 4301 TotRew = 0 (14.749s)\n",
      "[13:23:16]  Trial 4401 TotRew = 0 (14.053s)\n",
      "[13:23:31]  Trial 4501 TotRew = 0 (14.792s)\n",
      "[13:23:45]  Trial 4601 TotRew = 0 (14.088s)\n",
      "[13:23:59]  Trial 4701 TotRew = 0 (13.912s)\n",
      "[13:24:29]  Trial 4801 TotRew = 0 (30.273s)\n",
      "[13:24:43]  Trial 4901 TotRew = 0 (13.993s)\n",
      "[13:24:57]  Trial 5000 TotRew = 0 (13.284s)\n"
     ]
    }
   ],
   "source": [
    "#### run a bunch of trials to get rwds to average \n",
    "ec_sim = False\n",
    "for i in range(2):\n",
    "    print('-----\\n',i)\n",
    "    MF,opt = ac.make_agent(agent_params, freeze=False)\n",
    "    if ec_sim:\n",
    "        EC = ec.ep_mem(MF,agent_params['cachelim']) \n",
    "        run_trials(run_dict, True)\n",
    "    else:\n",
    "        run_trials(run_dict, False)\n",
    "    with open('1000_1posttrain_mf.csv', 'a+') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerows([run_dict['total_reward']])\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(run_dict['total_reward'], label='r')\n",
    "#plt.plot(run_dict['total_loss'][0], label='p')\n",
    "#plt.plot(run_dict['total_loss'][1], label='v')\n",
    "plt.legend(bbox_to_anchor=(1.1,1.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early_learn_mf_only = run_dict['total_reward']\n",
    "#early_learn_w_ec = run_dict['total_reward']\n",
    "#late_learn_mf_only = run_dict['total_reward']\n",
    "#late_learn_w_ec = run_dict['total_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(run_dict['total_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, sharex = True, sharey=True)\n",
    "ax[0].plot(early_learning_w_ec, 'b', alpha =0.7, label = 'With EC')\n",
    "ax[0].plot(early_learn_mf_only, 'k--', alpha = 0.7, label=\"MF only\")\n",
    "\n",
    "ax[1].plot(late_learn_w_ec, 'r', label='With EC')\n",
    "ax[1].plot(late_learn_mf_only, 'k--', alpha = 0.7, label ='MF only')\n",
    "\n",
    "ax[0].legend(bbox_to_anchor = (1.0,.75))\n",
    "ax[1].legend(bbox_to_anchor = (1.0,.75))\n",
    "fig.savefig('compare.svg', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rpe) \n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap = cmap)\n",
    "\n",
    "ax1 = fig.add_axes([0.04, 0, 0.85, 0.85])\n",
    "axc = fig.add_axes([0.8, 0, 0.05, 0.85])\n",
    "\n",
    "cb1 = colorbar.ColorbarBase(axc, cmap = cmap, norm = cNorm)\n",
    "ax1.imshow(maze.grid, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax1.add_patch(patches.Circle(maze.rwd_loc[0], 0.35, fc='w'))\n",
    "\n",
    "for entry in compare_policies.keys():\n",
    "    x = entry[0]\n",
    "    y = entry[1]\n",
    "    \n",
    "    ec_policy = compare_policies[entry][1][0] - compare_policies[entry][0][0]\n",
    "    action = np.argmax(ec_policy)\n",
    "    prob = max(ec_policy)\n",
    "    \n",
    "    dx1, dy1, head_w, head_l = gp.make_arrows(action,prob)\n",
    "    if prob > 1/4:\n",
    "        if (dx1, dy1) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal1 = scalarMap.to_rgba(prob)\n",
    "            ax1.arrow(x,y, dx1, dy1, head_width=0.3, head_length=0.2, color = colorVal1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "ax1.invert_yaxis()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = compare_policies[(7,5)][1]\n",
    "for i in range(len(x)):\n",
    "    plt.bar(np.arange(6)+(0.1*i), x[i], width = 0.1, alpha = 0.3)\n",
    "    plt.bar(np.arange(6), np.ones(6)*0.16666667, width =0.1, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(DND.cache_list))\n",
    "def cosine_sim(mem_dict, key, **kwargs):\n",
    "    similarity_threshold = kwargs.get('threshold', 0.9)\n",
    "\n",
    "    mem_cache = np.asarray(list(mem_dict.keys()))\n",
    "    print(mem_cache.shape)\n",
    "    entry = np.asarray(key)\n",
    "\n",
    "    mqt = np.dot(mem_cache, entry)\n",
    "    norm = np.linalg.norm(mem_cache, axis=1) * np.linalg.norm(entry)\n",
    "\n",
    "    cosine_similarity = mqt / norm\n",
    "\n",
    "    index = np.argmax(cosine_similarity)\n",
    "    similar_activity = mem_cache[index]\n",
    "    if max(cosine_similarity) >= similarity_threshold:\n",
    "        return similar_activity, index, max(cosine_similarity)\n",
    "\n",
    "    else:\n",
    "        # print('max memory similarity:', max(cosine_similarity))\n",
    "        return [], [], max(cosine_similarity)\n",
    "\n",
    "def make_pvals(p, **kwargs):\n",
    "    envelope = kwargs.get('envelope', 50)\n",
    "    return np.round(1 / np.cosh(p / envelope),8)\n",
    "\n",
    "def softmax(x, T=1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp((x - np.max(x))/T)\n",
    "    return e_x / e_x.sum(axis=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_key, ind, max_cs = cosine_sim(DND.cache_list, sts[0])\n",
    "print('sim', max_cs)\n",
    "memory = np.nan_to_num(DND.cache_list[tuple(state_key)][0])\n",
    "deltas = memory[:,0]\n",
    "print(deltas)\n",
    "rec_times = memory[:,1]\n",
    "print(\"rtimes\",rec_times)\n",
    "times        = abs(timestep - memory[:,1])\n",
    "print('tiems', times)\n",
    "pv =make_pvals(times)\n",
    "print(pv)\n",
    "\n",
    "mult = np.multiply(deltas, pv)\n",
    "print(softmax(max_cs*mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in sts[0:10]:\n",
    "    timestep = 1000\n",
    "    a = DND.recall_mem(key=i,timestep=timestep,env=100)\n",
    "    print(a,\"----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs =['r','g','b','c', 'y','pink']\n",
    "'''\n",
    "r = list(self.cache_list.keys())\n",
    "g = [t for e, t in self.cache_list.values()]\n",
    "b = lp = persistence_.index(min(persistence_))\n",
    "c = old_activity = cache_keys[lp]\n",
    "y = del self.cache_list[old_activity]\n",
    "'''\n",
    "plt.figure()\n",
    "for i in range(len(EC.stupid_df)):\n",
    "    xs = np.arange(len(EC.stupid_df[i]))\n",
    "    ys = EC.stupid_df[i]\n",
    "    \n",
    "    plt.scatter(xs, ys, c=cs[i], alpha=0.3)\n",
    "plt.ylim([-0.00002,0.00002])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(run_dict['total_reward'])\n",
    "plt.ylim([0,run_dict['NUM_EVENTS']])\n",
    " \n",
    "plt.figure(2)\n",
    "plt.plot(run_dict['total_loss'][0], label = 'pol')\n",
    "plt.plot(run_dict['total_loss'][1], label = 'val')\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "plt.close()\n",
    "#gp.print_value_maps(maze, run_dict['val_maps'], maps=0, val_range=(-1,50), save_dir=fig_savedir, title='Value Map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.torch.save(MF,agent_params['load_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
