{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from importlib import reload\n",
    "from modules import * \n",
    "fig_savedir = '../data/figures/'\n",
    "\n",
    "grid_params = {\n",
    "    'y_height':   20,\n",
    "    'x_width':    20,\n",
    "    'walls':      False,\n",
    "    'rho':        0,\n",
    "    'maze_type':  'none',\n",
    "    'port_shift': 'none'\n",
    "}\n",
    "\n",
    "\n",
    "agent_params = {\n",
    "    'load_model':   True,\n",
    "    'load_dir':     '../data/outputs/gridworld/MF{}{}training.pt'.format(grid_params['x_width'],grid_params['y_height']),\n",
    "    'action_dims':  6, #=len(maze.actionlist)\n",
    "    'lin_dims':     500,\n",
    "    'batch_size':   1,\n",
    "    'gamma':        0.98, #discount factor\n",
    "    'eta':          5e-4,\n",
    "    'temperature':  1,\n",
    "    'use_EC':       False,\n",
    "    'cachelim':     100, # memory limit should be ~75% of #actions x #states\n",
    "    'state_type':   'conv'\n",
    "}\n",
    "\n",
    "run_dict = {\n",
    "    'NUM_EVENTS':   150,\n",
    "    'NUM_TRIALS':   5000,\n",
    "    'print_freq':   1/10,\n",
    "    'total_loss':   [[],[]],\n",
    "    'total_reward': [],\n",
    "    'val_maps':     [],\n",
    "    'policies':     [{},{}],\n",
    "    'deltas':       [],\n",
    "    'spots':        [],\n",
    "    'vls':          []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annik/.local/lib/python3.6/site-packages/torch/serialization.py:425: SourceChangeWarning: source code of class 'actorcritic.AC_Net' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make environment\n",
    "maze = eu.gridworld(grid_params)\n",
    "maze.set_rwd([(7,5)])  #([(int(grid_params['y_height']/2),int(grid_params['x_width']/2))])\n",
    "env = eu.gymworld(maze) # openAI-like wrapper \n",
    "\n",
    "#update agent params dictionary with layer sizes appropriate for environment \n",
    "agent_params = sg.gen_input(maze, agent_params)\n",
    "MF,opt = ac.make_agent(agent_params)\n",
    "gp.make_env_plots(maze,env=True)\n",
    "\n",
    "agent_params['cachelim'] = int(0.5*np.prod(maze.grid.shape))\n",
    "\n",
    "EC = ec.ep_mem(MF,agent_params['cachelim']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function for runs with episodic mem and without -- take use_EC as a param\n",
    "# assume just for conv inputs \n",
    "def run_trials(run_dict, use_EC, **kwargs):\n",
    "    save_data  = kwargs.get('save', True)\n",
    "    NUM_TRIALS = 30#run_dict['NUM_TRIALS']\n",
    "    NUM_EVENTS = run_dict['NUM_EVENTS']\n",
    "    \n",
    "    blocktime = time.time()\n",
    "    \n",
    "    if use_EC:\n",
    "        EC.reset_cache()\n",
    "        add_mem_dict = {} #dictionary of items which get put into memory cache\n",
    "        timestamp    = 0\n",
    "        ststs = []\n",
    "        tslr = 1\n",
    "        track_cs = [[],[]]\n",
    "        compare_policies = {}\n",
    "        rpe = {}\n",
    "        reward = 0\n",
    "        for trial in range(NUM_TRIALS):\n",
    "            trialstart_stamp = timestamp\n",
    "\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "\n",
    "            env.reset() \n",
    "\n",
    "            state = ac.Variable(ac.torch.FloatTensor(sg.get_frame(maze)))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "\n",
    "            for event in range(NUM_EVENTS):\n",
    "                #compute confidence in MFC\n",
    "                MF_cs = EC.make_pvals(tslr,envelope=10)\n",
    "                \n",
    "                # pass state through EC module\n",
    "                policy_, value_, lin_act_ = MF(state, temperature = 1)\n",
    "                lin_act = tuple(np.round(lin_act_.data[0].numpy(),4))\n",
    "                if event is not 0:\n",
    "                    ec_pol = torch.from_numpy(EC.recall_mem(lin_act, timestamp, env=150))\n",
    "                    \n",
    "                    pol = np.random.choice([policy_, ec_pol], p=[MF_cs, 1-MF_cs])\n",
    "                else:\n",
    "                    pol = policy_\n",
    "                choice, policy, value = ac.select_action(MF,pol, value_)\n",
    "                \n",
    "                add_mem_dict['state'] = maze.cur_state\n",
    "                #compute eligibility trace/rpe approximation\n",
    "                delta = reward + agent_params['gamma']*value - v_last  \n",
    "                rpe[maze.cur_state] = delta\n",
    "                \n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "                if event is not 0:\n",
    "                    if reward == 1:\n",
    "                        tslr = 0\n",
    "                    else:\n",
    "                        tslr += 1\n",
    "                    \n",
    "                    if maze.cur_state == (7,5) and (7,5) in compare_policies.keys():\n",
    "                        compare_policies[maze.cur_state][0].append(policy.data.numpy())\n",
    "                        compare_policies[maze.cur_state][1].append(ec_pol.data.numpy())\n",
    "                    else:    \n",
    "                        compare_policies[maze.cur_state] = [[policy.data.numpy()],[ec_pol.data.numpy()]]\n",
    "                    \n",
    "                    track_cs[0].append(tslr)\n",
    "                    track_cs[1].append(MF_cs)\n",
    "                \n",
    "                MF.rewards.append(reward)\n",
    "                \n",
    "                \n",
    "                add_mem_dict['activity']  = lin_act\n",
    "                add_mem_dict['action']    = choice\n",
    "                add_mem_dict['delta']     = delta\n",
    "                add_mem_dict['timestamp'] = timestamp            \n",
    "                \n",
    "                EC.add_mem(add_mem_dict)#add event to memory cache\n",
    "                \n",
    "                # because we need to include batch size of 1 \n",
    "                state = ac.Variable(ac.torch.FloatTensor(sg.get_frame(maze)))\n",
    "                reward_sum += reward\n",
    "\n",
    "                v_last = value\n",
    "                timestamp += 1\n",
    "\n",
    "            del MF.rewards[:]\n",
    "            del MF.saved_actions[:]\n",
    "            #p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "            #print(tt)\n",
    "\n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                #run_dict['total_loss'][0].append(p_loss.data[0])\n",
    "                #run_dict['total_loss'][1].append(v_loss.data[0])\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "\n",
    "            if trial ==0 or trial%10==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()\n",
    "        return compare_policies, track_cs, rpe\n",
    "\n",
    "    else:\n",
    "        rpe = {}\n",
    "        for trial in range(NUM_TRIALS):\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "            track_deltas = []\n",
    "            track_spots  = []\n",
    "            visited_locs = []\n",
    "\n",
    "            env.reset() \n",
    "            state = ac.Variable(ac.torch.FloatTensor(sg.get_frame(maze)))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "\n",
    "            for event in range(NUM_EVENTS):\n",
    "                policy_, value_ = MF(state, agent_params['temperature'])[0:2]\n",
    "                choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "\n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "                MF.rewards.append(reward)\n",
    "                #compute eligibility trace/rpe approximation\n",
    "                delta = reward + agent_params['gamma']*value - v_last  \n",
    "                state = ac.Variable(ac.torch.FloatTensor(sg.get_frame(maze)))\n",
    "                rpe[maze.cur_state] = delta\n",
    "                \n",
    "                reward_sum += reward\n",
    "                v_last = value\n",
    "\n",
    "            p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "\n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                run_dict['total_loss'][0].append(p_loss.data[0])\n",
    "                run_dict['total_loss'][1].append(v_loss.data[0])\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "\n",
    "            if trial ==0 or trial%100==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()\n",
    "        return rpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../memory/episodic.py:58: RuntimeWarning: overflow encountered in cosh\n",
      "  return np.round(1 / np.cosh(p / self.memory_envelope), 8)\n",
      "../rl_network/actorcritic.py:286: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  return action.data[0], policy_.data[0], value_.data[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:56:17]  Trial 1 TotRew = 0 (4.287s)\n",
      "[18:57:12]  Trial 11 TotRew = 0 (55.203s)\n",
      "[18:58:19]  Trial 21 TotRew = 0 (66.942s)\n",
      "[18:59:22]  Trial 30 TotRew = 0 (63.049s)\n"
     ]
    }
   ],
   "source": [
    "compare_policies, track_cs, rpe = run_trials(run_dict, True)\n",
    "#rpe = run_trials(run_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(track_cs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap = cmap)\n",
    "\n",
    "ax1 = fig.add_axes([0.04, 0, 0.85, 0.85])\n",
    "axc = fig.add_axes([0.8, 0, 0.05, 0.85])\n",
    "\n",
    "cb1 = colorbar.ColorbarBase(axc, cmap = cmap, norm = cNorm)\n",
    "ax1.imshow(maze.grid, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax1.add_patch(patches.Circle(maze.rwd_loc[0], 0.35, fc='w'))\n",
    "\n",
    "for entry in compare_policies.keys():\n",
    "    x = entry[0]\n",
    "    y = entry[1]\n",
    "    \n",
    "    ec_policy = compare_policies[entry][1][0] - compare_policies[entry][0][0]\n",
    "    action = np.argmax(ec_policy)\n",
    "    prob = max(ec_policy)\n",
    "    \n",
    "    dx1, dy1, head_w, head_l = gp.make_arrows(action,prob)\n",
    "    if prob > 1/4:\n",
    "        if (dx1, dy1) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal1 = scalarMap.to_rgba(prob)\n",
    "            ax1.arrow(x,y, dx1, dy1, head_width=0.3, head_length=0.2, color = colorVal1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "ax1.invert_yaxis()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = compare_policies[(7,5)][1]\n",
    "for i in range(len(x)):\n",
    "    plt.bar(np.arange(6)+(0.1*i), x[i], width = 0.1, alpha = 0.3)\n",
    "    plt.bar(np.arange(6), np.ones(6)*0.16666667, width =0.1, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.empty((20,20))\n",
    "for i in rpe.keys():\n",
    "    x[i[1], i[0]] = rpe[i]\n",
    "\n",
    "\n",
    "plt.imshow(x, vmin=-5, vmax=5) \n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(DND.cache_list))\n",
    "def cosine_sim(mem_dict, key, **kwargs):\n",
    "    similarity_threshold = kwargs.get('threshold', 0.9)\n",
    "\n",
    "    mem_cache = np.asarray(list(mem_dict.keys()))\n",
    "    print(mem_cache.shape)\n",
    "    entry = np.asarray(key)\n",
    "\n",
    "    mqt = np.dot(mem_cache, entry)\n",
    "    norm = np.linalg.norm(mem_cache, axis=1) * np.linalg.norm(entry)\n",
    "\n",
    "    cosine_similarity = mqt / norm\n",
    "\n",
    "    index = np.argmax(cosine_similarity)\n",
    "    similar_activity = mem_cache[index]\n",
    "    if max(cosine_similarity) >= similarity_threshold:\n",
    "        return similar_activity, index, max(cosine_similarity)\n",
    "\n",
    "    else:\n",
    "        # print('max memory similarity:', max(cosine_similarity))\n",
    "        return [], [], max(cosine_similarity)\n",
    "\n",
    "def make_pvals(p, **kwargs):\n",
    "    envelope = kwargs.get('envelope', 50)\n",
    "    return np.round(1 / np.cosh(p / envelope),8)\n",
    "\n",
    "def softmax(x, T=1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp((x - np.max(x))/T)\n",
    "    return e_x / e_x.sum(axis=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_key, ind, max_cs = cosine_sim(DND.cache_list, sts[0])\n",
    "print('sim', max_cs)\n",
    "memory = np.nan_to_num(DND.cache_list[tuple(state_key)][0])\n",
    "deltas = memory[:,0]\n",
    "print(deltas)\n",
    "rec_times = memory[:,1]\n",
    "print(\"rtimes\",rec_times)\n",
    "times        = abs(timestep - memory[:,1])\n",
    "print('tiems', times)\n",
    "pv =make_pvals(times)\n",
    "print(pv)\n",
    "\n",
    "mult = np.multiply(deltas, pv)\n",
    "print(softmax(max_cs*mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in sts[0:10]:\n",
    "    timestep = 1000\n",
    "    a = DND.recall_mem(key=i,timestep=timestep,env=100)\n",
    "    print(a,\"----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs =['r','g','b','c', 'y','pink']\n",
    "'''\n",
    "r = list(self.cache_list.keys())\n",
    "g = [t for e, t in self.cache_list.values()]\n",
    "b = lp = persistence_.index(min(persistence_))\n",
    "c = old_activity = cache_keys[lp]\n",
    "y = del self.cache_list[old_activity]\n",
    "'''\n",
    "plt.figure()\n",
    "for i in range(len(EC.stupid_df)):\n",
    "    xs = np.arange(len(EC.stupid_df[i]))\n",
    "    ys = EC.stupid_df[i]\n",
    "    \n",
    "    plt.scatter(xs, ys, c=cs[i], alpha=0.3)\n",
    "plt.ylim([-0.00002,0.00002])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(run_dict['total_reward'])\n",
    "plt.ylim([0,run_dict['NUM_EVENTS']])\n",
    " \n",
    "plt.figure(2)\n",
    "plt.plot(run_dict['total_loss'][0], label = 'pol')\n",
    "plt.plot(run_dict['total_loss'][1], label = 'val')\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "plt.close()\n",
    "#gp.print_value_maps(maze, run_dict['val_maps'], maps=0, val_range=(-1,50), save_dir=fig_savedir, title='Value Map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.torch.save(MF,agent_params['load_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
