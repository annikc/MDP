{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from importlib import reload\n",
    "from modules import * \n",
    "fig_savedir = '../data/figures/'\n",
    "\n",
    "grid_params = {\n",
    "    'y_height':   20,\n",
    "    'x_width':    20,\n",
    "    'walls':      False,\n",
    "    'rho':        0,\n",
    "    'maze_type':  'none',\n",
    "    'port_shift': 'none'\n",
    "}\n",
    "\n",
    "\n",
    "agent_params = {\n",
    "    'load_model':   True,\n",
    "    'load_dir':     '../data/outputs/gridworld/MF{}{}training.pt'.format(grid_params['x_width'],grid_params['y_height']),\n",
    "    'action_dims':  6, #=len(maze.actionlist)\n",
    "    'batch_size':   1,\n",
    "    'gamma':        0.98, #discount factor\n",
    "    'eta':          5e-4,\n",
    "    'temperature':  1,\n",
    "    'use_EC':       False,\n",
    "    'cachelim':     100, # memory limit should be ~75% of #actions x #states\n",
    "    'state_type':   'conv'\n",
    "}\n",
    "\n",
    "run_dict = {\n",
    "    'NUM_EVENTS':   150,\n",
    "    'NUM_TRIALS':   5000,\n",
    "    'print_freq':   1/10,\n",
    "    'total_loss':   [[],[]],\n",
    "    'total_reward': [],\n",
    "    'val_maps':     [],\n",
    "    'policies':     [{},{}],\n",
    "    'deltas':       [],\n",
    "    'spots':        [],\n",
    "    'vls':          []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annik/.local/lib/python3.6/site-packages/torch/serialization.py:425: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/annik/.local/lib/python3.6/site-packages/torch/serialization.py:425: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/annik/.local/lib/python3.6/site-packages/torch/serialization.py:425: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/annik/.local/lib/python3.6/site-packages/torch/serialization.py:425: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "#make environment\n",
    "maze = eu.gridworld(grid_params)\n",
    "maze.set_rwd([(int(grid_params['y_height']/2),int(grid_params['x_width']/2))])\n",
    "env = eu.gymworld(maze) # openAI-like wrapper \n",
    "\n",
    "#update agent params dictionary with layer sizes appropriate for environment \n",
    "agent_params = sg.gen_input(maze, agent_params)\n",
    "\n",
    "MF,opt = ac.make_agent(agent_params)\n",
    "\n",
    "agent_params['cachelim'] = int(0.75*np.prod(maze.grid.shape))\n",
    "\n",
    "EC = ec.ep_mem(MF,agent_params['cachelim']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function for runs with episodic mem and without -- take use_EC as a param\n",
    "# assume just for conv inputs \n",
    "def run_trials(run_dict, use_EC, **kwargs):\n",
    "    save_data  = kwargs.get('save', True)\n",
    "    NUM_TRIALS = 1 #run_dict['NUM_TRIALS']\n",
    "    NUM_EVENTS = 100 #run_dict['NUM_EVENTS']\n",
    "    \n",
    "    blocktime = time.time()\n",
    "    \n",
    "    if use_EC:\n",
    "        \n",
    "        add_mem_dict = {} #dictionary of items which get put into memory cache\n",
    "        timestamp    = 0\n",
    "        \n",
    "\n",
    "        for trial in range(NUM_TRIALS):\n",
    "            tt = 0\n",
    "            trialstart_stamp = timestamp\n",
    "\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "\n",
    "            env.reset() \n",
    "\n",
    "            state = ac.Variable(ac.torch.FloatTensor(sg.get_frame(maze)))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "\n",
    "            for event in range(NUM_EVENTS):\n",
    "                # pass state through EC module\n",
    "                policy_, value_, lin_act_ = MF(state)\n",
    "                add_mem_dict['state'] = maze.cur_state\n",
    "\n",
    "                choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "                MF.rewards.append(reward)\n",
    "\n",
    "                delta = reward + agent_params['gamma']*value - v_last  #compute eligibility trace/rpe approximation\n",
    "\n",
    "                add_mem_dict['activity']  = tuple(lin_act_.view(-1).data)\n",
    "                add_mem_dict['action']    = choice\n",
    "                add_mem_dict['delta']     = delta\n",
    "                add_mem_dict['timestamp'] = timestamp            \n",
    "                \n",
    "                st = time.time()\n",
    "                EC.add_mem(add_mem_dict)#add event to memory cache\n",
    "                tx = time.time()-st\n",
    "                tt += tx\n",
    "                print(tx)\n",
    "                \n",
    "                # because we need to include batch size of 1 \n",
    "                state = ac.Variable(ac.torch.FloatTensor(sg.get_frame(maze)))\n",
    "                reward_sum += reward\n",
    "\n",
    "                v_last = value\n",
    "                timestamp += 1\n",
    "\n",
    "            p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "            print(tt)\n",
    "\n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                run_dict['total_loss'][0].append(p_loss.data[0])\n",
    "                run_dict['total_loss'][1].append(v_loss.data[0])\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "\n",
    "            if trial ==0 or trial%100==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()\n",
    "\n",
    "\n",
    "    else:\n",
    "        for trial in range(NUM_TRIALS):\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "            track_deltas = []\n",
    "            track_spots  = []\n",
    "            visited_locs = []\n",
    "\n",
    "            env.reset() \n",
    "            state = ac.Variable(ac.torch.FloatTensor(sg.get_frame(maze)))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "\n",
    "            for event in range(NUM_EVENTS):\n",
    "                policy_, value_ = MF(state, agent_params['temperature'])[0:2]\n",
    "                choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "\n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "                MF.rewards.append(reward)\n",
    "                delta = reward + agent_params['gamma']*value - v_last  #compute eligibility trace/rpe approximation\n",
    "                state = ac.Variable(ac.torch.FloatTensor(sg.get_frame(maze)))\n",
    "\n",
    "                reward_sum += reward\n",
    "                v_last = value\n",
    "\n",
    "            p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "\n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                run_dict['total_loss'][0].append(p_loss.data[0])\n",
    "                run_dict['total_loss'][1].append(v_loss.data[0])\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "\n",
    "            if trial ==0 or trial%10==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007002353668212891\n",
      "0.0006928443908691406\n",
      "0.0005285739898681641\n",
      "0.00044846534729003906\n",
      "0.00043082237243652344\n",
      "0.0004546642303466797\n",
      "0.0019047260284423828\n",
      "0.0009000301361083984\n",
      "0.0007407665252685547\n",
      "0.0006945133209228516\n",
      "0.0004391670227050781\n",
      "0.00043654441833496094\n",
      "0.00032830238342285156\n",
      "0.000331878662109375\n",
      "0.0003285408020019531\n",
      "0.00032520294189453125\n",
      "0.0002791881561279297\n",
      "0.0002796649932861328\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'list' and 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e930f6f7a090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-cd6ea87a9def>\u001b[0m in \u001b[0;36mrun_trials\u001b[0;34m(run_dict, use_EC, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_mem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_mem_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#add event to memory cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mtt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/LINC Lab Documents/Code/MEMRL/memory/episodic.py\u001b[0m in \u001b[0;36madd_mem\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    123\u001b[0m                                 \u001b[0mpersistence_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#[t for e, t in cache_items[:,1]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                                 \u001b[0mlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistence_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersistence_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                                 \u001b[0mold_activity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_activity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'list' and 'function'"
     ]
    }
   ],
   "source": [
    "a1 = run_trials(run_dict, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs =['r','g','b','c']\n",
    "plt.figure()\n",
    "for i in range(len(EC.stupid_df)):\n",
    "    xs = np.arange(len(EC.stupid_df[i]))\n",
    "    ys = EC.stupid_df[i]\n",
    "    \n",
    "    plt.scatter(xs, ys, c=cs[i], alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(run_dict['total_reward'])\n",
    "plt.ylim([0,run_dict['NUM_EVENTS']])\n",
    " \n",
    "plt.figure(2)\n",
    "plt.plot(run_dict['total_loss'][0], label = 'pol')\n",
    "plt.plot(run_dict['total_loss'][1], label = 'val')\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "plt.close()\n",
    "#gp.print_value_maps(maze, run_dict['val_maps'], maps=0, val_range=(-1,50), save_dir=fig_savedir, title='Value Map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.torch.save(MF,agent_params['load_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
