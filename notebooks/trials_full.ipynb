{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from importlib import reload\n",
    "from modules import * \n",
    "import pickle\n",
    "import csv\n",
    "fig_savedir = '../data/figures/'\n",
    "\n",
    "## TO do list in experiment.py \n",
    "## Notes:\n",
    "## Freeze weights is on to test storage into memory with fixed linear activity (ac.make_agent arg freeze = True)\n",
    "# this requires loading the saved weights (agent_params['load_model'] = True)\n",
    "# in ec.rec_mem temperature is set to 0.1 on softmax -- need to deal with this \n",
    "#\n",
    "## Need the MPLD3 stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(eu)\n",
    "grid_params = {\n",
    "    'y_height':     10, \n",
    "    'x_width':      10,\n",
    "    'walls':        False,\n",
    "    'rho':          0.05,\n",
    "    'maze_type':    'none',\n",
    "    'barheight':    4,\n",
    "    'port_shift':   'none',\n",
    "    'step_penaliz': -0.01\n",
    "    \n",
    "}\n",
    "\n",
    "#make environment\n",
    "maze = eu.gridworld(grid_params)\n",
    "gp.plot_env(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reset_agt(agent_params):\n",
    "        ## some stupid reward placement shit -- replace later\n",
    "    if agent_params['load_model'] == True:\n",
    "        if agent_params['rwd_placement'] == 'training_loc':\n",
    "            maze.set_rwd([(int(grid_params['x_width']/2), int(grid_params['y_height']/2))])\n",
    "        if agent_params['rwd_placement'] == 'moved_loc':\n",
    "            maze.set_rwd([(int(3*grid_params['x_width']/4),int(grid_params['y_height']/4))])\n",
    "    else:\n",
    "        maze.set_rwd([(int(grid_params['x_width']/2),int(grid_params['y_height']/2))])\n",
    "        \n",
    "    # make agent \n",
    "    agent_params = expt.gen_input(maze, agent_params)\n",
    "    MF,opt = ac.make_agent(agent_params, freeze=True)\n",
    "\n",
    "    if agent_params['use_EC']:\n",
    "        #agent_params['cachelim'] = int(0.5*np.prod(maze.grid.shape))\n",
    "        agent_params['EC'] = ec.ep_mem(MF,agent_params['cachelim'])\n",
    "    \n",
    "    run_dict = {}\n",
    "    run_dict = {\n",
    "        'NUM_EVENTS':   300,\n",
    "        'NUM_TRIALS':   1,\n",
    "        'environment':  maze,\n",
    "        'agent':        MF,\n",
    "        'optimizer':    opt,\n",
    "        'agt_param':    agent_params\n",
    "    }\n",
    "    \n",
    "    return run_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_params = {\n",
    "        'load_model':   True,\n",
    "        'load_dir':     '../data/outputs/gridworld/openfield{}{}.pt'.format(grid_params['x_width'],grid_params['y_height']),\n",
    "        'rwd_placement':'moved_loc',\n",
    "        'action_dims':  len(maze.actionlist),\n",
    "        'lin_dims':     500,\n",
    "        'batch_size':   1,\n",
    "        'gamma':        0.98, #discount factor\n",
    "        'eta':          5e-4,\n",
    "        'temperature':  1,\n",
    "        'use_EC':       True,\n",
    "        'cachelim':     300, # memory limit should be ~75% of #actions x #states\n",
    "        'state_type':   'conv'\n",
    "    }\n",
    "run_dict = reset_agt(agent_params)\n",
    "print(maze.rwd_loc)\n",
    "gp.plot_env(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(expt)\n",
    "#run_dict = reset_agt(agent_params)\n",
    "expt.run(run_dict,full=False, rec_mem=True, use_EC=False) ## by default runs truncated trials with MF only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ac)\n",
    "abcd = ac.mem_snapshot(maze, agent_params['EC'], 77, decay=ec.calc_env(halfmax = 200))\n",
    "gp.plot_polmap(maze,abcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv, pp = ac.snapshot(agent=run_dict['agent'], maze =run_dict['environment'])\n",
    "\n",
    "gp.plot_polmap(run_dict['environment'], pp)\n",
    "gp.plot_valmap(run_dict['environment'], vv, p_range = [0,1.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_loss = run_dict['total_loss'][0]\n",
    "value_loss = run_dict['total_loss'][1]\n",
    "tot_reward = run_dict['total_reward']\n",
    "trial_length = run_dict['trial_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ysteps = int(grid_params['y_height']/2)\n",
    "max_xsteps = int(grid_params['x_width']/2)\n",
    "\n",
    "reward_threshold = 1+ grid_params['step_penaliz']*(max_xsteps + max_ysteps)\n",
    "print(reward_threshold)\n",
    "\n",
    "fig, ax = plt.subplots(2,1, sharex=True)\n",
    "ax[0].plot(gp.running_mean(tot_reward,30))\n",
    "ax[0].plot(tot_reward, 'b', alpha = 0.2)\n",
    "ax[0].axhline(reward_threshold)\n",
    "#plt.plot(trial_length)\n",
    "ax[1].plot(policy_loss)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "rwd_mean = gp.running_mean(tot_reward,30)\n",
    "plt.figure(1)\n",
    "plt.plot(rwd_mean)\n",
    "plt.plot(tot_reward, 'b', alpha = 0.2)\n",
    "plt.axhline(reward_threshold)\n",
    "plt.savefig('rdtest.svg', format='svg')\n",
    "\n",
    "get_sd =np.where(rwd_mean > reward_threshold)[0][0] \n",
    "clipped_pol_loss = policy_loss[get_sd:-1]\n",
    "print(np.std(clipped_pol_loss))\n",
    "\n",
    "#pick = [tot_reward, policy_loss, value_loss, trial_length]\n",
    "#pickle.dump(pick, open(\"pickles/initial_training.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.torch.save(run_dict['agent'],agent_params['load_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(gp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = ac.mem_snapshot(run_dict['environment'], agent_params['EC'], trial_timestamp=1, decay=100)\n",
    "gp.plot_polmap(run_dict['environment'],ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ = 0 \n",
    "for k, v in agent_params['EC'].cache_list.items():\n",
    "    if v[2] == (10,10):\n",
    "        key_ = k\n",
    "        print(agent_params['EC'].cache_list[key_][0])\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(np.arange(6), agent_params['EC'].recall_mem(key_, 26))\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,1, sharex=True)\n",
    "ax[0].plot(policy_loss)\n",
    "ax[0].axvline(x = run_dict['NUM_TRIALS'],color='k', linestyle=':', alpha=0.5)\n",
    "ax[0].set_ylabel('Policy Loss')\n",
    "ax[1].plot(value_loss)\n",
    "ax[1].axvline(x = run_dict['NUM_TRIALS'],color='k', linestyle=':', alpha=0.5)\n",
    "ax[1].set_ylabel('Value Loss')\n",
    "\n",
    "#ax[0].arrow(2000, max(policy_loss), 0, -600, head_width=50, head_length=100, fc='k', ec='k')\n",
    "#ax[1].arrow(2000, max(value_loss), 0, -600, head_width=50, head_length=100, fc='k', ec='k')\n",
    "\n",
    "#plt.savefig('../data/figures/loss_after_trunc_training.svg', format='svg')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -1\n",
    "print(value_loss[x])\n",
    "print(policy_loss[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv, pp = ac.snapshot(agent=run_dict['agent'], maze =run_dict['environment'])\n",
    "reload(gp)\n",
    "gp.plot_polmap(run_dict['environment'], pp)\n",
    "gp.plot_valmap(run_dict['environment'], vv, p_range=[0,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pvals(p, **kwargs):\n",
    "    envelope = kwargs.get('envelope', 50)\n",
    "    mfc      = kwargs.get('conf_score', 1)\n",
    "    return mfc*np.round(1 / np.cosh(p / envelope), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = np.linspace(1,200, 100)#np.sort(np.random.randint(1,100,10))\n",
    "b = 10*np.arange(10)+5\n",
    "for i in b:\n",
    "    a_p = test_pvals(a, envelope=i)\n",
    "    plt.plot(a, a_p, '-', label=f'{i}')\n",
    "plt.legend(loc=0)\n",
    "#ax[1].plot(a_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EC = agent_params['EC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "meas_time = time.time()\n",
    "x = np.asarray(list(agent_params['EC'].cache_list.keys()))\n",
    "#x = np.asarray([*agent_params['EC'].cache_list.keys()])\n",
    "print(time.time() - meas_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ec)\n",
    "test_case = x[0]+ 0.3*(np.random.randn(500))\n",
    "key, index, sim = EC.cosine_sim(test_case)\n",
    "print(key[0:10], index, sim)\n",
    "EC.recall_mem(key, timestep=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pvals(p, envelope):\n",
    "    return np.round(1 / np.cosh(p / envelope), 8)\n",
    "## try to plot relationship of envelope and half max x value \n",
    "bunchadots = np.linspace(0, 1000, 10000)\n",
    "envs = np.arange(50, step = 2)+1\n",
    "\n",
    "cp = sns.color_palette(n_colors=len(envs))\n",
    "\n",
    "tracking = []\n",
    "for ind, i in enumerate(envs):\n",
    "    sech = make_pvals(bunchadots, envelope=i)\n",
    "    plt.plot(bunchadots,sech, color = cp[ind])\n",
    "    xhalfmax = bunchadots[np.where(sech<0.5)[0][0]]\n",
    "    plt.plot(xhalfmax,0.5, color = cp[ind], marker = 'o')\n",
    "    tracking.append((i, xhalfmax))\n",
    "plt.xlim([0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunchadots = np.linspace(0, 10, 1000)\n",
    "sech = make_pvals(bunchadots, envelope = 0.78967)\n",
    "plt.plot(bunchadots, sech)\n",
    "plt.xlim([0,5])\n",
    "plt.axhline(0.5, color='r')\n",
    "plt.axvline(1.04, color = 'r')\n",
    "\n",
    "plt.axvline(3.12, color = 'g')\n",
    "plt.axhline(0.04, color = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.torch.save(run_dict['agent'],agent_params['load_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
