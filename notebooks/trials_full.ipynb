{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from importlib import reload\n",
    "from modules import * \n",
    "import pickle\n",
    "import csv\n",
    "fig_savedir = '../data/figures/'\n",
    "from scipy.stats import entropy\n",
    "## TO do list in experiment.py \n",
    "## Notes:\n",
    "## Freeze weights is on to test storage into memory with fixed linear activity (ac.make_agent arg freeze = True)\n",
    "# this requires loading the saved weights (agent_params['load_model'] = True)\n",
    "# in ec.rec_mem temperature is set to 0.1 on softmax -- need to deal with this \n",
    "#\n",
    "## Need the MPLD3 stuff \n",
    "\n",
    "## Nov 30: There is the issue of using EC / mf confidence. When rwd is in training location for testing, EC\n",
    "## should agree with the MF and default to MF. What is happening is that if MF loses confidence, then it \n",
    "## fails catastrophically. Need to look into this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 5)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(eu)\n",
    "grid_params = {\n",
    "    'y_height':     25, \n",
    "    'x_width':      25,\n",
    "    'walls':        False,\n",
    "    'rho':          0,\n",
    "    'maze_type':    'none',\n",
    "    'barheight':    14,\n",
    "    'port_shift':   'none',\n",
    "    'step_penaliz': -0.01,\n",
    "    'rwd_loc':      [(15,5)]\n",
    "}\n",
    "\n",
    "#make environment\n",
    "\n",
    "maze = eu.gridworld(grid_params, around_reward = True)\n",
    "maze.rwd_loc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(ac)\n",
    "agent_params = {\n",
    "        'load_model':   False,\n",
    "        'load_dir':     '../data/outputs/gridworld/openfield{}{}.pt'.format(grid_params['x_width'],grid_params['y_height']),\n",
    "        'rwd_placement':'moved_loc',\n",
    "        \n",
    "        'state_type':   'conv',\n",
    "        'lin_dims':     500,\n",
    "        'rfsize':       10,\n",
    "        'stride':       1,\n",
    "        'action_dims':  len(maze.actionlist),\n",
    "        'temperature':  1,\n",
    "    \n",
    "        'batch_size':   1,\n",
    "        'gamma':        0.98, #discount factor\n",
    "        'eta':          5e-4,\n",
    "        \n",
    "        'use_EC':       True,\n",
    "        'cachelim':     300, # memory limit should be ~75% of #actions x #states\n",
    "        'EC':           {},\n",
    "        'mem_temp':     0.3\n",
    "        \n",
    "    }\n",
    "run_dict = ac.reset_agt(maze, agent_params)\n",
    "\n",
    "\n",
    "if agent_params['use_EC']:\n",
    "    #agent_params['cachelim'] = int(0.5*np.prod(maze.grid.shape))\n",
    "    agent_params['EC'] = ec.ep_mem(run_dict['agent'],agent_params['cachelim'])\n",
    "\n",
    "gp.plot_env(run_dict['environment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CURRENTLY \n",
    " - FREEZE WEIGHTS == TRUE\n",
    " - in maze.reset() around_reward defaults True\n",
    "don't forget to turn it off before running trials without preloaded weights\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_output(img):\n",
    "    img = img - img.min()\n",
    "    img = img / img.max()\n",
    "    return img\n",
    "\n",
    "# get weights from conv layer and normalize them for visualization\n",
    "weights = run_dict['agent'].hidden[0].weight.data\n",
    "weights = normalize_output(weights)\n",
    "\n",
    "n_filters, ix = 3, 1\n",
    "for i in range(n_filters):\n",
    "    # get filter:\n",
    "    f = weights[i,:,:,:]\n",
    "    for j in range(3):\n",
    "        ax = plt.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.imshow(f[j,:,:], cmap ='gray')\n",
    "        ix+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_params['mem_temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(gp)\n",
    "reload(expt)\n",
    "maze = eu.gridworld(grid_params, around_reward = True)\n",
    "run_dict = ac.reset_agt(maze, \n",
    "                        agent_params, \n",
    "                        freeze_weights=False)\n",
    "run_dict['NUM_TRIALS'] = 50\n",
    "expt.run(run_dict,full=False, rec_mem=True, use_EC=False) ## by default runs truncated trials with MF only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights from conv layer and normalize them for visualization\n",
    "weights = run_dict['agent'].hidden[0].weight.data\n",
    "weights = normalize_output(weights)\n",
    "print(weights.shape)\n",
    "n_filters, ix = 3, 1\n",
    "for i in range(n_filters):\n",
    "    # get filter:\n",
    "    f = weights[i,:,:,:]\n",
    "    for j in range(3):\n",
    "        ax = plt.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.imshow(f[j,:,:], cmap ='gray')\n",
    "        plt.ylabel(f'nf={i}, ch={j}')\n",
    "        ix+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_loss = run_dict['total_loss'][0]\n",
    "value_loss = run_dict['total_loss'][1]\n",
    "tot_reward = run_dict['total_reward']\n",
    "trial_length = run_dict['trial_length']\n",
    "\n",
    "plt.plot(tot_reward)\n",
    "plt.show()\n",
    "\n",
    "vv, pp = ac.snapshot(agent=run_dict['agent'], maze =run_dict['environment'])\n",
    "\n",
    "gp.plot_polmap(run_dict['environment'], pp)\n",
    "gp.plot_valmap(run_dict['environment'], vv, v_range =[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pp[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.torch.save(run_dict['agent'],agent_params['load_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state = torch.Tensor(maze.observation)\n",
    "print(maze.cur_state)\n",
    "policy_, value_, lin_act_ = run_dict['agent'](state)\n",
    "conv = run_dict['agent'].conv.data[0].numpy()\n",
    "print(conv.shape)\n",
    "for i in range(3):\n",
    "    plt.imshow(conv[i,:,:],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ac)\n",
    "abcd, efgh = ac.mem_snapshot(maze, \n",
    "                       agent_params['EC'], \n",
    "                       trial_timestamp = 50, \n",
    "                       decay           = ec.calc_env(halfmax = 20), \n",
    "                       mem_temp        = 0.3,\n",
    "                            get_vals=True)#agent_params['mem_temp'])\n",
    "gp.plot_polmap(maze, abcd,threshold=0)\n",
    "\n",
    "# KL Divergence\n",
    "ec_policies = abcd\n",
    "mf_policies = np.vstack(pp) \n",
    "\n",
    "kld = np.zeros(ec_policies.shape)\n",
    "for i in range(ec_policies.shape[0]):\n",
    "    for j in range(ec_policies.shape[1]):\n",
    "        mf_pol = ac.softmax([m for m in mf_policies[i][j]])\n",
    "        ec_pol = ac.softmax([e for e in ec_policies[i][j]])\n",
    "        kld[i][j] = entropy(ec_pol,mf_pol)\n",
    "gp.plot_valmap(maze, kld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print state action values\n",
    "data_frame = []\n",
    "for i in range(6): # num actions\n",
    "    data_frame.append(np.zeros((maze.y, maze.x)))\n",
    "    for j in range(maze.x): # width\n",
    "        for k in range(maze.y): #height\n",
    "            data_frame[i][k,j] = efgh[k,j][i]\n",
    "    gp.plot_valmap(maze,data_frame[i], title =f\"State Action Value Estimate for = {maze.actionlist[i]}\", v_range = [-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tot_reward += run_dict['total_reward']\n",
    "plt.plot(tot_reward,'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tens = [np.zeros((10,10)) for i in range(6)]\n",
    "print(len(val_tens))\n",
    "for index, x in np.ndenumerate(efgh):\n",
    "    for i in range(6):\n",
    "        val_tens[i][index]= x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = run_dict['environment'].actionlist\n",
    "reload(gp)\n",
    "for i in range(6):\n",
    "    gp.plot_valmap(run_dict['environment'],val_tens[i], p_range = [val_tens[i].min(), val_tens[0].max()], title='action='+acts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_aside = abcd\n",
    "### this is for plotting the biggest change in the policy after a single reward is received\n",
    "'''\n",
    "hootenany = np.zeros_like(abcd)\n",
    "for index, x in np.ndenumerate(abcd):\n",
    "    change = np.asarray(list(abcd[index]) - np.asarray(list(set_aside[index])))\n",
    "    hootenany[index] = tuple(change)\n",
    "\n",
    "gp.plot_polmap(maze, hootenany)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_loss = run_dict['total_loss'][0]\n",
    "value_loss = run_dict['total_loss'][1]\n",
    "tot_reward = run_dict['total_reward']\n",
    "trial_length = run_dict['trial_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ysteps = int(grid_params['y_height']/2)\n",
    "max_xsteps = int(grid_params['x_width']/2)\n",
    "\n",
    "reward_threshold = 1+ grid_params['step_penaliz']*(max_xsteps + max_ysteps)\n",
    "print(reward_threshold)\n",
    "\n",
    "fig, ax = plt.subplots(2,1, sharex=True)\n",
    "ax[0].plot(gp.running_mean(tot_reward,30))\n",
    "ax[0].plot(tot_reward, 'b', alpha = 0.2)\n",
    "ax[0].axhline(reward_threshold)\n",
    "#plt.plot(trial_length)\n",
    "ax[1].plot(policy_loss)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "rwd_mean = gp.running_mean(tot_reward,30)\n",
    "plt.figure(1)\n",
    "plt.plot(rwd_mean)\n",
    "plt.plot(tot_reward, 'b', alpha = 0.2)\n",
    "plt.axhline(reward_threshold)\n",
    "plt.savefig('rdtest.svg', format='svg')\n",
    "\n",
    "get_sd =np.where(rwd_mean > reward_threshold)[0]\n",
    "clipped_pol_loss = policy_loss[get_sd:-1]\n",
    "print(np.std(clipped_pol_loss))\n",
    "\n",
    "#pick = [tot_reward, policy_loss, value_loss, trial_length]\n",
    "#pickle.dump(pick, open(\"pickles/initial_training.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(gp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = ac.mem_snapshot(run_dict['environment'], agent_params['EC'], trial_timestamp=1, decay=100)\n",
    "gp.plot_polmap(run_dict['environment'],ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ = 0 \n",
    "for k, v in agent_params['EC'].cache_list.items():\n",
    "    if v[2] == (10,10):\n",
    "        key_ = k\n",
    "        print(agent_params['EC'].cache_list[key_][0])\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(np.arange(6), agent_params['EC'].recall_mem(key_, 26))\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,1, sharex=True)\n",
    "ax[0].plot(policy_loss)\n",
    "ax[0].axvline(x = run_dict['NUM_TRIALS'],color='k', linestyle=':', alpha=0.5)\n",
    "ax[0].set_ylabel('Policy Loss')\n",
    "ax[1].plot(value_loss)\n",
    "ax[1].axvline(x = run_dict['NUM_TRIALS'],color='k', linestyle=':', alpha=0.5)\n",
    "ax[1].set_ylabel('Value Loss')\n",
    "\n",
    "#ax[0].arrow(2000, max(policy_loss), 0, -600, head_width=50, head_length=100, fc='k', ec='k')\n",
    "#ax[1].arrow(2000, max(value_loss), 0, -600, head_width=50, head_length=100, fc='k', ec='k')\n",
    "\n",
    "#plt.savefig('../data/figures/loss_after_trunc_training.svg', format='svg')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -1\n",
    "print(value_loss[x])\n",
    "print(policy_loss[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv, pp = ac.snapshot(agent=run_dict['agent'], maze =run_dict['environment'])\n",
    "reload(gp)\n",
    "gp.plot_polmap(run_dict['environment'], pp)\n",
    "gp.plot_valmap(run_dict['environment'], vv, p_range=[0,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pvals(p, **kwargs):\n",
    "    envelope = kwargs.get('envelope', 50)\n",
    "    mfc      = kwargs.get('conf_score', 1)\n",
    "    return mfc*np.round(1 / np.cosh(p / envelope), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = np.linspace(1,200, 100)#np.sort(np.random.randint(1,100,10))\n",
    "b = 10*np.arange(10)+5\n",
    "for i in b:\n",
    "    a_p = test_pvals(a, envelope=i)\n",
    "    plt.plot(a, a_p, '-', label=f'{i}')\n",
    "plt.legend(loc=0)\n",
    "#ax[1].plot(a_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EC = agent_params['EC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "meas_time = time.time()\n",
    "x = np.asarray(list(agent_params['EC'].cache_list.keys()))\n",
    "#x = np.asarray([*agent_params['EC'].cache_list.keys()])\n",
    "print(time.time() - meas_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ec)\n",
    "test_case = x[0]+ 0.3*(np.random.randn(500))\n",
    "key, index, sim = EC.cosine_sim(test_case)\n",
    "print(key[0:10], index, sim)\n",
    "EC.recall_mem(key, timestep=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pvals(p, envelope):\n",
    "    return np.round(1 / np.cosh(p / envelope), 8)\n",
    "## try to plot relationship of envelope and half max x value \n",
    "bunchadots = np.linspace(0, 1000, 10000)\n",
    "envs = np.arange(50, step = 2)+1\n",
    "\n",
    "cp = sns.color_palette(n_colors=len(envs))\n",
    "\n",
    "tracking = []\n",
    "for ind, i in enumerate(envs):\n",
    "    sech = make_pvals(bunchadots, envelope=i)\n",
    "    plt.plot(bunchadots,sech, color = cp[ind])\n",
    "    xhalfmax = bunchadots[np.where(sech<0.5)[0][0]]\n",
    "    plt.plot(xhalfmax,0.5, color = cp[ind], marker = 'o')\n",
    "    tracking.append((i, xhalfmax))\n",
    "plt.xlim([0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunchadots = np.linspace(0, 10, 1000)\n",
    "sech = make_pvals(bunchadots, envelope = 0.78967)\n",
    "plt.plot(bunchadots, sech)\n",
    "plt.xlim([0,5])\n",
    "plt.axhline(0.5, color='r')\n",
    "plt.axvline(1.04, color = 'r')\n",
    "\n",
    "plt.axvline(3.12, color = 'g')\n",
    "plt.axhline(0.04, color = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(5)\n",
    "b = 0.5\n",
    "for i in range(len(a)):\n",
    "    a[i] = b*(b**i)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.torch.save(run_dict['agent'],agent_params['load_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
