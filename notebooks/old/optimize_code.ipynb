{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from importlib import reload\n",
    "from modules import * \n",
    "import csv\n",
    "fig_savedir = '../data/figures/'\n",
    "\n",
    "grid_params = {\n",
    "    'y_height':   10,\n",
    "    'x_width':    10,\n",
    "    'walls':      False,\n",
    "    'rho':        0,\n",
    "    'maze_type':  'none',\n",
    "    'port_shift': 'none'\n",
    "}\n",
    "\n",
    "\n",
    "agent_params = {\n",
    "    'load_model':   True,\n",
    "    'load_dir':     '../data/outputs/gridworld/MF{}{}training_rwd7_2.pt'.format(grid_params['x_width'],grid_params['y_height']),\n",
    "    'action_dims':  6, #=len(maze.actionlist)\n",
    "    'lin_dims':     500,\n",
    "    'batch_size':   1,\n",
    "    'gamma':        0.98, #discount factor\n",
    "    'eta':          5e-4,\n",
    "    'temperature':  1,\n",
    "    'use_EC':       False,\n",
    "    'cachelim':     100, # memory limit should be ~75% of #actions x #states\n",
    "    'state_type':   'conv'\n",
    "}\n",
    "\n",
    "run_dict = {\n",
    "    'NUM_EVENTS':   150,\n",
    "    'NUM_TRIALS':   5000,\n",
    "    'print_freq':   1/10,\n",
    "    'total_loss':   [[],[]],\n",
    "    'total_reward': [],\n",
    "    'val_maps':     [],\n",
    "    'policies':     [{},{}],\n",
    "    'deltas':       [],\n",
    "    'spots':        [],\n",
    "    'vls':          []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 5)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make environment\n",
    "maze = eu.gridworld(grid_params)\n",
    "maze.set_rwd([(int(grid_params['y_height']/2),int(grid_params['x_width']/2))])\n",
    "env = eu.gymworld(maze) # openAI-like wrapper \n",
    "\n",
    "#update agent params dictionary with layer sizes appropriate for environment \n",
    "agent_params = sg.gen_input(maze, agent_params)\n",
    "MF,opt = ac.make_agent(agent_params, freeze=False)\n",
    "gp.make_env_plots(maze,env=True)\n",
    "\n",
    "\n",
    "agent_params['cachelim'] = int(0.5*np.prod(maze.grid.shape))\n",
    "\n",
    "EC = ec.ep_mem(MF,agent_params['cachelim']) \n",
    "print(maze.rwd_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    }
   ],
   "source": [
    "for param_ in opt.param_groups:\n",
    "    print(param_['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "print(maze.grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function for runs with episodic mem and without -- take use_EC as a param\n",
    "# assume just for conv inputs \n",
    "def run_trials(run_dict, use_EC, **kwargs):\n",
    "    save_data  = kwargs.get('save', True)\n",
    "    NUM_TRIALS = run_dict['NUM_TRIALS']\n",
    "    NUM_EVENTS = run_dict['NUM_EVENTS']\n",
    "    \n",
    "    blocktime = time.time()\n",
    "    \n",
    "    if use_EC:\n",
    "        EC.reset_cache()\n",
    "        add_mem_dict = {} #dictionary of items which get put into memory cache\n",
    "        \n",
    "        run_dict['total_loss']   =  [[],[]]\n",
    "        run_dict['total_reward'] = []\n",
    "        run_dict['track_cs']     = [[],[]]\n",
    "        run_dict['rpe']          = np.zeros(maze.grid.shape)\n",
    "        \n",
    "        reward    = 0\n",
    "        timestamp = 0\n",
    "        tslr      = np.nan_to_num(np.inf)\n",
    "        \n",
    "        for trial in range(NUM_TRIALS):\n",
    "            trialstart_stamp = timestamp\n",
    "\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "\n",
    "            env.reset() \n",
    "            \n",
    "            state = torch.Tensor(sg.get_frame(maze))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "\n",
    "            for event in range(NUM_EVENTS):\n",
    "                #compute confidence in MFC\n",
    "                if event in [0,1]:\n",
    "                    MF_cs = EC.make_pvals(tslr,envelope=10)\n",
    "                else: \n",
    "                    MF_cs = EC.make_pvals(tslr,envelope=10, pol_id = pol_flag, mfc=MF_cs)\n",
    "                # pass state through EC module\n",
    "                \n",
    "                policy_, value_, lin_act_ = MF(state, temperature = 1)\n",
    "                lin_act = tuple(np.round(lin_act_.data[0].numpy(),4))\n",
    "                if event is not 0:\n",
    "                    ec_pol = torch.from_numpy(EC.recall_mem(lin_act, timestamp, env=150))\n",
    "                    candidate_policies = [policy_, ec_pol]\n",
    "                    pol_choice = np.random.choice([0,1], p=[MF_cs, 1-MF_cs])\n",
    "                    pol = candidate_policies[pol_choice]\n",
    "                    if pol_choice == 0:\n",
    "                        pol_flag = 'MF'\n",
    "                    else:\n",
    "                        pol_flag = 'EC'\n",
    "                    choice, policy, value = ac.select_ec_action(MF, policy_, value_, pol)\n",
    "                else:\n",
    "                    choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "                \n",
    "                add_mem_dict['state'] = maze.cur_state\n",
    "                #compute eligibility trace/rpe approximation\n",
    "                delta = reward + agent_params['gamma']*value - v_last  \n",
    "                run_dict['rpe'][maze.cur_state[1]][maze.cur_state[0]] = delta\n",
    "                \n",
    "                \n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "                if event is not 0:\n",
    "                    if reward == 1:\n",
    "                        tslr = 0\n",
    "                        delta = 25\n",
    "                    else:\n",
    "                        tslr += 1\n",
    "                    \n",
    "                    run_dict['track_cs'][0].append(tslr)\n",
    "                    run_dict['track_cs'][1].append(MF_cs)\n",
    "                \n",
    "                MF.rewards.append(reward)\n",
    "                \n",
    "                \n",
    "                add_mem_dict['activity']  = lin_act\n",
    "                add_mem_dict['action']    = choice\n",
    "                add_mem_dict['delta']     = delta\n",
    "                add_mem_dict['timestamp'] = timestamp            \n",
    "                \n",
    "                EC.add_mem(add_mem_dict)#add event to memory cache\n",
    "                \n",
    "                # because we need to include batch size of 1 \n",
    "                state = torch.Tensor(sg.get_frame(maze))\n",
    "                reward_sum += reward\n",
    "\n",
    "                v_last = value\n",
    "                timestamp += 1\n",
    "\n",
    "            p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "            \n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                #run_dict['total_loss'][0].append(p_loss.data[0])\n",
    "                #run_dict['total_loss'][1].append(v_loss.data[0])\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "\n",
    "            if trial ==0 or trial%10==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()\n",
    "\n",
    "    else:\n",
    "        run_dict['rpe'] = np.zeros(maze.grid.shape)\n",
    "        run_dict['total_loss'] =  [[],[]]\n",
    "        run_dict['total_reward'] = []\n",
    "        for trial in range(NUM_TRIALS):\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "            track_deltas = []\n",
    "            track_spots  = []\n",
    "            visited_locs = []\n",
    "\n",
    "            env.reset() \n",
    "            state = torch.Tensor(sg.get_frame(maze))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "\n",
    "            for event in range(NUM_EVENTS):\n",
    "                policy_, value_ = MF(state, agent_params['temperature'])[0:2]\n",
    "                choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "\n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "                MF.rewards.append(reward)\n",
    "                #compute eligibility trace/rpe approximation\n",
    "                delta = reward + agent_params['gamma']*value - v_last  \n",
    "                state = torch.Tensor(sg.get_frame(maze))\n",
    "                run_dict['rpe'][maze.cur_state[1]][maze.cur_state[0]] = delta\n",
    "                \n",
    "                reward_sum += reward\n",
    "                v_last = value\n",
    "\n",
    "            p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "            \n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                run_dict['total_loss'][0].append(p_loss.item())\n",
    "                run_dict['total_loss'][1].append(v_loss.item())\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "\n",
    "            if trial ==0 or trial%100==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../memory/episodic.py:66: RuntimeWarning: overflow encountered in cosh\n",
      "  return np.round(1 / np.cosh(p / self.memory_envelope), 8)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'readable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3996c9019818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#false = mf only, true = w ec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-3f98d2da8b85>\u001b[0m in \u001b[0;36mrun_trials\u001b[0;34m(run_dict, use_EC, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0madd_mem_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_mem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_mem_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#add event to memory cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;31m# because we need to include batch size of 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/LINC Lab Documents/Code/MEMRL/memory/episodic.py\u001b[0m in \u001b[0;36madd_mem\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mdelta\u001b[0m           \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mtimestamp\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mreadable\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'readable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \t\t'''\n",
      "\u001b[0;31mKeyError\u001b[0m: 'readable'"
     ]
    }
   ],
   "source": [
    "run_trials(run_dict, True) #false = mf only, true = w ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### run a bunch of trials to get rwds to average \n",
    "ec_sim = False\n",
    "for i in range(2):\n",
    "    print('-----\\n',i)\n",
    "    MF,opt = ac.make_agent(agent_params, freeze=False)\n",
    "    if ec_sim:\n",
    "        EC = ec.ep_mem(MF,agent_params['cachelim']) \n",
    "        run_trials(run_dict, True)\n",
    "    else:\n",
    "        run_trials(run_dict, False)\n",
    "    with open('1000_1posttrain_mf.csv', 'a+') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerows([run_dict['total_reward']])\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(run_dict['total_reward'], label='r')\n",
    "#plt.plot(run_dict['total_loss'][0], label='p')\n",
    "#plt.plot(run_dict['total_loss'][1], label='v')\n",
    "plt.legend(bbox_to_anchor=(1.1,1.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early_learn_mf_only = run_dict['total_reward']\n",
    "#early_learn_w_ec = run_dict['total_reward']\n",
    "#late_learn_mf_only = run_dict['total_reward']\n",
    "#late_learn_w_ec = run_dict['total_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(run_dict['total_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, sharex = True, sharey=True)\n",
    "ax[0].plot(early_learning_w_ec, 'b', alpha =0.7, label = 'With EC')\n",
    "ax[0].plot(early_learn_mf_only, 'k--', alpha = 0.7, label=\"MF only\")\n",
    "\n",
    "ax[1].plot(late_learn_w_ec, 'r', label='With EC')\n",
    "ax[1].plot(late_learn_mf_only, 'k--', alpha = 0.7, label ='MF only')\n",
    "\n",
    "ax[0].legend(bbox_to_anchor = (1.0,.75))\n",
    "ax[1].legend(bbox_to_anchor = (1.0,.75))\n",
    "fig.savefig('compare.svg', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rpe) \n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap = cmap)\n",
    "\n",
    "ax1 = fig.add_axes([0.04, 0, 0.85, 0.85])\n",
    "axc = fig.add_axes([0.8, 0, 0.05, 0.85])\n",
    "\n",
    "cb1 = colorbar.ColorbarBase(axc, cmap = cmap, norm = cNorm)\n",
    "ax1.imshow(maze.grid, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax1.add_patch(patches.Circle(maze.rwd_loc[0], 0.35, fc='w'))\n",
    "\n",
    "for entry in compare_policies.keys():\n",
    "    x = entry[0]\n",
    "    y = entry[1]\n",
    "    \n",
    "    ec_policy = compare_policies[entry][1][0] - compare_policies[entry][0][0]\n",
    "    action = np.argmax(ec_policy)\n",
    "    prob = max(ec_policy)\n",
    "    \n",
    "    dx1, dy1, head_w, head_l = gp.make_arrows(action,prob)\n",
    "    if prob > 1/4:\n",
    "        if (dx1, dy1) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal1 = scalarMap.to_rgba(prob)\n",
    "            ax1.arrow(x,y, dx1, dy1, head_width=0.3, head_length=0.2, color = colorVal1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "ax1.invert_yaxis()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = compare_policies[(7,5)][1]\n",
    "for i in range(len(x)):\n",
    "    plt.bar(np.arange(6)+(0.1*i), x[i], width = 0.1, alpha = 0.3)\n",
    "    plt.bar(np.arange(6), np.ones(6)*0.16666667, width =0.1, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(DND.cache_list))\n",
    "def cosine_sim(mem_dict, key, **kwargs):\n",
    "    similarity_threshold = kwargs.get('threshold', 0.9)\n",
    "\n",
    "    mem_cache = np.asarray(list(mem_dict.keys()))\n",
    "    print(mem_cache.shape)\n",
    "    entry = np.asarray(key)\n",
    "\n",
    "    mqt = np.dot(mem_cache, entry)\n",
    "    norm = np.linalg.norm(mem_cache, axis=1) * np.linalg.norm(entry)\n",
    "\n",
    "    cosine_similarity = mqt / norm\n",
    "\n",
    "    index = np.argmax(cosine_similarity)\n",
    "    similar_activity = mem_cache[index]\n",
    "    if max(cosine_similarity) >= similarity_threshold:\n",
    "        return similar_activity, index, max(cosine_similarity)\n",
    "\n",
    "    else:\n",
    "        # print('max memory similarity:', max(cosine_similarity))\n",
    "        return [], [], max(cosine_similarity)\n",
    "\n",
    "def make_pvals(p, **kwargs):\n",
    "    envelope = kwargs.get('envelope', 50)\n",
    "    return np.round(1 / np.cosh(p / envelope),8)\n",
    "\n",
    "def softmax(x, T=1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp((x - np.max(x))/T)\n",
    "    return e_x / e_x.sum(axis=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_key, ind, max_cs = cosine_sim(DND.cache_list, sts[0])\n",
    "print('sim', max_cs)\n",
    "memory = np.nan_to_num(DND.cache_list[tuple(state_key)][0])\n",
    "deltas = memory[:,0]\n",
    "print(deltas)\n",
    "rec_times = memory[:,1]\n",
    "print(\"rtimes\",rec_times)\n",
    "times        = abs(timestep - memory[:,1])\n",
    "print('tiems', times)\n",
    "pv =make_pvals(times)\n",
    "print(pv)\n",
    "\n",
    "mult = np.multiply(deltas, pv)\n",
    "print(softmax(max_cs*mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in sts[0:10]:\n",
    "    timestep = 1000\n",
    "    a = DND.recall_mem(key=i,timestep=timestep,env=100)\n",
    "    print(a,\"----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs =['r','g','b','c', 'y','pink']\n",
    "'''\n",
    "r = list(self.cache_list.keys())\n",
    "g = [t for e, t in self.cache_list.values()]\n",
    "b = lp = persistence_.index(min(persistence_))\n",
    "c = old_activity = cache_keys[lp]\n",
    "y = del self.cache_list[old_activity]\n",
    "'''\n",
    "plt.figure()\n",
    "for i in range(len(EC.stupid_df)):\n",
    "    xs = np.arange(len(EC.stupid_df[i]))\n",
    "    ys = EC.stupid_df[i]\n",
    "    \n",
    "    plt.scatter(xs, ys, c=cs[i], alpha=0.3)\n",
    "plt.ylim([-0.00002,0.00002])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(run_dict['total_reward'])\n",
    "plt.ylim([0,run_dict['NUM_EVENTS']])\n",
    " \n",
    "plt.figure(2)\n",
    "plt.plot(run_dict['total_loss'][0], label = 'pol')\n",
    "plt.plot(run_dict['total_loss'][1], label = 'val')\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "plt.close()\n",
    "#gp.print_value_maps(maze, run_dict['val_maps'], maps=0, val_range=(-1,50), save_dir=fig_savedir, title='Value Map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.torch.save(MF,agent_params['load_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
