{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from modules import * \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridworld Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'obs_rho'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-744e3693cdee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#make environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m maze = eu.gridworld([eu.height, eu.width],\n\u001b[0;32m---> 13\u001b[0;31m                     \u001b[0mrho\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0meu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_rho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                     \u001b[0mnum_pc\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0meu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_cells\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mpc_fwhm\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mfwhm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'obs_rho'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "reload(eu)\n",
    "y_new = 7\n",
    "x_new = 7\n",
    "eu.height = y_new\n",
    "eu.width = x_new\n",
    "fwhm_range = np.linspace(0.05,0.5,6)\n",
    "\n",
    "fwhm = 0.1\n",
    "\n",
    "fig_savedir = '../data/outputs/gridworld/figures/'\n",
    "#make environment\n",
    "maze = eu.gridworld([eu.height, eu.width],\n",
    "                    rho        = eu.obs_rho,\n",
    "                    num_pc     = eu.place_cells, \n",
    "                    pc_fwhm    = fwhm, \n",
    "                    maze_type  = 'none', \n",
    "                    port_shift = eu.portshift,walls=True)\n",
    "maze.rwd_loc = [(4,7)]#[(int(y_new/2),int(x_new/2))]\n",
    "print maze.fwhm\n",
    "for i in maze.rwd_loc: \n",
    "    maze.orig_rwd_loc.append(i)\n",
    "\n",
    "#show environment\n",
    "eu.make_env_plots(maze,1,1)\n",
    "\n",
    "## test out gridworld wrapper. \n",
    "env = eu.gymworld(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Free Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MF module parameters\n",
    "input_dim  = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "h_dims     = [400]\n",
    "h_types    = ['linear'] #lstm does not work yet, presumably also gru\n",
    "MF = mf.AC_Net(input_dim, action_dim, hidden_dimensions = h_dims, hidden_types = h_types)\n",
    "discount_factor = 0.98\n",
    "\n",
    "#learning parameters\n",
    "eta = 5e-4 #gradient descent learning rate\n",
    "opt = mf.optim.Adam(MF.parameters(), lr = eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EC module parameters\n",
    "use_EC = True\n",
    "EC = ec.ep_mem(MF,int(0.75*np.prod(maze.grid.shape)*env.action_space.n)) # memory limit should be ~75% of #actions x #states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMULATION RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------\n",
    "# Run Parameters\n",
    "#----------------\n",
    "NUM_TRIALS = 1000\n",
    "NUM_EVENTS = 300\n",
    "\n",
    "# --------------------------------\n",
    "# empty data frames for recording\n",
    "# --------------------------------\n",
    "EC.reset_cache()\n",
    "EC.reward_unseen = True \n",
    "add_mem_dict = {} #dictionary of items which get put into memory cache\n",
    "timestamp = 0\n",
    "success_benchmark = 0.95 # average success before employing EC or removing most rewarded port\n",
    "\n",
    "# data frames for value and policy maps\n",
    "total_loss    = [[],[]]           # keep track of policy loss and value loss \n",
    "total_reward  = []                # keep track of rewards achieved per trial\n",
    "val_maps      = []                # keep a list of state value arrays for each trial\n",
    "value_map     = maze.empty_map    # array of state values\n",
    "\n",
    "# flags to be changed mid run\n",
    "rwd_threshold = True\n",
    "\n",
    "# --------------------------------\n",
    "# optional settings for recording\n",
    "# --------------------------------\n",
    "add_episodic_cache = False  ## Possibly unnecessary now \n",
    "midrun_rwd_removal = False\n",
    "track_occupancy    = False\n",
    "track_MF_policies  = False\n",
    "track_CS           = False\n",
    "track_KLD          = True\n",
    "\n",
    "if midrun_rwd_removal: \n",
    "    reward_tally = {}\n",
    "    for _ in maze.rwd_loc: \n",
    "        reward_tally[_] = []\n",
    "    trial_rwd_switch = 0\n",
    "\n",
    "if track_occupancy: \n",
    "    occ_list = []\n",
    "if track_MF_policies:\n",
    "    track_policy_dict = {}\n",
    "if track_CS:\n",
    "    track_confidence_score = [[],[]] #[[rewards],[computed confidence score]]\n",
    "if track_KLD:\n",
    "    policy_KLD = [[],[]]\n",
    "    ec_kld = eu.KLD_holder(maze)\n",
    "    mf_kld = eu.KLD_holder(maze)\n",
    "    \n",
    "deltas = []\n",
    "    \n",
    "# record current time before beginning of trial\n",
    "print_freq = 1/10\n",
    "runtime = time.time()\n",
    "blocktime = time.time()\n",
    "print \"Run started: \"\n",
    "#==================================\n",
    "# Run Trial\n",
    "#==================================\n",
    "for trial in xrange(NUM_TRIALS):\n",
    "    trialstart_stamp = timestamp\n",
    "    state       = Variable(torch.FloatTensor(env.reset()))\n",
    "    reward_sum  = 0\n",
    "    v_last      = 0\n",
    "    reward_last = 0 \n",
    "    if track_occupancy:\n",
    "        occupancy = np.zeros((eu.height, eu.width))\n",
    "    if track_KLD:\n",
    "        ec_kld.reset()\n",
    "        mf_kld.reset()\n",
    "        if trial == NUM_TRIALS-1:\n",
    "            MF_pol_map = eu.KLD_holder(maze, track='policy')\n",
    "            EC_pol_map = eu.KLD_holder(maze, track='policy')\n",
    "    \n",
    "    track_deltas = []\n",
    "    trial_start = time.time()\n",
    "    \n",
    "    for event in xrange(NUM_EVENTS):\n",
    "        if track_occupancy:\n",
    "            occupancy[maze.cur_state[1], maze.cur_state[0]] += 1\n",
    "            \n",
    "        # pass state through MF module\n",
    "        policy_, value_ = MF(state)\n",
    "        \n",
    "        # pass state through EC module\n",
    "        if use_EC:\n",
    "            add_mem_dict['state'] = maze.cur_state\n",
    "            #policy_EC = EC.recall_mem(tuple(state.data[0]))\n",
    "            #policy_ = EC.composite_policy(policy_, policy_EC,reward)    # get policy composed of MF and EC outputs\n",
    "        \n",
    "        choice, policy, value = mf.select_action_end(MF,policy_, value_)\n",
    "        \n",
    "        if track_MF_policies:\n",
    "            track_policy_dict[maze.cur_state] = policy, timestamp\n",
    "        \n",
    "        if event < NUM_EVENTS: \n",
    "            next_state, reward, done, info = env.step(choice)\n",
    "        \n",
    "        MF.rewards.append(reward)\n",
    "        delta = reward + discount_factor*value - v_last  #compute eligibility trace/rpe approximation\n",
    "        \n",
    "        if use_EC:\n",
    "            add_mem_dict['activity']  = tuple(state.data[0])\n",
    "            add_mem_dict['action']    = choice\n",
    "            add_mem_dict['delta']     = delta\n",
    "            add_mem_dict['timestamp'] = timestamp            \n",
    "            EC.add_mem(add_mem_dict,mixing=False, keep_hist=True)             #add event to memory cache\n",
    "            if reward != 0:\n",
    "                EC.reward_update(trialstart_stamp, timestamp, reward)\n",
    "            track_deltas.append(delta[0])\n",
    "                \n",
    "        if track_KLD:\n",
    "            mf_kld.update(maze.cur_state, policy)\n",
    "            ec_kld.update(maze.cur_state, eu.softmax(EC.cache_list[tuple(state.data[0])][0],T=3))\n",
    "            if trial == NUM_TRIALS-1:\n",
    "                MF_pol_map.update(maze.cur_state, policy)\n",
    "                EC_pol_map.update(maze.cur_state, eu.softmax(EC.cache_list[tuple(state.data[0])][0]))\n",
    "            \n",
    "            \n",
    "        if track_CS:\n",
    "            track_confidence_score[0].append(reward)\n",
    "            track_confidence_score[1].append(EC.confidence_score)\n",
    "            EC.compute_confidence(reward)\n",
    "\n",
    "        state = Variable(torch.FloatTensor(next_state))       # update state\n",
    "        reward_sum += reward\n",
    "        reward_last = reward\n",
    "\n",
    "        v_last = value\n",
    "        \n",
    "        timestamp += 1\n",
    "    \n",
    "    if add_episodic_cache:\n",
    "        if use_EC==False and (len(total_reward)>50)  and (np.array(total_reward[-50:]).mean() > success_benchmark*NUM_EVENTS):\n",
    "            if rwd_threshold:\n",
    "                print \" \\t Started Memory at Trial \", trial\n",
    "                if midrun_rwd_removal:\n",
    "                    maxsums = {}\n",
    "                    for item in reward_tally.items():\n",
    "                        maxsums[item[0]] = sum(item[1])\n",
    "                    most_rewarded_location = max(maxsums.iteritems(), key=operator.itemgetter(1))[0] \n",
    "                    maze.rwd_loc.remove(most_rewarded_location)\n",
    "                    trial_rwd_switch = trial\n",
    "                    print \"removed reward at \", most_rewarded_location\n",
    "\n",
    "                rwd_threshold = False\n",
    "                use_EC = True\n",
    "\n",
    "    if midrun_rwd_removal:\n",
    "        if (trial_rwd_switch!=0) and (trial == trial_rwd_switch + 1000):\n",
    "            maze.rwd_loc.append(most_rewarded_location)\n",
    "   \n",
    "    #--- AT end of trial, update weights     \n",
    "    p_loss, v_loss = mf.finish_trial(MF, discount_factor,opt)\n",
    "    \n",
    "    total_loss[0].append(p_loss.data[0])\n",
    "    total_loss[1].append(v_loss.data[0])\n",
    "    total_reward.append(reward_sum)\n",
    "\n",
    "    value_map = mf.generate_values(maze,MF,None)\n",
    "    val_maps.append(value_map.copy())    \n",
    "    \n",
    "    if track_KLD:\n",
    "        policy_KLD[0].append(mf_kld.map.copy())\n",
    "        policy_KLD[1].append(ec_kld.map.copy())\n",
    "        \n",
    "    \n",
    "    if midrun_rwd_removal:\n",
    "        for item in maze.reward_tally.items():\n",
    "            reward_tally[item[0]].append(item[1])\n",
    "    if track_occupancy:\n",
    "        occ_list.append(occupancy.copy())\n",
    "    \n",
    "    deltas.append(track_deltas)\n",
    "    # print reward measure\n",
    "    if trial%(print_freq*NUM_TRIALS)==0 or trial == NUM_TRIALS-1: \n",
    "        print \"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)\n",
    "        blocktime = time.time() \n",
    "    \n",
    "print \"Run took {0:.3f}\".format(time.time()-runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(EC.cache_list)\n",
    "print trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss & Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.plot(total_loss[0], label='ploss')\n",
    "plt.plot(total_loss[1], label='vloss')\n",
    "plt.legend(loc=0)\n",
    "plt.savefig(fig_savedir+'loss.svg',format='svg')\n",
    "plt.suptitle('Loss Per Trial', fontsize=14)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(total_reward, label='rwd')\n",
    "plt.xlabel(\"Number of Trials\")\n",
    "plt.ylabel(\"Total Reward per Trial\")\n",
    "plt.suptitle('Reward Per Trial', fontsize=14)\n",
    "if midrun_rwd_removal:\n",
    "    plt.axvline(x=trial_rwd_switch, color='r')\n",
    "    #plt.axvline(x=trial_rwd_switch+1000, color='r')\n",
    "plt.savefig(fig_savedir+'trialrwd.svg',format='svg')\n",
    "\n",
    "\n",
    "avg_delta = []\n",
    "for j in xrange(NUM_EVENTS):\n",
    "    avg_sum = 0\n",
    "    for i in xrange(len(deltas)):\n",
    "        avg_sum += deltas[i][j]\n",
    "    avg = avg_sum/NUM_EVENTS\n",
    "    avg_delta.append(avg)\n",
    "plt.figure(2)\n",
    "plt.plot(avg_delta)\n",
    "#plt.ylim([0,2])\n",
    "plt.suptitle('TD(0) Estimate per Event \\nAveraged Over Trials', fontsize=14)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Maps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eu.print_value_maps(maze,val_maps,title='Value Map',save_dir=fig_savedir) ### see individual map with kwarg maps=X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFC-Generated Policy Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_policy_dict = {}\n",
    "for __ in maze.useable:\n",
    "    temp_state = Variable(torch.FloatTensor(maze.mk_state(state=__)))\n",
    "    policy_, value_ = MF(temp_state)\n",
    "    track_policy_dict[__] = np.array(policy_.data[0])\n",
    "    \n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(maze.grid.shape[0],  maze.grid.shape[1])\n",
    "for i in track_policy_dict:\n",
    "    y_dim = i[0]\n",
    "    x_dim = i[1]\n",
    "    ax[i[1], i[0]].bar(np.arange(6), np.array(track_policy_dict[i])) #what was generated by network\n",
    "    ax[i[1], i[0]].set_xticks([])\n",
    "    ax[i[1], i[0]].set_yticks([])\n",
    "    ax[i[1], i[0]].set_ylim([0,1])\n",
    "    if i == maze.rwd_loc[0]:\n",
    "        ax[i[1], i[0]].annotate('R', xy=(0,0.5), color='r')\n",
    "#plt.savefig('policies.svg', format='svg')\n",
    "fig.suptitle('MFC Policy', fontsize=16) \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EC-Generated Policy Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# policy maps in memory\n",
    "if use_EC:\n",
    "    test_frame = [(v[2],v[0], v[1],eu.softmax(v[0],T=1), maze.actionlist[np.argmax(eu.softmax(v[0]))], [x for _,x in sorted(zip(eu.softmax(v[0]),maze.actionlist), reverse=True)]) for (k,v) in EC.cache_list.items()]\n",
    "\n",
    "    with open('mycsvfile.csv','wb') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerows(test_frame)\n",
    "\n",
    "    resultant_frame = {}\n",
    "    for enum, i in enumerate(test_frame):\n",
    "        resultant_frame[i[0]] = test_frame[enum][1:]\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(maze.grid.shape[0], maze.grid.shape[1])\n",
    "    cmap = plt.cm.Spectral_r\n",
    "    cNorm  = colors.Normalize(vmin=0, vmax=1)\n",
    "    scalarMap = cm.ScalarMappable(norm = cNorm, cmap=cmap)\n",
    "\n",
    "    for i in test_frame:\n",
    "        state_loc = i[0]\n",
    "        av = i[1]\n",
    "        softmax = i[3]\n",
    "        time_recorded = i[2]\n",
    "\n",
    "        ax[state_loc[1], state_loc[0]].cla()\n",
    "        alpha_factor = timestamp-time_recorded\n",
    "        if alpha_factor == 0:\n",
    "            alpha_factor = 0.1\n",
    "        opacity = 5000./alpha_factor\n",
    "        if opacity >1:\n",
    "            opacity = 1\n",
    "        ax[state_loc[1], state_loc[0]].bar(np.arange(6), softmax, alpha = opacity)\n",
    "        arg_ = np.argmax(softmax)\n",
    "        if softmax[arg_] > 0.65:\n",
    "            dx1,dy1,_,__ = eu.make_arrows(arg_, softmax[arg_])\n",
    "            colorVal1 = scalarMap.to_rgba(softmax[arg_])\n",
    "            ax[state_loc[1], state_loc[0]].arrow(5,.5,dx1,dy1,head_width =0.4, head_length =0.4, color=colorVal1)\n",
    "        ax[state_loc[1], state_loc[0]].set_xticks([])\n",
    "        ax[state_loc[1], state_loc[0]].set_yticks([])\n",
    "        ax[state_loc[1], state_loc[0]].set_ylim([0,1])\n",
    "        ax[state_loc[1], state_loc[0]].annotate('{}'.format(timestamp -time_recorded), xy=(0,.6), color='g')\n",
    "        if state_loc == maze.rwd_loc[0]:\n",
    "            ax[state_loc[1], state_loc[0]].annotate('R', xy=(0,0.5), color='r')\n",
    "\n",
    "    #plt.savefig('wlastaction10.svg', format='svg')\n",
    "    fig.suptitle('EC Memory Generated Policy', fontsize=16) \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Policy Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = eu.opt_pol_map(maze)\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(maze.grid.shape[0], maze.grid.shape[1])\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cm.ScalarMappable(norm = cNorm, cmap=cmap)\n",
    "\n",
    "for state_loc in maze.useable:\n",
    "    ax[state_loc[1], state_loc[0]].cla()\n",
    "    softmax  = optimal_policy[state_loc[1],state_loc[0]]\n",
    "    ax[state_loc[1], state_loc[0]].bar(np.arange(6), softmax)\n",
    "    ax[state_loc[1], state_loc[0]].set_xticks([])\n",
    "    ax[state_loc[1], state_loc[0]].set_yticks([])\n",
    "    ax[state_loc[1], state_loc[0]].set_ylim([0,1])\n",
    "    \n",
    "    if state_loc == maze.rwd_loc[0]:\n",
    "        ax[state_loc[1], state_loc[0]].annotate('R', xy=(0,0.5), color='r')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KLD_means1 = [[],[],[],[]] # min, max, mean, median for MF\n",
    "KLD_means2 = [[],[],[],[]] # EC\n",
    "\n",
    "\n",
    "\n",
    "for i in xrange(len(policy_KLD[1])):\n",
    "    #min\n",
    "    KLD_means1[0].append(policy_KLD[0][i].min())\n",
    "    KLD_means2[0].append(policy_KLD[1][i].min())\n",
    "    #max\n",
    "    KLD_means1[1].append(policy_KLD[0][i].max())\n",
    "    KLD_means2[1].append(policy_KLD[1][i].max())\n",
    "    #mean    \n",
    "    KLD_means1[2].append(policy_KLD[0][i].mean())\n",
    "    KLD_means2[2].append(policy_KLD[1][i].mean())\n",
    "    #median\n",
    "    KLD_means1[3].append(np.median(policy_KLD[0][i]))\n",
    "    KLD_means2[3].append(np.median(policy_KLD[1][i]))\n",
    "    \n",
    "plt.figure()\n",
    "# MF\n",
    "run_mean_smoothing = 1\n",
    "#plt.fill_between(np.arange(len(KLD_means1[0])), KLD_means1[0], KLD_means1[1], facecolor='b', alpha =0.3)\n",
    "plt.plot(eu.running_mean(KLD_means1[2], run_mean_smoothing), 'b',label=\"MF Mean\",alpha=1)\n",
    "plt.plot(eu.running_mean(KLD_means1[3], run_mean_smoothing), 'b--',label=\"MF Median\",alpha=1)\n",
    "# EC\n",
    "#plt.fill_between(np.arange(len(KLD_means1[0])), KLD_means2[0], KLD_means2[1], facecolor='g', alpha =0.3)\n",
    "plt.plot(eu.running_mean(KLD_means2[2], run_mean_smoothing), 'g', label='EC Mean',alpha=1)\n",
    "plt.plot(eu.running_mean(KLD_means2[3], run_mean_smoothing), 'g--', label='EC Median',alpha=1)\n",
    "\n",
    "plt.legend(loc=0)\n",
    "#plt.xlim([0,300])\n",
    "#plt.ylim([0,50])\n",
    "plt.savefig(fig_savedir+'KLD_avgs.svg',format='svg')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_fwhm_KLD = {}\n",
    "track_fwhm_KLD[str(fwhm)] = KLD_means1, KLD_means2\n",
    "plt.figure()\n",
    "run_mean_smoothing = 10\n",
    "#colours1 = \n",
    "track_fwhm_KLD.keys()[0] = str(0.05)\n",
    "for i in xrange(len(track_fwhm_KLD)):\n",
    "    ct=i*2\n",
    "    KLD_means1 = track_fwhm_KLD[track_fwhm_KLD.keys()[i]][0]\n",
    "    KLD_means2 = track_fwhm_KLD[track_fwhm_KLD.keys()[i]][1]\n",
    "    # MF\n",
    "    #Median\n",
    "    plt.plot(eu.running_mean(KLD_means1[3], run_mean_smoothing), color=cm.tab20.colors[ct+1], label='MF '+str(track_fwhm_KLD.keys()[i]),alpha=1)\n",
    "    #Mean\n",
    "    #plt.plot(eu.running_mean(KLD_means1[2], run_mean_smoothing), color=cm.tab20.colors[ct+1], linestyle ='--',alpha=1)\n",
    "    # EC \n",
    "    #Median\n",
    "    plt.plot(eu.running_mean(KLD_means2[3], run_mean_smoothing), color=cm.tab20.colors[ct], label='EC '+str(track_fwhm_KLD.keys()[i]),alpha=1)\n",
    "    #Mean\n",
    "    #plt.plot(eu.running_mean(KLD_means2[2], run_mean_smoothing), color=cm.tab20.colors[ct],linestyle='--', alpha=1)\n",
    "\n",
    "plt.suptitle('Median KLD For Different Place Cell FWHM Values')\n",
    "plt.legend(loc=0)\n",
    "plt.xlim([0,300])\n",
    "#plt.ylim([0,50])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make KLD Maps over time \n",
    "reload(eu)\n",
    "eu.print_value_maps(maze,policy_KLD[0],title='MF KLD',save_dir=fig_savedir)\n",
    "eu.print_value_maps(maze,policy_KLD[1],title='EC KLD',save_dir=fig_savedir, val_range=(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "infopolicy = optimal_policy\n",
    "fig, ax = plt.subplots(maze.grid.shape[0], maze.grid.shape[1])\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cm.ScalarMappable(norm = cNorm, cmap=cmap)\n",
    "\n",
    "for state_loc in maze.useable:\n",
    "    ax[state_loc[1], state_loc[0]].cla()\n",
    "    softmax  = infopolicy[state_loc[1],state_loc[0]]\n",
    "    ax[state_loc[1], state_loc[0]].bar(np.arange(6), softmax)\n",
    "    ax[state_loc[1], state_loc[0]].set_xticks([])\n",
    "    ax[state_loc[1], state_loc[0]].set_yticks([])\n",
    "    ax[state_loc[1], state_loc[0]].set_ylim([0,1])\n",
    "    \n",
    "    if state_loc == maze.rwd_loc[0]:\n",
    "        ax[state_loc[1], state_loc[0]].annotate('R', xy=(0,0.5), color='r')\n",
    "plt.suptitle('Optimal Policy', fontsize=14)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(1)\n",
    "infopolicy = MF_pol_map.map\n",
    "fig, ax = plt.subplots(maze.grid.shape[0], maze.grid.shape[1])\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cm.ScalarMappable(norm = cNorm, cmap=cmap)\n",
    "\n",
    "for state_loc in maze.useable:\n",
    "    ax[state_loc[1], state_loc[0]].cla()\n",
    "    softmax  = infopolicy[state_loc[1],state_loc[0]]\n",
    "    ax[state_loc[1], state_loc[0]].bar(np.arange(6), softmax)\n",
    "    ax[state_loc[1], state_loc[0]].set_xticks([])\n",
    "    ax[state_loc[1], state_loc[0]].set_yticks([])\n",
    "    ax[state_loc[1], state_loc[0]].set_ylim([0,1])\n",
    "    \n",
    "    if state_loc == maze.rwd_loc[0]:\n",
    "        ax[state_loc[1], state_loc[0]].annotate('R', xy=(0,0.5), color='r')\n",
    "plt.suptitle('MF Policy', fontsize=14)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "infopolicy = EC_pol_map.map\n",
    "fig, ax = plt.subplots(maze.grid.shape[0], maze.grid.shape[1])\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cm.ScalarMappable(norm = cNorm, cmap=cmap)\n",
    "\n",
    "for state_loc in maze.useable:\n",
    "    ax[state_loc[1], state_loc[0]].cla()\n",
    "    softmax  = infopolicy[state_loc[1],state_loc[0]]\n",
    "    ax[state_loc[1], state_loc[0]].bar(np.arange(6), softmax)\n",
    "    ax[state_loc[1], state_loc[0]].set_xticks([])\n",
    "    ax[state_loc[1], state_loc[0]].set_yticks([])\n",
    "    ax[state_loc[1], state_loc[0]].set_ylim([0,1])\n",
    "    \n",
    "    if state_loc == maze.rwd_loc[0]:\n",
    "        ax[state_loc[1], state_loc[0]].annotate('R', xy=(0,0.5), color='r')\n",
    "plt.suptitle('EC Policy', fontsize=14)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Occupancy Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if track_occupancy: \n",
    "    inde = 0\n",
    "    plt.figure()\n",
    "    plt.pcolor(occ_list[inde], cmap=\"Spectral_r\", vmin=0, vmax = 300)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axes().set_aspect('equal', 'datalim')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Confidence Score Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if track_CS:\n",
    "    plt.figure()\n",
    "    plt.plot(track_confidence_score[0], 'r', label='rwd')\n",
    "    plt.plot(track_confidence_score[1], 'b', label='cs')\n",
    "    plt.legend(loc=0)\n",
    "    plt.xlim([0, 8000])\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Reward Switching Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if midrun_rwd_removal:\n",
    "    plt.figure(0)\n",
    "    colours = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    for i,item in enumerate(maze.orig_rwd_loc):\n",
    "        plt.plot(reward_tally[item],colours[i], label=\"Port{}\".format(item), alpha = 0.5)\n",
    "    #plt.plot(reward_tally[1],'g', label=\"Port2\", alpha = 0.5)\n",
    "    #plt.plot(reward_tally[2],'r', label=\"Port3\", alpha = 0.5)\n",
    "    plt.legend(loc=0)\n",
    "    #plt.xlim([1700,2000])\n",
    "    #plt.ylim([0,50])\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================\n",
    "### generate a series of plots of what the policy map is at each trial\n",
    "plt.figure()\n",
    "eu.width = 5\n",
    "eu.height = 5\n",
    "for j, iterable in enumerate(check_EC_pols):\n",
    "    if j < 25:\n",
    "        fig, ax = plt.subplots(eu.width,eu.height)\n",
    "        axes = np.ravel(ax)\n",
    "        data = np.ravel(check_EC_pols[j])\n",
    "        for i, item in enumerate(axes): \n",
    "            item.bar(np.arange(6), list(data[i]))\n",
    "            item.set_ylim([0,1])\n",
    "            item.set_yticks([])\n",
    "            item.set_xticks([])\n",
    "        #plt.savefig('pol_tests/pol_test{}.svg'.format(j), format='svg')\n",
    "        plt.close()\n",
    "\n",
    "    elif j % 10 == 0: \n",
    "        fig, ax = plt.subplots(eu.height,eu.width)\n",
    "        axes = np.ravel(ax)\n",
    "        data = np.ravel(check_EC_pols[j])\n",
    "        for i, item in enumerate(axes): \n",
    "            item.bar(np.arange(6), list(data[i]))\n",
    "            item.set_ylim([0,1])\n",
    "            item.set_yticks([])\n",
    "            item.set_xticks([])\n",
    "        #plt.savefig('pol_tests/pol_test{}.svg'.format(j), format='svg')\n",
    "        plt.close()\n",
    "#plt.show()\n",
    "\n",
    "#==============================================================================\n",
    "### don't know what is for, moving to junkpile -- May 23/18\n",
    "if use_EC:\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(maze.grid.shape[0], maze.grid.shape[1])\n",
    "    visits = np.zeros(maze.grid.shape)\n",
    "    time__ = np.zeros(maze.grid.shape)\n",
    "    colours = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    for i in test_frame:\n",
    "        state_loc = i[0]\n",
    "        av = i[1]\n",
    "        #time = i[2]\n",
    "        softmax = i[3]\n",
    "\n",
    "        if i[2] > time__[state_loc[1],state_loc[0]]:\n",
    "            ax[state_loc[1], state_loc[0]].cla()\n",
    "            ax[state_loc[1], state_loc[0]].bar(np.arange(6), softmax, label=time, alpha = .8, color = colours[int(visits[state_loc[1],state_loc[0]])])\n",
    "            ax[state_loc[1], state_loc[0]].set_xticks([])\n",
    "            ax[state_loc[1], state_loc[0]].set_yticks([])\n",
    "            ax[state_loc[1], state_loc[0]].set_ylim([0,1])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        visits[state_loc[1],state_loc[0]] += 1\n",
    "        time__[state_loc[1],state_loc[0]] = i[2]\n",
    "\n",
    "    plt.savefig('no_h_policies.svg',format='svg')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
