{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from importlib import reload\n",
    "from modules import * \n",
    "fig_savedir = '../data/figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {\n",
    "    'y_height':   20,\n",
    "    'x_width':    20,\n",
    "    'walls':      False,\n",
    "    'rho':        0,\n",
    "    'maze_type':  'none',\n",
    "    'port_shift': 'none'\n",
    "}\n",
    "\n",
    "agent_params = {\n",
    "    'load_model':   False,\n",
    "    'load_dir':     '' ,#'../data/outputs/gridworld/MF{}{}training.pt'.format(maze.x, maze.y)\n",
    "    'action_dims':  6, #=len(maze.actionlist)\n",
    "    'batch_size':   1,\n",
    "    'gamma':        0.98, #discount factor\n",
    "    'eta':          5e-4,\n",
    "    'temperature':  1,\n",
    "    'use_EC':       False,\n",
    "    'cachelim':     300, #int(0.75*np.prod(maze.grid.shape)) # memory limit should be ~75% of #actions x #states\n",
    "    'state_type':   'conv'\n",
    "}\n",
    "\n",
    "run_dict = {\n",
    "    'NUM_EVENTS':   100,\n",
    "    'NUM_TRIALS':   100,\n",
    "    'print_freq':   1/10,\n",
    "    'total_loss':   [[],[]],\n",
    "    'total_reward': [],\n",
    "    'val_maps':     [],\n",
    "    'policies':     [{},{}],\n",
    "    'deltas':       [],\n",
    "    'spots':        [],\n",
    "    'vls':          []\n",
    "}\n",
    "\n",
    "#make environment\n",
    "maze = eu.gridworld(grid_params)\n",
    "maze.set_rwd([(5,5)]) # [(int(y_height/2),int(x_width/2))]\n",
    "\n",
    "#if grid_params['maze_type'] is not 'triple_reward':\n",
    "#    for i in maze.rwd_loc: \n",
    "#        maze.orig_rwd_loc.append(i)\n",
    "\n",
    "agent_params = sg.gen_input(maze, agent_params)\n",
    "\n",
    "env = eu.gymworld(maze)\n",
    "\n",
    "if agent_params['load_model']: \n",
    "    MF = ac.torch.load(agent_params['load_dir']) # load previously saved model\n",
    "else:\n",
    "    MF = ac.AC_Net(agent_params)\n",
    "opt = ac.optim.Adam(MF.parameters(), lr = agent_params['eta'])\n",
    "\n",
    "EC = ec.ep_mem(MF,agent_params['cachelim']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# empty data frames for recording\n",
    "# --------------------------------\n",
    "\n",
    "# flags to be changed mid run\n",
    "add_episodic_cache = False  ## Possibly unnecessary now \n",
    "if add_episodic_cache:\n",
    "    rwd_threshold  = True\n",
    "midrun_rwd_removal = False\n",
    "success_benchmark  = (run_dict['NUM_EVENTS'] -((maze.y-1)+(maze.x-1)))/run_dict['NUM_EVENTS']\n",
    "print(success_benchmark)\n",
    "\n",
    "if midrun_rwd_removal: \n",
    "    reward_tally = {}\n",
    "    for _ in maze.rwd_loc: \n",
    "        reward_tally[_] = []\n",
    "    trial_rwd_switch = 0\n",
    "    \n",
    "use_EC = agent_params['use_EC']\n",
    "EC.reset_cache()\n",
    "EC.reward_unseen = True\n",
    "\n",
    "add_mem_dict     = {} #dictionary of items which get put into memory cache\n",
    "timestamp        = 0\n",
    "blocktime        = time.time()\n",
    "#==================================\n",
    "# Run Trial\n",
    "#==================================\n",
    "for trial in range(run_dict['NUM_TRIALS']):\n",
    "    trialstart_stamp = timestamp\n",
    "    \n",
    "    reward_sum   = 0\n",
    "    v_last       = 0\n",
    "    track_deltas = []\n",
    "    track_spots  = []\n",
    "    visited_locs = []\n",
    "    \n",
    "    if agent_params['state_type'] == 'pcs':\n",
    "        get_pcs = pcs.activity(env.reset())\n",
    "        state   = ac.Variable(ac.torch.FloatTensor(get_pcs))\n",
    "    elif agent_params['state_type'] == 'conv':\n",
    "        env.reset() \n",
    "        frame = np.expand_dims(sg.get_frame(maze), axis=0) # because we need to include batch size of 1\n",
    "        state = ac.Variable(ac.torch.FloatTensor(frame))\n",
    "        \n",
    "    MF.reinit_hid() #reinit recurrent hidden layers\n",
    "    for event in range(run_dict['NUM_EVENTS']):\n",
    "        # pass state through EC module\n",
    "        if use_EC:\n",
    "            policy_, value_, lin_act_ = MF(state,agent_params['temperature'])\n",
    "            add_mem_dict['state'] = maze.cur_state\n",
    "            visited_locs.append(maze.cur_state)\n",
    "        else: \n",
    "            policy_, value_ = MF(state, agent_params['temperature'])[0:2]\n",
    "        \n",
    "        choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "        if event < run_dict['NUM_EVENTS']: \n",
    "            next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "        MF.rewards.append(reward)\n",
    "        delta = reward + agent_params['gamma']*value - v_last  #compute eligibility trace/rpe approximation\n",
    "\n",
    "        \n",
    "        if use_EC:\n",
    "            add_mem_dict['activity']  = tuple(lin_act_.view(-1).data)\n",
    "            add_mem_dict['action']    = choice\n",
    "            add_mem_dict['delta']     = delta\n",
    "            add_mem_dict['timestamp'] = timestamp            \n",
    "            EC.add_mem(add_mem_dict, keep_hist = True)             #add event to memory cache\n",
    "            if reward != 0:\n",
    "                EC.reward_update(trialstart_stamp, timestamp, reward)\n",
    "            #EC.reward_update(trialstart_stamp, timestamp, delta[0])\n",
    "            track_deltas.append(delta[0])\n",
    "            track_spots.append(maze.cur_state)\n",
    "        \n",
    "        if agent_params['state_type'] == 'pcs':\n",
    "            state = ac.Variable(ac.torch.FloatTensor(pcs.activity(next_state)))       # update state\n",
    "        elif agent_params['state_type'] == 'conv':\n",
    "            # because we need to include batch size of 1 \n",
    "            frame = np.expand_dims(sg.get_frame(maze), axis = 0)\n",
    "            state = ac.Variable(ac.torch.FloatTensor(frame))\n",
    "        reward_sum += reward\n",
    "    \n",
    "        v_last = value\n",
    "        timestamp += 1\n",
    "    \n",
    "    \n",
    "    if add_episodic_cache:\n",
    "        if (np.array(total_reward[-50:]).mean() > success_benchmark*NUM_EVENTS):\n",
    "            if rwd_threshold:\n",
    "                print(\" \\t Started Memory at Trial \", trial)\n",
    "                if midrun_rwd_removal:\n",
    "                    maxsums = {}\n",
    "                    for item in reward_tally.items():\n",
    "                        maxsums[item[0]] = sum(item[1])\n",
    "                    most_rewarded_location = max(maxsums.iteritems(), key=operator.itemgetter(1))[0] \n",
    "                    maze.rwd_loc.remove(most_rewarded_location)\n",
    "                    trial_rwd_switch = trial\n",
    "                    print(\"removed reward at \", most_rewarded_location)\n",
    "\n",
    "                rwd_threshold = False\n",
    "                use_EC = True\n",
    "    \n",
    "    if midrun_rwd_removal:\n",
    "        if (trial_rwd_switch!=0) and (trial == trial_rwd_switch + 1000):\n",
    "            maze.rwd_loc.append(most_rewarded_location)\n",
    "\n",
    "    p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "    \n",
    "    run_dict['total_loss'][0].append(p_loss.data[0])\n",
    "    run_dict['total_loss'][1].append(v_loss.data[0])\n",
    "    run_dict['total_reward'].append(reward_sum)\n",
    "    \n",
    "    if agent_params['state_type'] == 'pcs':\n",
    "        value_map = ac.generate_values(maze,MF,pcs=pcs)\n",
    "    else:\n",
    "        value_map = ac.generate_values(maze,MF)\n",
    "    run_dict['val_maps'].append(value_map.copy())\n",
    "    \n",
    "    if midrun_rwd_removal:\n",
    "        for item in maze.reward_tally.items():\n",
    "            reward_tally[item[0]].append(item[1])\n",
    "            \n",
    "    run_dict['deltas'].append(track_deltas)\n",
    "    run_dict['spots'].append(track_spots)\n",
    "    run_dict['vls'].append(visited_locs)\n",
    "    if trial ==0 or trial%100==0 or trial == run_dict['NUM_TRIALS']-1:\n",
    "        EC_policies, MF_policies = ac.generate_values(maze, MF,EC=EC)\n",
    "        run_dict['policies'][0]['{}'.format(trial)] = EC_policies\n",
    "        run_dict['policies'][1]['{}'.format(trial)] = MF_policies\n",
    "        #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime))\n",
    "        print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime))\n",
    "        blocktime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.plot(total_loss[0], label='p')\n",
    "plt.plot(total_loss[1], label='v')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(total_reward, label='r', color='r', marker='')\n",
    "plt.axhline(NUM_EVENTS-(grid_params['y_height']-maze.rwd_loc[0][0] + grid_params['x_width']-maze.rwd_loc[0][1]), color = 'gray', linestyle='--')\n",
    "#plt.figure(2)\n",
    "#delta_of_interest = deltas[-1]\n",
    "#plt.plot(deltas[-1])\n",
    "#plt.annotate('{}'.format(spots[-1][np.argmax(delta_of_interest)]), xy=(np.argmax(delta_of_interest), max(delta_of_interest)) )\n",
    "#print(maze.rwd_loc[0])\n",
    "\n",
    "#if maze_type == 'triple_reward':\n",
    "#    plt.figure(2)\n",
    "#    plt.plot(reward_tally[reward_tally.keys()[0]], label='{}'.format(reward_tally.keys()[0]))\n",
    "#    plt.plot(reward_tally[reward_tally.keys()[1]], label='{}'.format(reward_tally.keys()[1]))\n",
    "#    plt.plot(reward_tally[reward_tally.keys()[2]], label='{}'.format(reward_tally.keys()[2]))\n",
    "#    plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(eu)\n",
    "eu.print_value_maps(maze,\n",
    "                    val_maps,\n",
    "                    maps=0,#list(np.arange(850, 1050)), \n",
    "                    val_range=(-1,50),\n",
    "                    save_dir=fig_savedir,\n",
    "                    title='Value Map') ### see individual map with kwarg maps=X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(eu)\n",
    "EC_policy = np.zeros((maze.y, maze.x), dtype=[('N', 'f8'), ('E', 'f8'),('W', 'f8'), ('S', 'f8'),('stay', 'f8'), ('poke', 'f8')])\n",
    "ex = EC_policy[0][0]\n",
    "for i in EC.cache_list.keys():\n",
    "    loc = EC.cache_list[i][2]\n",
    "    pol = EC.cache_list[i][0]\n",
    "    t = EC.cache_list[i][1]\n",
    "    if max(tuple(eu.softmax(pol))) > max(EC_policy[loc[1]][loc[0]]):\n",
    "        EC_policy[loc[1]][loc[0]] = tuple(eu.softmax(pol))\n",
    "    #    previous_pol = np.asarray(EC_policy[loc[1]][loc[0]])\n",
    "    #    print( previous_pol,np.asarray(eu.softmax(pol) ))\n",
    "    #    new_pol = tuple(eu.softmax(np.add(np.asarray(eu.softmax(pol)),previous_pol)))\n",
    "    #    EC_policy[loc[1]][loc[0]] = tuple(eu.softmax(pol))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(eu)\n",
    "test1 = [int(po) for po in policies[1].keys()]\n",
    "\n",
    "epoch = max(test1)\n",
    "print(epoch)\n",
    "eu.make_dual_policy_plots(maze, EC_policy, policies[1][str(epoch)], savedir='PolMaps.svg')#EC_policies, MF_policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "cmap      = plt.cm.Spectral_r\n",
    "cNorm     = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cmx.ScalarMappable(norm = cNorm, cmap=cmap)\n",
    "\n",
    "\n",
    "ax1 = fig.add_axes([0.04, 0, 0.85, 0.85]) # [left, bottom, width, height]\n",
    "axc = fig.add_axes([0.89, 0.125, 0.05, 0.6])\n",
    "cb1 = colorbar.ColorbarBase(axc, cmap=cmap, norm=cNorm)\n",
    "\n",
    "ax1.imshow(maze.grid, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax1.add_patch(patches.Circle((10,10), 0.35, fc='k', ec='w'))\n",
    "#ax1.add_patch(patches.Circle(maze.start_loc, 0.5, fc='b', ec='w'))\n",
    "ax1.add_patch(patches.Circle(maze.rwd_loc[0], 0.35, fc='w'))\n",
    "\n",
    "codes = [Path.MOVETO]\n",
    "for i in range(len(vls[0])-1):\n",
    "    codes.append(Path.LINETO)\n",
    "\n",
    "for i in range(len(vls)):\n",
    "    path = Path(vls[i], codes)\n",
    "    patch = patches.PathPatch(path, edgecolor='gray', facecolor='none', linestyle=':', lw=2)\n",
    "    ax1.add_patch(patch)\n",
    "\n",
    "chance_threshold = np.round(1/len(maze.actionlist),6)\n",
    "use_recency      = True\n",
    "\n",
    "for i in EC.cache_list.keys():\n",
    "    loc = EC.cache_list[i][2]\n",
    "    pol = eu.softmax(EC.cache_list[i][0])\n",
    "    t   = EC.cache_list[i][1]\n",
    "    \n",
    "    if use_recency: \n",
    "        if t<101:\n",
    "            recency = 1\n",
    "        else:\n",
    "            recency = 0\n",
    "        #recency = abs(1-t/(trial*NUM_EVENTS))\n",
    "        #if recency > 1: \n",
    "        #    recency = 1\n",
    "    else:\n",
    "        recency = 1 \n",
    "    \n",
    "    action = np.argmax(pol)\n",
    "    prob   = max(pol)\n",
    "\n",
    "\n",
    "    dx1,dy1,head_w,head_l = eu.make_arrows(action, prob)\n",
    "    \n",
    "    if prob > chance_threshold:\n",
    "        if (dx1, dy1) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal1 = scalarMap.to_rgba(prob)\n",
    "            ax1.arrow(loc[0], loc[1], \n",
    "                      dx1, dy1, \n",
    "                      head_width  = 0.3, \n",
    "                      head_length = 0.2, \n",
    "                      color       = colorVal1, \n",
    "                      alpha       = recency)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "plt.savefig('ECpol_T_inf_newrwd.svg', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(visited_locs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dict = {}\n",
    "for prop, propval in vars(maze).iteritems():\n",
    "    grid_dict[prop]= propval\n",
    "store_data= {}\n",
    "store_data['total_reward'] = total_reward\n",
    "store_data['total_loss'] = total_loss\n",
    "store_data['val_maps'] = val_maps\n",
    "store_data['EC'] = EC\n",
    "store_data['MF'] = MF\n",
    "if use_EC:\n",
    "    store_data['policies'] = policies\n",
    "store_data['params'] = {'grid':grid_dict,\n",
    "                       'state_type':state_type,\n",
    "                       'discount_factor': discount_factor,\n",
    "                       'eta':eta, \n",
    "                       'runtime':[NUM_EVENTS, NUM_TRIALS]}\n",
    "store_data['maze'] = maze\n",
    "datestamp = time.strftime(\"%y%m%d_%H%M\", time.localtime())\n",
    "np.save('../data/outputs/gridworld/pydicts/{}.npy'.format(datestamp), store_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving model\n",
    "ac.torch.save(MF,'../data/outputs/gridworld/MF2020training.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "state_type = agent_params['state_type']\n",
    "if state_type == 'conv':\n",
    "    num_channels = 3\n",
    "    if maze.bound:\n",
    "        input_dims = (y_height+2, x_width+2, num_channels)\n",
    "    else:\n",
    "        input_dims = (y_height, x_width, num_channels)\n",
    "    hid_types = ['conv', 'pool', 'linear']\n",
    "    conv_dims = ac.conv_output(input_dims)\n",
    "    pool_dims = ac.conv_output(conv_dims)\n",
    "    hid_dims = [conv_dims, pool_dims, 500]\n",
    "\n",
    "elif state_type == 'pcs':\n",
    "    input_dims = 1000\n",
    "    hid_types = ['linear']\n",
    "    hid_dims = [500]\n",
    "    \n",
    "action_dims = len(maze.actionlist)\n",
    "batch_size = 1\n",
    "\n",
    "NUM_EVENTS = 100\n",
    "NUM_TRIALS = 5000\n",
    "\n",
    "discount_factor = 0.98\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
