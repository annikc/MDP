{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "gettting an error with persistence values when trying to select which memory to replace\n",
    "made snapshot function in actorcritic for collecting pol/val in each state\n",
    "'''\n",
    "\n",
    "from __future__ import division\n",
    "from importlib import reload\n",
    "from modules import * \n",
    "import csv\n",
    "import pickle\n",
    "fig_savedir = '../data/figures/'\n",
    "\n",
    "grid_params = {\n",
    "    'y_height':   10, \n",
    "    'x_width':    10,\n",
    "    'walls':      False,\n",
    "    'rho':        0,\n",
    "    'maze_type':  'none',\n",
    "    'port_shift': 'none'\n",
    "}\n",
    "\n",
    "\n",
    "agent_params = {\n",
    "    'load_model':   False,\n",
    "    'load_dir':     '../data/outputs/gridworld/MF{}{}training.pt'.format(grid_params['x_width'],grid_params['y_height']),\n",
    "    'action_dims':  6, #=len(maze.actionlist)\n",
    "    'lin_dims':     500,\n",
    "    'batch_size':   1,\n",
    "    'gamma':        0.98, #discount factor\n",
    "    'eta':          5e-4,\n",
    "    'temperature':  1,\n",
    "    'use_EC':       False,\n",
    "    'cachelim':     100, # memory limit should be ~75% of #actions x #states\n",
    "    'state_type':   'conv'\n",
    "}\n",
    "\n",
    "run_dict = {\n",
    "    'NUM_EVENTS':   500, ## max number of events the agent has to find reward\n",
    "    'NUM_TRIALS':   1000,\n",
    "    'print_freq':   1/10,\n",
    "    'rwd_placement':'training_loc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make environment\n",
    "maze = eu.gridworld(grid_params)\n",
    "if agent_params['load_model'] == True:\n",
    "    if run_dict['rwd_placement'] == 'training_loc':\n",
    "        maze.set_rwd([(int(grid_params['y_height']/2),int(grid_params['x_width']/2))])\n",
    "    if run_dict['rwd_placement'] == 'moved_loc':\n",
    "        maze.set_rwd([(int(grid_params['y_height']/4),int(3*grid_params['x_width']/4))])\n",
    "else:\n",
    "    maze.set_rwd([(int(grid_params['y_height']/2),int(grid_params['x_width']/2))])\n",
    "env = eu.gymworld(maze) # openAI-like wrapper \n",
    "\n",
    "#update agent params dictionary with layer sizes appropriate for environment \n",
    "agent_params = sg.gen_input(maze, agent_params)\n",
    "MF,opt = ac.make_agent(agent_params, freeze=False)\n",
    "gp.plot_env(maze)\n",
    "\n",
    "\n",
    "agent_params['cachelim'] = int(0.5*np.prod(maze.grid.shape))\n",
    "\n",
    "EC = ec.ep_mem(MF,agent_params['cachelim']) \n",
    "print(maze.cur_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function for runs with episodic mem and without -- take use_EC as a param\n",
    "# assume just for conv inputs \n",
    "def run_trials(run_dict, use_EC, **kwargs):\n",
    "    save_data  = kwargs.get('save', True)\n",
    "    NUM_TRIALS = run_dict['NUM_TRIALS']\n",
    "    NUM_EVENTS = run_dict['NUM_EVENTS']\n",
    "    \n",
    "    blocktime = time.time()\n",
    "    \n",
    "    if use_EC:\n",
    "        EC.reset_cache()\n",
    "        memory_buffer = [[],[],[],[], 0] # [timestamp, state_t, a_t, readable_state, trial]\n",
    "        run_dict['total_loss']   = [[],[]]\n",
    "        run_dict['total_reward'] = []\n",
    "        run_dict['track_cs']     = [[],[]]\n",
    "        run_dict['rpe']          = np.zeros(maze.grid.shape)\n",
    "        run_dict['trial_length'] = []\n",
    "        \n",
    "        reward    = 0\n",
    "        timestamp = 0\n",
    "        tslr      = np.nan_to_num(np.inf)\n",
    "        \n",
    "        compare_mfec = {}\n",
    "        \n",
    "        for trial in range(NUM_TRIALS):\n",
    "            trialstart_stamp = timestamp\n",
    "\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "\n",
    "            env.reset() \n",
    "            \n",
    "            state = torch.Tensor(sg.get_frame(maze))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "            print(maze.cur_state)\n",
    "            for event in range(NUM_EVENTS):\n",
    "                if trial is not 0:\n",
    "                    #compute confidence in MFC\n",
    "                    if event in [0,1]:\n",
    "                        MF_cs = EC.make_pvals(tslr,envelope=10)\n",
    "                    else: \n",
    "                        MF_cs = EC.make_pvals(tslr,envelope=10, pol_id = pol_flag, mfc=MF_cs)\n",
    "                    # pass state through EC module\n",
    "                else:\n",
    "                    MF_cs = EC.make_pvals(tslr,envelope=10)\n",
    "                policy_, value_, lin_act_ = MF(state, temperature = 1)\n",
    "                lin_act = tuple(np.round(lin_act_.data[0].numpy(),4))\n",
    "                \n",
    "                if trial == 0: \n",
    "                    choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "                else:\n",
    "                    if event is not 0:\n",
    "                        ec_pol = torch.from_numpy(EC.recall_mem(lin_act, timestamp, env=150))\n",
    "                        candidate_policies = [policy_, ec_pol]\n",
    "                        pol_choice = np.random.choice([0,1], p=[MF_cs, 1-MF_cs])\n",
    "                        pol = candidate_policies[pol_choice]\n",
    "                        if pol_choice == 0:\n",
    "                            pol_flag = 'MF'\n",
    "                        else:\n",
    "                            pol_flag = 'EC'\n",
    "                        choice, policy, value = ac.select_ec_action(MF, policy_, value_, pol)\n",
    "                    else:\n",
    "                        choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "                \n",
    "                memory_buffer[3].append(maze.cur_state)\n",
    "                \n",
    "                compare_mfec[maze.cur_state] = [policy.numpy(), choice, value]\n",
    "                \n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "                if event is not 0:\n",
    "                    if reward == 1:\n",
    "                        tslr = 0\n",
    "                        \n",
    "                    else:\n",
    "                        tslr += 1\n",
    "                        reward = -0.01\n",
    "                    run_dict['track_cs'][0].append(tslr)\n",
    "                    run_dict['track_cs'][1].append(MF_cs)\n",
    "                \n",
    "                MF.rewards.append(reward)\n",
    "                \n",
    "                memory_buffer[0].append(timestamp)\n",
    "                memory_buffer[1].append(lin_act)\n",
    "                memory_buffer[2].append(choice)\n",
    "                memory_buffer[4] = trial\n",
    "                \n",
    "                # because we need to include batch size of 1 \n",
    "                state = torch.Tensor(sg.get_frame(maze))\n",
    "                reward_sum += reward\n",
    "\n",
    "                v_last = value\n",
    "                timestamp += 1\n",
    "                if reward == 1:\n",
    "                    run_dict['trial_length'].append(event)\n",
    "                    #print(f'{trial}:{event}')\n",
    "                    break\n",
    "                if event == NUM_EVENTS-1:\n",
    "                    run_dict['trial_length'].append(event)\n",
    "            \n",
    "\n",
    "            p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt,cache=EC, buffer=memory_buffer)\n",
    "            \n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                #run_dict['total_loss'][0].append(p_loss.data[0])\n",
    "                #run_dict['total_loss'][1].append(v_loss.data[0])\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "            #if reward ==1:\n",
    "            #    break\n",
    "                \n",
    "            if trial ==0 or trial%10==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()\n",
    "            run_dict['comp_mfec'] = compare_mfec\n",
    "            \n",
    "    else:\n",
    "        run_dict['rpe'] = np.zeros(maze.grid.shape)\n",
    "        run_dict['total_loss']   = [[],[]]\n",
    "        run_dict['total_reward'] = []\n",
    "        run_dict['trial_length'] = []\n",
    "        compare_mfec = {} #[[],[],[],[]]\n",
    "\n",
    "        for trial in range(NUM_TRIALS):\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "            track_deltas = []\n",
    "            track_spots  = []\n",
    "            visited_locs = []\n",
    "\n",
    "            env.reset()\n",
    "            state = torch.Tensor(sg.get_frame(maze))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "            for event in range(NUM_EVENTS):\n",
    "                policy_, value_ = MF(state, agent_params['temperature'])[0:2]\n",
    "                choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "                \n",
    "                compare_mfec[maze.cur_state] = [policy.numpy(), choice, value]\n",
    "                \n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "                if reward != 1:\n",
    "                    reward = -0.01\n",
    "                MF.rewards.append(reward)\n",
    "                \n",
    "                #compute eligibility trace/rpe approximation\n",
    "                delta = reward + agent_params['gamma']*value - v_last  \n",
    "                state = torch.Tensor(sg.get_frame(maze))\n",
    "                run_dict['rpe'][maze.cur_state[1]][maze.cur_state[0]] = delta\n",
    "                \n",
    "                reward_sum += reward\n",
    "                v_last = value\n",
    "                \n",
    "                if reward == 1:\n",
    "                    run_dict['trial_length'].append(event)\n",
    "                    #print(f'{trial}:{event}')\n",
    "                    break\n",
    "                if event == NUM_EVENTS-1:\n",
    "                    run_dict['trial_length'].append(event)\n",
    "            p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "            \n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                run_dict['total_loss'][0].append(p_loss.item())\n",
    "                run_dict['total_loss'][1].append(v_loss.item())\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "\n",
    "            if trial ==0 or trial%100==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()\n",
    "            run_dict['comp_mfec'] = compare_mfec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_trials(run_dict, True) #false = mf only, true = w ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(EC.cache_list.keys()))\n",
    "#print(run_dict['comp_mfec'])\n",
    "plt.plot(run_dict['trial_length'], 'b')\n",
    "plt.plot(run_dict['total_reward'], 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c_ec = EC.cache_list.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cme = run_dict['comp_mfec']\n",
    "pickle.dump(cme, open('mf_run.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp= 1\n",
    "compare_policies = {}\n",
    "for i in range(len(EC.cache_list.keys())):\n",
    "    item = EC.cache_list[list(EC.cache_list.keys())[i]]\n",
    "    compare_policies[item[2]] = [ec.softmax(np.nan_to_num(item[0][:,0]), T = temp), item[1]]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap = cmap)\n",
    "\n",
    "ax1 = fig.add_axes([0.04, 0, 0.85, 0.85])\n",
    "axc = fig.add_axes([0.85, 0, 0.05, 0.85])\n",
    "\n",
    "cb1 = colorbar.ColorbarBase(axc, cmap = cmap, norm = cNorm)\n",
    "ax1.imshow(maze.grid, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "\n",
    "for entry in compare_policies.keys():\n",
    "    x = entry[0]\n",
    "    y = entry[1]\n",
    "    \n",
    "    ec_policy = compare_policies[entry][0]\n",
    "    action = np.argmax(ec_policy)\n",
    "    prob = max(ec_policy)\n",
    "    \n",
    "    timestamp = compare_policies[entry][1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    dx1, dy1, head_w, head_l = gp.make_arrows(action,prob)\n",
    "    if prob > 1/100:\n",
    "        if (dx1, dy1) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal1 = scalarMap.to_rgba(prob)\n",
    "            ax1.add_patch(patches.Rectangle([x,y],1,1, fc = colorVal1))\n",
    "            ax1.annotate(f'{timestamp}', np.add((x,y), (0.45, 0.45)), size=26)\n",
    "            #ax1.arrow(x,y, dx1, dy1, head_width=0.3, head_length=0.2, color = colorVal1)\n",
    "    else:\n",
    "        pass\n",
    "ax1.add_patch(patches.Circle(maze.rwd_loc[0], 0.35, fc='w'))\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "#f, axar = plt.subplots(len(EC.cache_list.keys()), 1, sharex=True, sharey=True)\n",
    "#for i, entry in enumerate(compare_policies.keys()):    \n",
    "#    axar[i].bar(np.arange(6), compare_policies[entry])\n",
    "#axar[0].set_ylim([0,1])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp= 1\n",
    "compare_policies = {}\n",
    "for i in range(len(EC.cache_list.keys())):\n",
    "    item = EC.cache_list[list(EC.cache_list.keys())[i]]\n",
    "    compare_policies[item[2]] = ec.softmax(np.nan_to_num(item[0][:,0]), T = temp)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap = cmap)\n",
    "\n",
    "ax1 = fig.add_axes([0.04, 0, 0.85, 0.85])\n",
    "axc = fig.add_axes([0.85, 0, 0.05, 0.85])\n",
    "\n",
    "cb1 = colorbar.ColorbarBase(axc, cmap = cmap, norm = cNorm)\n",
    "ax1.imshow(maze.grid, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax1.add_patch(patches.Circle(maze.rwd_loc[0], 0.35, fc='w'))\n",
    "\n",
    "for entry in compare_policies.keys():\n",
    "    x = entry[0]\n",
    "    y = entry[1]\n",
    "    \n",
    "    ec_policy = compare_policies[entry]\n",
    "    action = np.argmax(ec_policy)\n",
    "    prob = max(ec_policy)\n",
    "    \n",
    "    dx1, dy1, head_w, head_l = gp.make_arrows(action,prob)\n",
    "    if prob > 1/100:\n",
    "        if (dx1, dy1) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal1 = scalarMap.to_rgba(prob)\n",
    "            ax1.arrow(x,y, dx1, dy1, head_width=0.3, head_length=0.2, color = colorVal1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "#f, axar = plt.subplots(len(EC.cache_list.keys()), 1, sharex=True, sharey=True)\n",
    "#for i, entry in enumerate(compare_policies.keys()):    \n",
    "#    axar[i].bar(np.arange(6), compare_policies[entry])\n",
    "#axar[0].set_ylim([0,1])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(run_dict['total_reward'], label='r')\n",
    "#plt.plot(run_dict['total_loss'][0], label='p')\n",
    "#plt.plot(run_dict['total_loss'][1], label='v')\n",
    "plt.legend(bbox_to_anchor=(1.1,1.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early_learn_mf_only = run_dict['total_reward']\n",
    "#early_learn_w_ec = run_dict['total_reward']\n",
    "#late_learn_mf_only = run_dict['total_reward']\n",
    "#late_learn_w_ec = run_dict['total_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(run_dict['total_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, sharex = True, sharey=True)\n",
    "ax[0].plot(early_learning_w_ec, 'b', alpha =0.7, label = 'With EC')\n",
    "ax[0].plot(early_learn_mf_only, 'k--', alpha = 0.7, label=\"MF only\")\n",
    "\n",
    "ax[1].plot(late_learn_w_ec, 'r', label='With EC')\n",
    "ax[1].plot(late_learn_mf_only, 'k--', alpha = 0.7, label ='MF only')\n",
    "\n",
    "ax[0].legend(bbox_to_anchor = (1.0,.75))\n",
    "ax[1].legend(bbox_to_anchor = (1.0,.75))\n",
    "fig.savefig('compare.svg', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([(1, 0, 0, 1), (0, 1, 0, 1), (0, 0, 1, 1)])\n",
    "\n",
    "lc = mc.LineCollection(lines, colors=c, linewidths=2)\n",
    "fig, ax = pl.subplots()\n",
    "ax.add_collection(lc)\n",
    "ax.autoscale()\n",
    "ax.margins(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [[(0, 1), (1, 1)], [(2, 3), (3, 3)], [(1, 2), (1, 3)]]\n",
    "linepoints[4] = (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(DND.cache_list))\n",
    "def cosine_sim(mem_dict, key, **kwargs):\n",
    "    similarity_threshold = kwargs.get('threshold', 0.9)\n",
    "\n",
    "    mem_cache = np.asarray(list(mem_dict.keys()))\n",
    "    print(mem_cache.shape)\n",
    "    entry = np.asarray(key)\n",
    "\n",
    "    mqt = np.dot(mem_cache, entry)\n",
    "    norm = np.linalg.norm(mem_cache, axis=1) * np.linalg.norm(entry)\n",
    "\n",
    "    cosine_similarity = mqt / norm\n",
    "\n",
    "    index = np.argmax(cosine_similarity)\n",
    "    similar_activity = mem_cache[index]\n",
    "    if max(cosine_similarity) >= similarity_threshold:\n",
    "        return similar_activity, index, max(cosine_similarity)\n",
    "\n",
    "    else:\n",
    "        # print('max memory similarity:', max(cosine_similarity))\n",
    "        return [], [], max(cosine_similarity)\n",
    "\n",
    "def make_pvals(p, **kwargs):\n",
    "    envelope = kwargs.get('envelope', 50)\n",
    "    return np.round(1 / np.cosh(p / envelope),8)\n",
    "\n",
    "def softmax(x, T=1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp((x - np.max(x))/T)\n",
    "    return e_x / e_x.sum(axis=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_key, ind, max_cs = cosine_sim(DND.cache_list, sts[0])\n",
    "print('sim', max_cs)\n",
    "memory = np.nan_to_num(DND.cache_list[tuple(state_key)][0])\n",
    "deltas = memory[:,0]\n",
    "print(deltas)\n",
    "rec_times = memory[:,1]\n",
    "print(\"rtimes\",rec_times)\n",
    "times        = abs(timestep - memory[:,1])\n",
    "print('tiems', times)\n",
    "pv =make_pvals(times)\n",
    "print(pv)\n",
    "\n",
    "mult = np.multiply(deltas, pv)\n",
    "print(softmax(max_cs*mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in sts[0:10]:\n",
    "    timestep = 1000\n",
    "    a = DND.recall_mem(key=i,timestep=timestep,env=100)\n",
    "    print(a,\"----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs =['r','g','b','c', 'y','pink']\n",
    "'''\n",
    "r = list(self.cache_list.keys())\n",
    "g = [t for e, t in self.cache_list.values()]\n",
    "b = lp = persistence_.index(min(persistence_))\n",
    "c = old_activity = cache_keys[lp]\n",
    "y = del self.cache_list[old_activity]\n",
    "'''\n",
    "plt.figure()\n",
    "for i in range(len(EC.stupid_df)):\n",
    "    xs = np.arange(len(EC.stupid_df[i]))\n",
    "    ys = EC.stupid_df[i]\n",
    "    \n",
    "    plt.scatter(xs, ys, c=cs[i], alpha=0.3)\n",
    "plt.ylim([-0.00002,0.00002])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(run_dict['total_reward'])\n",
    "plt.ylim([0,run_dict['NUM_EVENTS']])\n",
    " \n",
    "plt.figure(2)\n",
    "plt.plot(run_dict['total_loss'][0], label = 'pol')\n",
    "plt.plot(run_dict['total_loss'][1], label = 'val')\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "plt.close()\n",
    "#gp.print_value_maps(maze, run_dict['val_maps'], maps=0, val_range=(-1,50), save_dir=fig_savedir, title='Value Map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.torch.save(MF,agent_params['load_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
