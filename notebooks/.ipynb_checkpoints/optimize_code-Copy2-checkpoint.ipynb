{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from importlib import reload\n",
    "from modules import * \n",
    "import csv\n",
    "fig_savedir = '../data/figures/'\n",
    "\n",
    "grid_params = {\n",
    "    'y_height':   10,\n",
    "    'x_width':    10,\n",
    "    'walls':      False,\n",
    "    'rho':        0,\n",
    "    'maze_type':  'none',\n",
    "    'port_shift': 'none'\n",
    "}\n",
    "\n",
    "\n",
    "agent_params = {\n",
    "    'load_model':   False,\n",
    "    'load_dir':     '../data/outputs/gridworld/MF{}{}training_rwd7_2.pt'.format(grid_params['x_width'],grid_params['y_height']),\n",
    "    'action_dims':  6, #=len(maze.actionlist)\n",
    "    'lin_dims':     500,\n",
    "    'batch_size':   1,\n",
    "    'gamma':        0.98, #discount factor\n",
    "    'eta':          5e-4,\n",
    "    'temperature':  1,\n",
    "    'use_EC':       False,\n",
    "    'cachelim':     100, # memory limit should be ~75% of #actions x #states\n",
    "    'state_type':   'conv'\n",
    "}\n",
    "\n",
    "run_dict = {\n",
    "    'NUM_EVENTS':   150,\n",
    "    'NUM_TRIALS':   500,\n",
    "    'print_freq':   1/10,\n",
    "    'total_loss':   [[],[]],\n",
    "    'total_reward': [],\n",
    "    'val_maps':     [],\n",
    "    'policies':     [{},{}],\n",
    "    'deltas':       [],\n",
    "    'spots':        [],\n",
    "    'vls':          []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 5)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make environment\n",
    "maze = eu.gridworld(grid_params)\n",
    "maze.set_rwd([(int(grid_params['y_height']/2),int(grid_params['x_width']/2))])\n",
    "env = eu.gymworld(maze) # openAI-like wrapper \n",
    "\n",
    "#update agent params dictionary with layer sizes appropriate for environment \n",
    "agent_params = sg.gen_input(maze, agent_params)\n",
    "MF,opt = ac.make_agent(agent_params, freeze=False)\n",
    "gp.make_env_plots(maze,env=True)\n",
    "\n",
    "\n",
    "agent_params['cachelim'] = int(0.5*np.prod(maze.grid.shape))\n",
    "\n",
    "EC = ec.ep_mem(MF,agent_params['cachelim']) \n",
    "print(maze.rwd_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    }
   ],
   "source": [
    "for param_ in opt.param_groups:\n",
    "    print(param_['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "print(maze.grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function for runs with episodic mem and without -- take use_EC as a param\n",
    "# assume just for conv inputs \n",
    "def run_trials(run_dict, use_EC, **kwargs):\n",
    "    save_data  = kwargs.get('save', True)\n",
    "    NUM_TRIALS = run_dict['NUM_TRIALS']\n",
    "    NUM_EVENTS = run_dict['NUM_EVENTS']\n",
    "    \n",
    "    blocktime = time.time()\n",
    "    \n",
    "    if use_EC:\n",
    "        EC.reset_cache()\n",
    "        add_mem_dict = {} #dictionary of items which get put into memory cache\n",
    "        \n",
    "        run_dict['total_loss']   =  [[],[]]\n",
    "        run_dict['total_reward'] = []\n",
    "        run_dict['track_cs']     = [[],[]]\n",
    "        run_dict['rpe']          = np.zeros(maze.grid.shape)\n",
    "        \n",
    "        reward    = 0\n",
    "        timestamp = 0\n",
    "        tslr      = np.nan_to_num(np.inf)\n",
    "        \n",
    "        for trial in range(NUM_TRIALS):\n",
    "            trialstart_stamp = timestamp\n",
    "\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "\n",
    "            env.reset() \n",
    "            \n",
    "            state = torch.Tensor(sg.get_frame(maze))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "\n",
    "            for event in range(NUM_EVENTS):\n",
    "                #compute confidence in MFC\n",
    "                if event in [0,1]:\n",
    "                    MF_cs = EC.make_pvals(tslr,envelope=10)\n",
    "                else: \n",
    "                    MF_cs = EC.make_pvals(tslr,envelope=10, pol_id = pol_flag, mfc=MF_cs)\n",
    "                # pass state through EC module\n",
    "                \n",
    "                policy_, value_, lin_act_ = MF(state, temperature = 1)\n",
    "                lin_act = tuple(np.round(lin_act_.data[0].numpy(),4))\n",
    "                if event is not 0:\n",
    "                    ec_pol = torch.from_numpy(EC.recall_mem(lin_act, timestamp, env=150))\n",
    "                    candidate_policies = [policy_, ec_pol]\n",
    "                    pol_choice = np.random.choice([0,1], p=[MF_cs, 1-MF_cs])\n",
    "                    pol = candidate_policies[pol_choice]\n",
    "                    if pol_choice == 0:\n",
    "                        pol_flag = 'MF'\n",
    "                    else:\n",
    "                        pol_flag = 'EC'\n",
    "                    choice, policy, value = ac.select_ec_action(MF, policy_, value_, pol)\n",
    "                else:\n",
    "                    choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "                \n",
    "                add_mem_dict['state'] = maze.cur_state\n",
    "                #compute eligibility trace/rpe approximation\n",
    "                delta = reward + agent_params['gamma']*value - v_last  \n",
    "                run_dict['rpe'][maze.cur_state[1]][maze.cur_state[0]] = delta\n",
    "                \n",
    "                \n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "                if event is not 0:\n",
    "                    if reward == 1:\n",
    "                        tslr = 0\n",
    "                        delta = 10\n",
    "                    else:\n",
    "                        tslr += 1\n",
    "                    \n",
    "                    run_dict['track_cs'][0].append(tslr)\n",
    "                    run_dict['track_cs'][1].append(MF_cs)\n",
    "                \n",
    "                MF.rewards.append(reward)\n",
    "                \n",
    "                \n",
    "                add_mem_dict['activity']  = lin_act\n",
    "                add_mem_dict['action']    = choice\n",
    "                add_mem_dict['delta']     = delta\n",
    "                add_mem_dict['timestamp'] = timestamp            \n",
    "                \n",
    "                EC.add_mem(add_mem_dict)#add event to memory cache\n",
    "                \n",
    "                # because we need to include batch size of 1 \n",
    "                state = torch.Tensor(sg.get_frame(maze))\n",
    "                reward_sum += reward\n",
    "\n",
    "                v_last = value\n",
    "                timestamp += 1\n",
    "\n",
    "            p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "            \n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                #run_dict['total_loss'][0].append(p_loss.data[0])\n",
    "                #run_dict['total_loss'][1].append(v_loss.data[0])\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "\n",
    "            if trial ==0 or trial%10==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()\n",
    "\n",
    "    else:\n",
    "        run_dict['rpe'] = np.zeros(maze.grid.shape)\n",
    "        run_dict['total_loss'] =  [[],[]]\n",
    "        run_dict['total_reward'] = []\n",
    "        for trial in range(NUM_TRIALS):\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "            track_deltas = []\n",
    "            track_spots  = []\n",
    "            visited_locs = []\n",
    "\n",
    "            env.reset() \n",
    "            state = torch.Tensor(sg.get_frame(maze))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "\n",
    "            for event in range(NUM_EVENTS):\n",
    "                policy_, value_ = MF(state, agent_params['temperature'])[0:2]\n",
    "                choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "\n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "                MF.rewards.append(reward)\n",
    "                #compute eligibility trace/rpe approximation\n",
    "                delta = reward + agent_params['gamma']*value - v_last  \n",
    "                state = torch.Tensor(sg.get_frame(maze))\n",
    "                run_dict['rpe'][maze.cur_state[1]][maze.cur_state[0]] = delta\n",
    "                \n",
    "                reward_sum += reward\n",
    "                v_last = value\n",
    "\n",
    "            p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "\n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                run_dict['total_loss'][0].append(p_loss.item())\n",
    "                run_dict['total_loss'][1].append(v_loss.item())\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "\n",
    "            if trial ==0 or trial%100==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:38:35]  Trial 1 TotRew = 0 (0.116s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-11d343095278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#false = mf only, true = w ec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-045921bbadb9>\u001b[0m in \u001b[0;36mrun_trials\u001b[0;34m(run_dict, use_EC, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EVENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mpolicy_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temperature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mchoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mNUM_EVENTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/LINC Lab Documents/Code/MEMRL/rl_network/actorcritic.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(model, policy_, value_)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSavedAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Either `probs` or `logits` must be specified, but not both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_trials(run_dict, False) #false = mf only, true = w ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      " 0\n",
      "[21:38:39]  Trial 1 TotRew = 1 (0.115s)\n",
      "[21:38:52]  Trial 101 TotRew = 0 (12.947s)\n",
      "[21:39:05]  Trial 201 TotRew = 0 (13.264s)\n",
      "[21:39:19]  Trial 301 TotRew = 19 (13.252s)\n",
      "[21:39:32]  Trial 401 TotRew = 140 (13.335s)\n",
      "[21:39:45]  Trial 500 TotRew = 146 (13.120s)\n",
      "-----\n",
      " 1\n",
      "[21:39:45]  Trial 1 TotRew = 0 (0.157s)\n",
      "[21:39:59]  Trial 101 TotRew = 3 (13.529s)\n",
      "[21:40:12]  Trial 201 TotRew = 0 (13.190s)\n",
      "[21:40:25]  Trial 301 TotRew = 141 (13.198s)\n",
      "[21:40:38]  Trial 401 TotRew = 121 (13.240s)\n",
      "[21:40:51]  Trial 500 TotRew = 141 (13.005s)\n",
      "-----\n",
      " 2\n",
      "[21:40:51]  Trial 1 TotRew = 1 (0.151s)\n",
      "[21:41:05]  Trial 101 TotRew = 0 (13.416s)\n",
      "[21:41:18]  Trial 201 TotRew = 0 (13.273s)\n",
      "[21:41:31]  Trial 301 TotRew = 52 (13.242s)\n",
      "[21:41:45]  Trial 401 TotRew = 87 (13.275s)\n",
      "[21:41:58]  Trial 500 TotRew = 138 (13.241s)\n",
      "-----\n",
      " 3\n",
      "[21:41:58]  Trial 1 TotRew = 1 (0.162s)\n",
      "[21:42:11]  Trial 101 TotRew = 3 (13.386s)\n",
      "[21:42:25]  Trial 201 TotRew = 0 (13.194s)\n",
      "[21:42:38]  Trial 301 TotRew = 89 (13.164s)\n",
      "[21:42:51]  Trial 401 TotRew = 57 (13.169s)\n",
      "[21:43:04]  Trial 500 TotRew = 140 (13.018s)\n",
      "-----\n",
      " 4\n",
      "[21:43:04]  Trial 1 TotRew = 0 (0.155s)\n",
      "[21:43:18]  Trial 101 TotRew = 0 (13.380s)\n",
      "[21:43:31]  Trial 201 TotRew = 8 (13.433s)\n",
      "[21:43:45]  Trial 301 TotRew = 54 (13.635s)\n",
      "[21:43:59]  Trial 401 TotRew = 9 (14.079s)\n",
      "[21:44:12]  Trial 500 TotRew = 88 (12.828s)\n",
      "-----\n",
      " 5\n",
      "[21:44:12]  Trial 1 TotRew = 0 (0.145s)\n",
      "[21:44:25]  Trial 101 TotRew = 0 (13.446s)\n",
      "[21:44:38]  Trial 201 TotRew = 14 (13.050s)\n",
      "[21:44:51]  Trial 301 TotRew = 0 (13.111s)\n",
      "[21:45:04]  Trial 401 TotRew = 0 (13.029s)\n",
      "[21:45:17]  Trial 500 TotRew = 120 (12.742s)\n",
      "-----\n",
      " 6\n",
      "[21:45:17]  Trial 1 TotRew = 0 (0.166s)\n",
      "[21:45:31]  Trial 101 TotRew = 0 (13.287s)\n",
      "[21:45:44]  Trial 201 TotRew = 4 (13.146s)\n",
      "[21:45:57]  Trial 301 TotRew = 131 (13.291s)\n",
      "[21:46:10]  Trial 401 TotRew = 100 (13.138s)\n",
      "[21:46:23]  Trial 500 TotRew = 139 (12.939s)\n",
      "-----\n",
      " 7\n",
      "[21:46:23]  Trial 1 TotRew = 0 (0.142s)\n",
      "[21:46:36]  Trial 101 TotRew = 0 (13.247s)\n",
      "[21:46:50]  Trial 201 TotRew = 0 (13.079s)\n",
      "[21:47:02]  Trial 301 TotRew = 62 (12.898s)\n",
      "[21:47:15]  Trial 401 TotRew = 129 (12.948s)\n",
      "[21:47:28]  Trial 500 TotRew = 141 (12.995s)\n",
      "-----\n",
      " 8\n",
      "[21:47:29]  Trial 1 TotRew = 0 (0.145s)\n",
      "[21:47:42]  Trial 101 TotRew = 2 (13.077s)\n",
      "[21:47:55]  Trial 201 TotRew = 0 (13.206s)\n",
      "[21:48:08]  Trial 301 TotRew = 0 (13.036s)\n",
      "[21:48:21]  Trial 401 TotRew = 31 (13.019s)\n",
      "[21:48:34]  Trial 500 TotRew = 116 (12.794s)\n",
      "-----\n",
      " 9\n",
      "[21:48:34]  Trial 1 TotRew = 0 (0.148s)\n",
      "[21:48:47]  Trial 101 TotRew = 2 (13.345s)\n",
      "[21:49:00]  Trial 201 TotRew = 0 (13.145s)\n",
      "[21:49:13]  Trial 301 TotRew = 48 (13.043s)\n",
      "[21:49:26]  Trial 401 TotRew = 116 (13.058s)\n",
      "[21:49:39]  Trial 500 TotRew = 141 (12.789s)\n",
      "-----\n",
      " 10\n",
      "[21:49:39]  Trial 1 TotRew = 0 (0.141s)\n",
      "[21:49:53]  Trial 101 TotRew = 5 (13.255s)\n",
      "[21:50:06]  Trial 201 TotRew = 0 (13.154s)\n",
      "[21:50:19]  Trial 301 TotRew = 0 (12.841s)\n",
      "[21:50:32]  Trial 401 TotRew = 0 (12.923s)\n",
      "[21:50:44]  Trial 500 TotRew = 66 (12.767s)\n",
      "-----\n",
      " 11\n",
      "[21:50:44]  Trial 1 TotRew = 0 (0.182s)\n",
      "[21:50:58]  Trial 101 TotRew = 0 (13.220s)\n",
      "[21:51:11]  Trial 201 TotRew = 0 (13.133s)\n",
      "[21:51:24]  Trial 301 TotRew = 0 (13.065s)\n",
      "[21:51:37]  Trial 401 TotRew = 0 (13.021s)\n",
      "[21:51:50]  Trial 500 TotRew = 0 (13.062s)\n",
      "-----\n",
      " 12\n",
      "[21:51:50]  Trial 1 TotRew = 0 (0.154s)\n",
      "[21:52:03]  Trial 101 TotRew = 0 (13.096s)\n",
      "[21:52:16]  Trial 201 TotRew = 1 (12.954s)\n",
      "[21:52:29]  Trial 301 TotRew = 99 (13.125s)\n",
      "[21:52:43]  Trial 401 TotRew = 135 (13.251s)\n",
      "[21:52:56]  Trial 500 TotRew = 128 (13.014s)\n",
      "-----\n",
      " 13\n",
      "[21:52:56]  Trial 1 TotRew = 0 (0.156s)\n",
      "[21:53:09]  Trial 101 TotRew = 0 (13.065s)\n",
      "[21:53:22]  Trial 201 TotRew = 113 (13.175s)\n",
      "[21:53:35]  Trial 301 TotRew = 106 (13.040s)\n",
      "[21:53:48]  Trial 401 TotRew = 105 (13.280s)\n",
      "[21:54:01]  Trial 500 TotRew = 134 (12.957s)\n",
      "-----\n",
      " 14\n",
      "[21:54:01]  Trial 1 TotRew = 1 (0.143s)\n",
      "[21:54:14]  Trial 101 TotRew = 2 (13.045s)\n",
      "[21:54:27]  Trial 201 TotRew = 62 (12.984s)\n",
      "[21:54:40]  Trial 301 TotRew = 118 (13.019s)\n",
      "[21:54:53]  Trial 401 TotRew = 141 (12.946s)\n",
      "[21:55:06]  Trial 500 TotRew = 136 (12.858s)\n",
      "-----\n",
      " 15\n",
      "[21:55:06]  Trial 1 TotRew = 0 (0.135s)\n",
      "[21:55:20]  Trial 101 TotRew = 0 (13.133s)\n",
      "[21:55:33]  Trial 201 TotRew = 4 (13.156s)\n",
      "[21:55:46]  Trial 301 TotRew = 15 (12.943s)\n",
      "[21:55:59]  Trial 401 TotRew = 76 (13.146s)\n",
      "[21:56:12]  Trial 500 TotRew = 123 (12.767s)\n",
      "-----\n",
      " 16\n",
      "[21:56:12]  Trial 1 TotRew = 0 (0.146s)\n",
      "[21:56:25]  Trial 101 TotRew = 2 (13.154s)\n",
      "[21:56:38]  Trial 201 TotRew = 0 (12.867s)\n",
      "[21:56:51]  Trial 301 TotRew = 52 (13.089s)\n",
      "[21:57:04]  Trial 401 TotRew = 89 (12.885s)\n",
      "[21:57:17]  Trial 500 TotRew = 139 (12.887s)\n",
      "-----\n",
      " 17\n",
      "[21:57:17]  Trial 1 TotRew = 0 (0.144s)\n",
      "[21:57:30]  Trial 101 TotRew = 0 (13.153s)\n",
      "[21:57:43]  Trial 201 TotRew = 0 (13.070s)\n",
      "[21:57:56]  Trial 301 TotRew = 0 (13.106s)\n",
      "[21:58:09]  Trial 401 TotRew = 0 (12.887s)\n",
      "[21:58:22]  Trial 500 TotRew = 0 (12.850s)\n",
      "-----\n",
      " 18\n",
      "[21:58:22]  Trial 1 TotRew = 0 (0.143s)\n",
      "[21:58:37]  Trial 101 TotRew = 0 (14.984s)\n",
      "[21:58:50]  Trial 201 TotRew = 23 (13.337s)\n",
      "[21:59:03]  Trial 301 TotRew = 119 (13.033s)\n",
      "[21:59:16]  Trial 401 TotRew = 136 (12.894s)\n",
      "[21:59:29]  Trial 500 TotRew = 108 (12.853s)\n",
      "-----\n",
      " 19\n",
      "[21:59:29]  Trial 1 TotRew = 0 (0.137s)\n",
      "[21:59:42]  Trial 101 TotRew = 5 (13.203s)\n",
      "[21:59:56]  Trial 201 TotRew = 0 (13.323s)\n",
      "[22:00:09]  Trial 301 TotRew = 0 (12.972s)\n",
      "[22:00:22]  Trial 401 TotRew = 0 (13.022s)\n",
      "[22:00:35]  Trial 500 TotRew = 0 (12.882s)\n",
      "-----\n",
      " 20\n",
      "[22:00:35]  Trial 1 TotRew = 0 (0.140s)\n",
      "[22:00:48]  Trial 101 TotRew = 6 (13.131s)\n",
      "[22:01:01]  Trial 201 TotRew = 0 (13.234s)\n",
      "[22:01:14]  Trial 301 TotRew = 0 (13.008s)\n",
      "[22:01:27]  Trial 401 TotRew = 47 (13.004s)\n",
      "[22:01:40]  Trial 500 TotRew = 88 (12.920s)\n",
      "-----\n",
      " 21\n",
      "[22:01:40]  Trial 1 TotRew = 0 (0.163s)\n",
      "[22:01:53]  Trial 101 TotRew = 3 (13.212s)\n",
      "[22:02:06]  Trial 201 TotRew = 0 (12.943s)\n",
      "[22:02:21]  Trial 301 TotRew = 12 (14.767s)\n",
      "[22:02:34]  Trial 401 TotRew = 98 (13.034s)\n",
      "[22:02:47]  Trial 500 TotRew = 36 (12.985s)\n",
      "-----\n",
      " 22\n",
      "[22:02:47]  Trial 1 TotRew = 0 (0.144s)\n",
      "[22:03:00]  Trial 101 TotRew = 1 (13.117s)\n",
      "[22:03:13]  Trial 201 TotRew = 0 (13.042s)\n",
      "[22:03:27]  Trial 301 TotRew = 9 (13.290s)\n",
      "[22:03:40]  Trial 401 TotRew = 119 (13.028s)\n",
      "[22:03:53]  Trial 500 TotRew = 87 (13.130s)\n",
      "-----\n",
      " 23\n",
      "[22:03:53]  Trial 1 TotRew = 0 (0.145s)\n",
      "[22:04:06]  Trial 101 TotRew = 0 (13.143s)\n",
      "[22:04:19]  Trial 201 TotRew = 20 (12.985s)\n",
      "[22:04:32]  Trial 301 TotRew = 40 (13.024s)\n",
      "[22:04:45]  Trial 401 TotRew = 140 (13.097s)\n",
      "[22:04:58]  Trial 500 TotRew = 131 (12.892s)\n",
      "-----\n",
      " 24\n",
      "[22:04:58]  Trial 1 TotRew = 0 (0.159s)\n",
      "[22:05:12]  Trial 101 TotRew = 3 (13.242s)\n",
      "[22:05:25]  Trial 201 TotRew = 0 (12.994s)\n",
      "[22:05:38]  Trial 301 TotRew = 0 (13.037s)\n",
      "[22:05:51]  Trial 401 TotRew = 84 (13.151s)\n",
      "[22:06:04]  Trial 500 TotRew = 117 (12.834s)\n",
      "-----\n",
      " 25\n",
      "[22:06:04]  Trial 1 TotRew = 0 (0.149s)\n",
      "[22:06:17]  Trial 101 TotRew = 0 (13.179s)\n",
      "[22:06:30]  Trial 201 TotRew = 0 (13.056s)\n",
      "[22:06:43]  Trial 301 TotRew = 140 (12.904s)\n",
      "[22:06:56]  Trial 401 TotRew = 131 (12.905s)\n",
      "[22:07:09]  Trial 500 TotRew = 136 (12.950s)\n",
      "-----\n",
      " 26\n",
      "[22:07:09]  Trial 1 TotRew = 0 (0.145s)\n",
      "[22:07:22]  Trial 101 TotRew = 4 (13.131s)\n",
      "[22:07:35]  Trial 201 TotRew = 55 (13.141s)\n",
      "[22:07:48]  Trial 301 TotRew = 120 (13.088s)\n",
      "[22:08:01]  Trial 401 TotRew = 101 (13.077s)\n",
      "[22:08:14]  Trial 500 TotRew = 132 (12.950s)\n",
      "-----\n",
      " 27\n",
      "[22:08:15]  Trial 1 TotRew = 0 (0.149s)\n",
      "[22:08:28]  Trial 101 TotRew = 0 (13.095s)\n",
      "[22:08:41]  Trial 201 TotRew = 0 (13.186s)\n",
      "[22:08:54]  Trial 301 TotRew = 96 (13.343s)\n",
      "[22:09:07]  Trial 401 TotRew = 73 (12.888s)\n",
      "[22:09:20]  Trial 500 TotRew = 131 (12.870s)\n",
      "-----\n",
      " 28\n",
      "[22:09:20]  Trial 1 TotRew = 0 (0.159s)\n",
      "[22:09:33]  Trial 101 TotRew = 2 (13.075s)\n",
      "[22:09:46]  Trial 201 TotRew = 0 (13.054s)\n",
      "[22:09:59]  Trial 301 TotRew = 13 (13.165s)\n",
      "[22:10:12]  Trial 401 TotRew = 79 (12.857s)\n",
      "[22:10:25]  Trial 500 TotRew = 0 (12.978s)\n",
      "-----\n",
      " 29\n",
      "[22:10:25]  Trial 1 TotRew = 0 (0.147s)\n",
      "[22:10:38]  Trial 101 TotRew = 0 (13.117s)\n",
      "[22:10:52]  Trial 201 TotRew = 1 (13.047s)\n",
      "[22:11:05]  Trial 301 TotRew = 32 (13.125s)\n",
      "[22:11:18]  Trial 401 TotRew = 101 (13.001s)\n",
      "[22:11:31]  Trial 500 TotRew = 90 (12.926s)\n",
      "-----\n",
      " 30\n",
      "[22:11:31]  Trial 1 TotRew = 1 (0.161s)\n",
      "[22:11:44]  Trial 101 TotRew = 5 (13.043s)\n",
      "[22:11:57]  Trial 201 TotRew = 70 (13.331s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:12:10]  Trial 301 TotRew = 13 (13.022s)\n",
      "[22:12:23]  Trial 401 TotRew = 119 (12.989s)\n",
      "[22:12:36]  Trial 500 TotRew = 140 (12.917s)\n",
      "-----\n",
      " 31\n",
      "[22:12:36]  Trial 1 TotRew = 0 (0.153s)\n",
      "[22:12:49]  Trial 101 TotRew = 0 (13.045s)\n",
      "[22:13:02]  Trial 201 TotRew = 14 (12.956s)\n",
      "[22:13:15]  Trial 301 TotRew = 49 (12.945s)\n",
      "[22:13:28]  Trial 401 TotRew = 126 (13.164s)\n",
      "[22:13:41]  Trial 500 TotRew = 139 (12.906s)\n",
      "-----\n",
      " 32\n",
      "[22:13:41]  Trial 1 TotRew = 0 (0.153s)\n",
      "[22:13:55]  Trial 101 TotRew = 0 (13.394s)\n",
      "[22:14:08]  Trial 201 TotRew = 0 (13.120s)\n",
      "[22:14:21]  Trial 301 TotRew = 105 (12.998s)\n",
      "[22:14:34]  Trial 401 TotRew = 103 (13.017s)\n",
      "[22:14:47]  Trial 500 TotRew = 141 (12.871s)\n",
      "-----\n",
      " 33\n",
      "[22:14:47]  Trial 1 TotRew = 0 (0.167s)\n",
      "[22:15:00]  Trial 101 TotRew = 0 (13.200s)\n",
      "[22:15:13]  Trial 201 TotRew = 5 (13.143s)\n",
      "[22:15:26]  Trial 301 TotRew = 60 (12.848s)\n",
      "[22:15:39]  Trial 401 TotRew = 86 (13.109s)\n",
      "[22:15:52]  Trial 500 TotRew = 137 (13.167s)\n",
      "-----\n",
      " 34\n",
      "[22:15:53]  Trial 1 TotRew = 2 (0.158s)\n",
      "[22:16:06]  Trial 101 TotRew = 1 (13.185s)\n",
      "[22:16:19]  Trial 201 TotRew = 0 (13.141s)\n",
      "[22:16:32]  Trial 301 TotRew = 0 (12.921s)\n",
      "[22:16:45]  Trial 401 TotRew = 142 (12.981s)\n",
      "[22:16:58]  Trial 500 TotRew = 0 (12.879s)\n",
      "-----\n",
      " 35\n",
      "[22:16:58]  Trial 1 TotRew = 0 (0.149s)\n",
      "[22:17:11]  Trial 101 TotRew = 0 (13.159s)\n",
      "[22:17:24]  Trial 201 TotRew = 0 (13.060s)\n",
      "[22:17:37]  Trial 301 TotRew = 7 (12.923s)\n"
     ]
    }
   ],
   "source": [
    "#### run a bunch of trials to get rwds to average \n",
    "ec_sim = False\n",
    "for i in range(100):\n",
    "    print('-----\\n',i)\n",
    "    MF,opt = ac.make_agent(agent_params, freeze=False)\n",
    "    if ec_sim:\n",
    "        EC = ec.ep_mem(MF,agent_params['cachelim']) \n",
    "        run_trials(run_dict, True)\n",
    "    else:\n",
    "        run_trials(run_dict, False)\n",
    "    with open('1000pretrain_mf.csv', 'a+') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerows([run_dict['total_reward']])\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(run_dict['total_reward'], label='r')\n",
    "#plt.plot(run_dict['total_loss'][0], label='p')\n",
    "#plt.plot(run_dict['total_loss'][1], label='v')\n",
    "plt.legend(bbox_to_anchor=(1.1,1.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early_learn_mf_only = run_dict['total_reward']\n",
    "#early_learn_w_ec = run_dict['total_reward']\n",
    "#late_learn_mf_only = run_dict['total_reward']\n",
    "#late_learn_w_ec = run_dict['total_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(run_dict['total_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, sharex = True, sharey=True)\n",
    "ax[0].plot(early_learning_w_ec, 'b', alpha =0.7, label = 'With EC')\n",
    "ax[0].plot(early_learn_mf_only, 'k--', alpha = 0.7, label=\"MF only\")\n",
    "\n",
    "ax[1].plot(late_learn_w_ec, 'r', label='With EC')\n",
    "ax[1].plot(late_learn_mf_only, 'k--', alpha = 0.7, label ='MF only')\n",
    "\n",
    "ax[0].legend(bbox_to_anchor = (1.0,.75))\n",
    "ax[1].legend(bbox_to_anchor = (1.0,.75))\n",
    "fig.savefig('compare.svg', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rpe) \n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap = cmap)\n",
    "\n",
    "ax1 = fig.add_axes([0.04, 0, 0.85, 0.85])\n",
    "axc = fig.add_axes([0.8, 0, 0.05, 0.85])\n",
    "\n",
    "cb1 = colorbar.ColorbarBase(axc, cmap = cmap, norm = cNorm)\n",
    "ax1.imshow(maze.grid, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax1.add_patch(patches.Circle(maze.rwd_loc[0], 0.35, fc='w'))\n",
    "\n",
    "for entry in compare_policies.keys():\n",
    "    x = entry[0]\n",
    "    y = entry[1]\n",
    "    \n",
    "    ec_policy = compare_policies[entry][1][0] - compare_policies[entry][0][0]\n",
    "    action = np.argmax(ec_policy)\n",
    "    prob = max(ec_policy)\n",
    "    \n",
    "    dx1, dy1, head_w, head_l = gp.make_arrows(action,prob)\n",
    "    if prob > 1/4:\n",
    "        if (dx1, dy1) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal1 = scalarMap.to_rgba(prob)\n",
    "            ax1.arrow(x,y, dx1, dy1, head_width=0.3, head_length=0.2, color = colorVal1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "ax1.invert_yaxis()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = compare_policies[(7,5)][1]\n",
    "for i in range(len(x)):\n",
    "    plt.bar(np.arange(6)+(0.1*i), x[i], width = 0.1, alpha = 0.3)\n",
    "    plt.bar(np.arange(6), np.ones(6)*0.16666667, width =0.1, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(DND.cache_list))\n",
    "def cosine_sim(mem_dict, key, **kwargs):\n",
    "    similarity_threshold = kwargs.get('threshold', 0.9)\n",
    "\n",
    "    mem_cache = np.asarray(list(mem_dict.keys()))\n",
    "    print(mem_cache.shape)\n",
    "    entry = np.asarray(key)\n",
    "\n",
    "    mqt = np.dot(mem_cache, entry)\n",
    "    norm = np.linalg.norm(mem_cache, axis=1) * np.linalg.norm(entry)\n",
    "\n",
    "    cosine_similarity = mqt / norm\n",
    "\n",
    "    index = np.argmax(cosine_similarity)\n",
    "    similar_activity = mem_cache[index]\n",
    "    if max(cosine_similarity) >= similarity_threshold:\n",
    "        return similar_activity, index, max(cosine_similarity)\n",
    "\n",
    "    else:\n",
    "        # print('max memory similarity:', max(cosine_similarity))\n",
    "        return [], [], max(cosine_similarity)\n",
    "\n",
    "def make_pvals(p, **kwargs):\n",
    "    envelope = kwargs.get('envelope', 50)\n",
    "    return np.round(1 / np.cosh(p / envelope),8)\n",
    "\n",
    "def softmax(x, T=1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp((x - np.max(x))/T)\n",
    "    return e_x / e_x.sum(axis=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_key, ind, max_cs = cosine_sim(DND.cache_list, sts[0])\n",
    "print('sim', max_cs)\n",
    "memory = np.nan_to_num(DND.cache_list[tuple(state_key)][0])\n",
    "deltas = memory[:,0]\n",
    "print(deltas)\n",
    "rec_times = memory[:,1]\n",
    "print(\"rtimes\",rec_times)\n",
    "times        = abs(timestep - memory[:,1])\n",
    "print('tiems', times)\n",
    "pv =make_pvals(times)\n",
    "print(pv)\n",
    "\n",
    "mult = np.multiply(deltas, pv)\n",
    "print(softmax(max_cs*mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in sts[0:10]:\n",
    "    timestep = 1000\n",
    "    a = DND.recall_mem(key=i,timestep=timestep,env=100)\n",
    "    print(a,\"----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs =['r','g','b','c', 'y','pink']\n",
    "'''\n",
    "r = list(self.cache_list.keys())\n",
    "g = [t for e, t in self.cache_list.values()]\n",
    "b = lp = persistence_.index(min(persistence_))\n",
    "c = old_activity = cache_keys[lp]\n",
    "y = del self.cache_list[old_activity]\n",
    "'''\n",
    "plt.figure()\n",
    "for i in range(len(EC.stupid_df)):\n",
    "    xs = np.arange(len(EC.stupid_df[i]))\n",
    "    ys = EC.stupid_df[i]\n",
    "    \n",
    "    plt.scatter(xs, ys, c=cs[i], alpha=0.3)\n",
    "plt.ylim([-0.00002,0.00002])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(run_dict['total_reward'])\n",
    "plt.ylim([0,run_dict['NUM_EVENTS']])\n",
    " \n",
    "plt.figure(2)\n",
    "plt.plot(run_dict['total_loss'][0], label = 'pol')\n",
    "plt.plot(run_dict['total_loss'][1], label = 'val')\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "plt.close()\n",
    "#gp.print_value_maps(maze, run_dict['val_maps'], maps=0, val_range=(-1,50), save_dir=fig_savedir, title='Value Map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.torch.save(MF,agent_params['load_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
