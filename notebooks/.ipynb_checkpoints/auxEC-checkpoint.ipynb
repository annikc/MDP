{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from importlib import reload\n",
    "from modules import * \n",
    "fig_savedir = '../data/figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_params = {\n",
    "    'y_height':   20,\n",
    "    'x_width':    20,\n",
    "    'walls':      False,\n",
    "    'rho':        0,\n",
    "    'maze_type':  'none',\n",
    "    'port_shift': 'none'\n",
    "}\n",
    "\n",
    "agent_params = {\n",
    "    'load_model':   False,\n",
    "    'load_dir':     '' ,#'../data/outputs/gridworld/MF{}{}training.pt'.format(maze.x, maze.y)\n",
    "    'action_dims':  6, #=len(maze.actionlist)\n",
    "    'batch_size':   1,\n",
    "    'gamma':        0.98, #discount factor\n",
    "    'eta':          5e-4,\n",
    "    'temperature':  1,\n",
    "    'use_EC':       False,\n",
    "    'cachelim':     300, #int(0.75*np.prod(maze.grid.shape)) # memory limit should be ~75% of #actions x #states\n",
    "    'state_type':   'conv'\n",
    "}\n",
    "\n",
    "run_dict = {\n",
    "    'NUM_EVENTS':   100,\n",
    "    'NUM_TRIALS':   1,\n",
    "    'print_freq':   1/10,\n",
    "    'total_loss':   [[],[]],\n",
    "    'total_reward': [],\n",
    "    'val_maps':     [],\n",
    "    'policies':     [{},{}],\n",
    "    'deltas':       [],\n",
    "    'spots':        [],\n",
    "    'vls':          []\n",
    "}\n",
    "\n",
    "#make environment\n",
    "maze = eu.gridworld(grid_params)\n",
    "maze.set_rwd([(5,5)]) # [(int(y_height/2),int(x_width/2))]\n",
    "\n",
    "#if grid_params['maze_type'] is not 'triple_reward':\n",
    "#    for i in maze.rwd_loc: \n",
    "#        maze.orig_rwd_loc.append(i)\n",
    "\n",
    "agent_params = sg.gen_input(maze, agent_params)\n",
    "\n",
    "env = eu.gymworld(maze)\n",
    "\n",
    "if agent_params['load_model']: \n",
    "    MF = ac.torch.load(agent_params['load_dir']) # load previously saved model\n",
    "else:\n",
    "    MF = ac.AC_Net(agent_params)\n",
    "opt = ac.optim.Adam(MF.parameters(), lr = agent_params['eta'])\n",
    "\n",
    "EC = ec.ep_mem(MF,agent_params['cachelim']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62\n",
      "[07:53:30]  Trial 1 TotRew = 0 (0.383s)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------\n",
    "# empty data frames for recording\n",
    "# --------------------------------\n",
    "\n",
    "# flags to be changed mid run\n",
    "add_episodic_cache = False  ## Possibly unnecessary now \n",
    "if add_episodic_cache:\n",
    "    rwd_threshold  = True\n",
    "midrun_rwd_removal = False\n",
    "success_benchmark  = (run_dict['NUM_EVENTS'] -((maze.y-1)+(maze.x-1)))/run_dict['NUM_EVENTS']\n",
    "print(success_benchmark)\n",
    "\n",
    "if midrun_rwd_removal: \n",
    "    reward_tally = {}\n",
    "    for _ in maze.rwd_loc: \n",
    "        reward_tally[_] = []\n",
    "    trial_rwd_switch = 0\n",
    "    \n",
    "use_EC = agent_params['use_EC']\n",
    "EC.reset_cache()\n",
    "EC.reward_unseen = True\n",
    "\n",
    "add_mem_dict     = {} #dictionary of items which get put into memory cache\n",
    "timestamp        = 0\n",
    "blocktime        = time.time()\n",
    "#==================================\n",
    "# Run Trial\n",
    "#==================================\n",
    "for trial in range(run_dict['NUM_TRIALS']):\n",
    "    trialstart_stamp = timestamp\n",
    "    \n",
    "    reward_sum   = 0\n",
    "    v_last       = 0\n",
    "    track_deltas = []\n",
    "    track_spots  = []\n",
    "    visited_locs = []\n",
    "    \n",
    "    if agent_params['state_type'] == 'pcs':\n",
    "        get_pcs = pcs.activity(env.reset())\n",
    "        state   = ac.Variable(ac.torch.FloatTensor(get_pcs))\n",
    "    elif agent_params['state_type'] == 'conv':\n",
    "        env.reset() \n",
    "        frame = np.expand_dims(sg.get_frame(maze), axis=0) # because we need to include batch size of 1\n",
    "        state = ac.Variable(ac.torch.FloatTensor(frame))\n",
    "        \n",
    "    MF.reinit_hid() #reinit recurrent hidden layers\n",
    "    for event in range(run_dict['NUM_EVENTS']):\n",
    "        # pass state through EC module\n",
    "        if use_EC:\n",
    "            policy_, value_, lin_act_ = MF(state,agent_params['temperature'])\n",
    "            add_mem_dict['state'] = maze.cur_state\n",
    "            visited_locs.append(maze.cur_state)\n",
    "        else: \n",
    "            policy_, value_ = MF(state, agent_params['temperature'])[0:2]\n",
    "        \n",
    "        choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "        if event < run_dict['NUM_EVENTS']: \n",
    "            next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "        MF.rewards.append(reward)\n",
    "        delta = reward + agent_params['gamma']*value - v_last  #compute eligibility trace/rpe approximation\n",
    "\n",
    "        \n",
    "        if use_EC:\n",
    "            add_mem_dict['activity']  = tuple(lin_act_.view(-1).data)\n",
    "            add_mem_dict['action']    = choice\n",
    "            add_mem_dict['delta']     = delta\n",
    "            add_mem_dict['timestamp'] = timestamp            \n",
    "            EC.add_mem(add_mem_dict, keep_hist = True)             #add event to memory cache\n",
    "            if reward != 0:\n",
    "                EC.reward_update(trialstart_stamp, timestamp, reward)\n",
    "            #EC.reward_update(trialstart_stamp, timestamp, delta[0])\n",
    "            track_deltas.append(delta[0])\n",
    "            track_spots.append(maze.cur_state)\n",
    "        \n",
    "        if agent_params['state_type'] == 'pcs':\n",
    "            state = ac.Variable(ac.torch.FloatTensor(pcs.activity(next_state)))       # update state\n",
    "        elif agent_params['state_type'] == 'conv':\n",
    "            # because we need to include batch size of 1 \n",
    "            frame = np.expand_dims(sg.get_frame(maze), axis = 0)\n",
    "            state = ac.Variable(ac.torch.FloatTensor(frame))\n",
    "        reward_sum += reward\n",
    "    \n",
    "        v_last = value\n",
    "        timestamp += 1\n",
    "    \n",
    "    \n",
    "    if add_episodic_cache:\n",
    "        if (np.array(total_reward[-50:]).mean() > success_benchmark*NUM_EVENTS):\n",
    "            if rwd_threshold:\n",
    "                print(\" \\t Started Memory at Trial \", trial)\n",
    "                if midrun_rwd_removal:\n",
    "                    maxsums = {}\n",
    "                    for item in reward_tally.items():\n",
    "                        maxsums[item[0]] = sum(item[1])\n",
    "                    most_rewarded_location = max(maxsums.iteritems(), key=operator.itemgetter(1))[0] \n",
    "                    maze.rwd_loc.remove(most_rewarded_location)\n",
    "                    trial_rwd_switch = trial\n",
    "                    print(\"removed reward at \", most_rewarded_location)\n",
    "\n",
    "                rwd_threshold = False\n",
    "                use_EC = True\n",
    "    \n",
    "    if midrun_rwd_removal:\n",
    "        if (trial_rwd_switch!=0) and (trial == trial_rwd_switch + 1000):\n",
    "            maze.rwd_loc.append(most_rewarded_location)\n",
    "\n",
    "    p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "    \n",
    "    run_dict['total_loss'][0].append(p_loss.data[0])\n",
    "    run_dict['total_loss'][1].append(v_loss.data[0])\n",
    "    run_dict['total_reward'].append(reward_sum)\n",
    "    \n",
    "    if agent_params['state_type'] == 'pcs':\n",
    "        value_map = ac.generate_values(maze,MF,pcs=pcs)\n",
    "    else:\n",
    "        value_map = ac.generate_values(maze,MF)\n",
    "    run_dict['val_maps'].append(value_map.copy())\n",
    "    \n",
    "    if midrun_rwd_removal:\n",
    "        for item in maze.reward_tally.items():\n",
    "            reward_tally[item[0]].append(item[1])\n",
    "            \n",
    "    run_dict['deltas'].append(track_deltas)\n",
    "    run_dict['spots'].append(track_spots)\n",
    "    run_dict['vls'].append(visited_locs)\n",
    "    if trial ==0 or trial%100==0 or trial == NUM_TRIALS-1:\n",
    "        EC_policies, MF_policies = ac.generate_values(maze, MF,EC=EC)\n",
    "        run_dict['policies'][0]['{}'.format(trial)] = EC_policies\n",
    "        run_dict['policies'][1]['{}'.format(trial)] = MF_policies\n",
    "        #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime))\n",
    "        print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime))\n",
    "        blocktime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.plot(total_loss[0], label='p')\n",
    "plt.plot(total_loss[1], label='v')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(total_reward, label='r', color='r', marker='')\n",
    "plt.axhline(NUM_EVENTS-(grid_params['y_height']-maze.rwd_loc[0][0] + grid_params['x_width']-maze.rwd_loc[0][1]), color = 'gray', linestyle='--')\n",
    "#plt.figure(2)\n",
    "#delta_of_interest = deltas[-1]\n",
    "#plt.plot(deltas[-1])\n",
    "#plt.annotate('{}'.format(spots[-1][np.argmax(delta_of_interest)]), xy=(np.argmax(delta_of_interest), max(delta_of_interest)) )\n",
    "#print(maze.rwd_loc[0])\n",
    "\n",
    "#if maze_type == 'triple_reward':\n",
    "#    plt.figure(2)\n",
    "#    plt.plot(reward_tally[reward_tally.keys()[0]], label='{}'.format(reward_tally.keys()[0]))\n",
    "#    plt.plot(reward_tally[reward_tally.keys()[1]], label='{}'.format(reward_tally.keys()[1]))\n",
    "#    plt.plot(reward_tally[reward_tally.keys()[2]], label='{}'.format(reward_tally.keys()[2]))\n",
    "#    plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(eu)\n",
    "eu.print_value_maps(maze,\n",
    "                    val_maps,\n",
    "                    maps=0,#list(np.arange(850, 1050)), \n",
    "                    val_range=(-1,50),\n",
    "                    save_dir=fig_savedir,\n",
    "                    title='Value Map') ### see individual map with kwarg maps=X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(eu)\n",
    "EC_policy = np.zeros((maze.y, maze.x), dtype=[('N', 'f8'), ('E', 'f8'),('W', 'f8'), ('S', 'f8'),('stay', 'f8'), ('poke', 'f8')])\n",
    "ex = EC_policy[0][0]\n",
    "for i in EC.cache_list.keys():\n",
    "    loc = EC.cache_list[i][2]\n",
    "    pol = EC.cache_list[i][0]\n",
    "    t = EC.cache_list[i][1]\n",
    "    if max(tuple(eu.softmax(pol))) > max(EC_policy[loc[1]][loc[0]]):\n",
    "        EC_policy[loc[1]][loc[0]] = tuple(eu.softmax(pol))\n",
    "    #    previous_pol = np.asarray(EC_policy[loc[1]][loc[0]])\n",
    "    #    print( previous_pol,np.asarray(eu.softmax(pol) ))\n",
    "    #    new_pol = tuple(eu.softmax(np.add(np.asarray(eu.softmax(pol)),previous_pol)))\n",
    "    #    EC_policy[loc[1]][loc[0]] = tuple(eu.softmax(pol))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(eu)\n",
    "test1 = [int(po) for po in policies[1].keys()]\n",
    "\n",
    "epoch = max(test1)\n",
    "print(epoch)\n",
    "eu.make_dual_policy_plots(maze, EC_policy, policies[1][str(epoch)], savedir='PolMaps.svg')#EC_policies, MF_policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "cmap      = plt.cm.Spectral_r\n",
    "cNorm     = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cmx.ScalarMappable(norm = cNorm, cmap=cmap)\n",
    "\n",
    "\n",
    "ax1 = fig.add_axes([0.04, 0, 0.85, 0.85]) # [left, bottom, width, height]\n",
    "axc = fig.add_axes([0.89, 0.125, 0.05, 0.6])\n",
    "cb1 = colorbar.ColorbarBase(axc, cmap=cmap, norm=cNorm)\n",
    "\n",
    "ax1.imshow(maze.grid, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax1.add_patch(patches.Circle((10,10), 0.35, fc='k', ec='w'))\n",
    "#ax1.add_patch(patches.Circle(maze.start_loc, 0.5, fc='b', ec='w'))\n",
    "ax1.add_patch(patches.Circle(maze.rwd_loc[0], 0.35, fc='w'))\n",
    "\n",
    "codes = [Path.MOVETO]\n",
    "for i in range(len(vls[0])-1):\n",
    "    codes.append(Path.LINETO)\n",
    "\n",
    "for i in range(len(vls)):\n",
    "    path = Path(vls[i], codes)\n",
    "    patch = patches.PathPatch(path, edgecolor='gray', facecolor='none', linestyle=':', lw=2)\n",
    "    ax1.add_patch(patch)\n",
    "\n",
    "chance_threshold = np.round(1/len(maze.actionlist),6)\n",
    "use_recency      = True\n",
    "\n",
    "for i in EC.cache_list.keys():\n",
    "    loc = EC.cache_list[i][2]\n",
    "    pol = eu.softmax(EC.cache_list[i][0])\n",
    "    t   = EC.cache_list[i][1]\n",
    "    \n",
    "    if use_recency: \n",
    "        if t<101:\n",
    "            recency = 1\n",
    "        else:\n",
    "            recency = 0\n",
    "        #recency = abs(1-t/(trial*NUM_EVENTS))\n",
    "        #if recency > 1: \n",
    "        #    recency = 1\n",
    "    else:\n",
    "        recency = 1 \n",
    "    \n",
    "    action = np.argmax(pol)\n",
    "    prob   = max(pol)\n",
    "\n",
    "\n",
    "    dx1,dy1,head_w,head_l = eu.make_arrows(action, prob)\n",
    "    \n",
    "    if prob > chance_threshold:\n",
    "        if (dx1, dy1) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal1 = scalarMap.to_rgba(prob)\n",
    "            ax1.arrow(loc[0], loc[1], \n",
    "                      dx1, dy1, \n",
    "                      head_width  = 0.3, \n",
    "                      head_length = 0.2, \n",
    "                      color       = colorVal1, \n",
    "                      alpha       = recency)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "plt.savefig('ECpol_T_inf_newrwd.svg', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(visited_locs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dict = {}\n",
    "for prop, propval in vars(maze).iteritems():\n",
    "    grid_dict[prop]= propval\n",
    "store_data= {}\n",
    "store_data['total_reward'] = total_reward\n",
    "store_data['total_loss'] = total_loss\n",
    "store_data['val_maps'] = val_maps\n",
    "store_data['EC'] = EC\n",
    "store_data['MF'] = MF\n",
    "if use_EC:\n",
    "    store_data['policies'] = policies\n",
    "store_data['params'] = {'grid':grid_dict,\n",
    "                       'state_type':state_type,\n",
    "                       'discount_factor': discount_factor,\n",
    "                       'eta':eta, \n",
    "                       'runtime':[NUM_EVENTS, NUM_TRIALS]}\n",
    "store_data['maze'] = maze\n",
    "datestamp = time.strftime(\"%y%m%d_%H%M\", time.localtime())\n",
    "np.save('../data/outputs/gridworld/pydicts/{}.npy'.format(datestamp), store_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving model\n",
    "ac.torch.save(MF,'../data/outputs/gridworld/MF2020training.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "state_type = agent_params['state_type']\n",
    "if state_type == 'conv':\n",
    "    num_channels = 3\n",
    "    if maze.bound:\n",
    "        input_dims = (y_height+2, x_width+2, num_channels)\n",
    "    else:\n",
    "        input_dims = (y_height, x_width, num_channels)\n",
    "    hid_types = ['conv', 'pool', 'linear']\n",
    "    conv_dims = ac.conv_output(input_dims)\n",
    "    pool_dims = ac.conv_output(conv_dims)\n",
    "    hid_dims = [conv_dims, pool_dims, 500]\n",
    "\n",
    "elif state_type == 'pcs':\n",
    "    input_dims = 1000\n",
    "    hid_types = ['linear']\n",
    "    hid_dims = [500]\n",
    "    \n",
    "action_dims = len(maze.actionlist)\n",
    "batch_size = 1\n",
    "\n",
    "NUM_EVENTS = 100\n",
    "NUM_TRIALS = 5000\n",
    "\n",
    "discount_factor = 0.98\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
