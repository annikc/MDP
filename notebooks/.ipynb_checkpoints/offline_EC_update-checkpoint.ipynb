{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from importlib import reload\n",
    "from modules import * \n",
    "import csv\n",
    "fig_savedir = '../data/figures/'\n",
    "\n",
    "grid_params = {\n",
    "    'y_height':   20,\n",
    "    'x_width':    20,\n",
    "    'walls':      False,\n",
    "    'rho':        0,\n",
    "    'maze_type':  'none',\n",
    "    'port_shift': 'none'\n",
    "}\n",
    "\n",
    "\n",
    "agent_params = {\n",
    "    'load_model':   False,\n",
    "    'load_dir':     '../data/outputs/gridworld/MF{}{}training_rwd15_5.pt'.format(grid_params['x_width'],grid_params['y_height']),\n",
    "    'action_dims':  6, #=len(maze.actionlist)\n",
    "    'lin_dims':     500,\n",
    "    'batch_size':   1,\n",
    "    'gamma':        0.98, #discount factor\n",
    "    'eta':          5e-4,\n",
    "    'temperature':  1,\n",
    "    'use_EC':       False,\n",
    "    'cachelim':     100, # memory limit should be ~75% of #actions x #states\n",
    "    'state_type':   'conv'\n",
    "}\n",
    "\n",
    "run_dict = {\n",
    "    'NUM_EVENTS':   150,\n",
    "    'NUM_TRIALS':   100,\n",
    "    'print_freq':   1/10,\n",
    "    'total_loss':   [[],[]],\n",
    "    'total_reward': [],\n",
    "    'val_maps':     [],\n",
    "    'policies':     [{},{}],\n",
    "    'deltas':       [],\n",
    "    'spots':        [],\n",
    "    'vls':          []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 10)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAToAAAEmCAYAAADyVly8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAE3BJREFUeJzt3X/sZXV95/HnaxnczSItQ1lHBFq0JSRoWsQJqy66uNoRJ6xjG9uFdOuobKa2JdGkm5auWWrcbFZt1Gy3jWSqE9BQZLtKJe0gjK0puhF0hgwwCHYGg2Emw0xwLEjtakff+8c9X3P5cu/31znfH/OZ5yO5ued+zuecz+eee+/rez7n3HO/qSokqWX/bLU7IEnLzaCT1DyDTlLzDDpJzTPoJDXPoJPUvHmDLsl5Sb6Y5OtJHkry7q78zCS7kuzv7tdPWX5rV2d/kq1DPwFJmk/m+x5dkrOBs6vqviSnA3uAtwBvB45V1QeSXAesr6rfm7XsmcBuYCNQ3bKvqKrvDP5MJGmKeffoqupwVd3XTX8XeBg4B9gC3NRVu4lR+M32RmBXVR3rwm0XcMUQHZekhVrUMbok5wMvB+4FNlTV4W7WE8CGCYucAzw+9vhgVyZJK2bdQismeT7wGeA9VfV0kh/Pq6pK0utasiTbgG3dw1f0WZekZjxZVf+q70oWtEeX5FRGIXdzVX22Kz7SHb+bOY53dMKih4Dzxh6f25U9R1Vtr6qNVbVxoZ2X1LxvDbGShZx1DfAJ4OGq+sjYrNuBmbOoW4HPTVj8TmBTkvXdWdlNXZkkrZyqmvMGXMbojOkDwN7uthn4KeCvgf3AF4Azu/obgY+PLf9O4EB3e8d87XXLlDdv3rwBuxeSGfPd5v16yWroe7xPUjP2DHE4yysjJDXPoJPUPINOUvMMOknNM+gkNc+gk9Q8g05S8ww6Sc0z6CQ1z6CT1DyDTlLzDDpJzTPoJDXPoJPUPINOUvMMOknNM+gkNc+gk9Q8g05S8ww6Sc0z6CQ1z6CT1DyDTlLzDDpJzVs3X4UkO4ArgaNV9bKu7Fbgwq7KGcDfV9XFE5Z9DPgu8EPg+BD/iFaSFmveoANuBP4Y+ORMQVX9h5npJB8Gnppj+ddV1ZNL7aAk9TVv0FXV3UnOnzQvSYBfBf7dsN2SpOH0PUb3GuBIVe2fMr+Au5LsSbJtrhUl2ZZkd5LdPfskSc+ykKHrXK4Gbplj/mVVdSjJC4BdSR6pqrsnVayq7cB2gCTVs1+S9GNL3qNLsg74ZeDWaXWq6lB3fxS4Dbh0qe1J0lL1Gbq+AXikqg5OmpnktCSnz0wDm4B9PdqTpCWZN+iS3AJ8BbgwycEk13SzrmLWsDXJi5Ls7B5uAL6c5H7gq8BfVdXnh+u6JC1Mqtbe4TCP0Unq7Bni+7deGSGpeQadpOYZdJKaZ9BJap5BJ6l5Bp2k5hl0kppn0ElqnkEnqXkGnaTmGXSSmmfQSWqeQSepeQadpOYZdJKaZ9BJap5BJ6l5Bp2k5hl0kppn0ElqnkEnqXkGnaTmGXSSmreQf2C9I8nRJPvGyt6X5FCSvd1t85Rlr0jyjSQHklw3ZMclaaEWskd3I3DFhPKPVtXF3W3n7JlJTgH+BHgTcBFwdZKL+nRWkpZi3qCrqruBY0tY96XAgar6ZlX9APg0sGUJ65GkXvoco7s2yQPd0Hb9hPnnAI+PPT7YlU2UZFuS3Ul29+iTJD3HUoPuY8DPAhcDh4EP9+1IVW2vqo1VtbHvuiRp3JKCrqqOVNUPq+pHwJ8yGqbOdgg4b+zxuV2ZJK2oJQVdkrPHHv4SsG9Cta8BFyR5cZLnAVcBty+lPUnqY918FZLcAlwOnJXkIPAHwOVJLgYKeAz4ja7ui4CPV9Xmqjqe5FrgTuAUYEdVPbQsz0KS5pCqWu0+PEeStdcpSathzxDH7b0yQlLzDDpJzTPoJDXPoJPUPINOUvMMOknNM+gkNc+gk9Q8g05S8ww6Sc0z6CQ1z6CT1DyDTlLzDDpJzTPoJDXPoJPUPINOUvMMOknNM+gkNc+gk9Q8g05S8ww6Sc0z6CQ1b96gS7IjydEk+8bK/jDJI0keSHJbkjOmLPtYkgeT7E2ye8iOS9JCLWSP7kbgilllu4CXVdXPA38H/P4cy7+uqi4e4p/QStJSzBt0VXU3cGxW2V1Vdbx7eA9w7jL0TZIGMcQxuncCd0yZV8BdSfYk2TZAW5K0aOv6LJzkvcBx4OYpVS6rqkNJXgDsSvJIt4c4aV3bAMNQ0uCWvEeX5O3AlcCvVVVNqlNVh7r7o8BtwKXT1ldV26tqo8fyJA1tSUGX5Argd4E3V9X3ptQ5LcnpM9PAJmDfpLqStJwW8vWSW4CvABcmOZjkGuCPgdMZDUf3Jrmhq/uiJDu7RTcAX05yP/BV4K+q6vPL8iwkaQ6ZMupcVUnWXqckrYY9QxzO8soISc0z6CQ1z6CT1DyDTlLzDDpJzTPoJDXPoJPUPINOUvMMOknNM+gkNc+gk9Q8g05S8ww6Sc0z6CQ1z6CT1DyDTlLzDDqdEF4NfAr4GnAn8B+BU1e1RzqR+AvDWvPeB/zBhPIvM/rP6v+wor3RCvMXhtW+1zM55AAuA/7HCvZFJy6DTmvab80zfyvwL1eiIzqhGXRa035+nvk/AZy/Av3Qic2g05r21ALqPL3svdCJzqDTmvbpeeZ/CTi4Eh3RCc2g05q2HfjGlHnfB967gn3RiWtBQZdkR5KjSfaNlZ2ZZFeS/d39+inLbu3q7E+ydaiO6+TwNPBvgT8H/mmsfA+jr5Z8aTU6pRPOgr5Hl+S1wDPAJ6vqZV3Zh4BjVfWBJNcB66vq92YtdyawG9gIFKP35yuq6jvztOf36PQcG4CfA74NPLLKfdGKWbnv0VXV3cCxWcVbgJu66ZuAt0xY9I3Arqo61oXbLkZ/iKVFOwL8Xww5Ld66HstuqKrD3fQTjP7gznYO8PjY44Nd2XMk2QZs69EfSZqoT9D9WFVV3+FmVW1ndOzZoaukQfU563okydkA3f3RCXUOAeeNPT63K5OkFdMn6G5ndAUO3f3nJtS5E9iUZH13VnZTVyZJK2ahXy+5BfgKcGGSg0muAT4A/GKS/cAbusck2Zjk4wBVdQz4b4x+XedrwPu7MklaMf5Mk6S1zJ9pkqSFMOgkNc+gk9Q8g05S8ww6Sc0z6CQ1z6CT1DyDTlLzDDpJzTPoJDXPoJPUPINOUvMMOknNM+gkNc+gk9Q8g05S8ww6Sc0z6CQ1z6CT1DyDTlLzDDpJzTPoJDXPoJPUvCUHXZILk+wduz2d5D2z6lye5KmxOtf377IkLc66pS5YVd8ALgZIcgpwCLhtQtUvVdWVS21Hkvoaauj6euDRqvrWQOuTpMEMFXRXAbdMmfeqJPcnuSPJSwdqT5IWbMlD1xlJnge8Gfj9CbPvA36mqp5Jshn4C+CCKevZBmzr2x9Jmi1V1W8FyRbgt6tq0wLqPgZsrKon56nXr1OSWrGnqjb2XckQQ9ermTJsTfLCJOmmL+3a+/YAbUrSgvUauiY5DfhF4DfGyt4FUFU3AG8FfjPJceAfgauq7y6kJC1S76HrcnDoKqmzZoaukrSmGXSSmmfQSWqeQadV8DbgHuAZ4CDwIWDDqvZIbfNkhFbYDuAdE8ofB14DeBWhnsWTETrRXMnkkAM4D/joCvZFJxODTivoP80z/9/jEFbLwaDTCnrxPPPXAT+9Eh3RScag0wp6YqA60uIYdFpBN84z/wuMTkpIwzLotIL+N3DHlHl/D/zOCvZFJxODTivoh8AW4L8y+v4cwA+ATwOvBh5YpX6pdX6PTqskwHrgH4Dvr3JftIYN8j263r8wLC1NAcdWuxM6STh0ldQ8g05S8ww6Sc0z6CQ1z6CT1DyDTlLzDDpJzTPoJDXPoJPUPINOUvN6B12Sx5I8mGRvkt0T5ifJHyU5kOSBJJf0bVOSFmOoa11fV1VPTpn3JuCC7vavgY9195K0IlZi6LoF+GSN3AOckeTsFWhXkoBhgq6Au5LsSbJtwvxzePbPxh7syp4lybYkuycNfyWpjyGGrpdV1aEkLwB2JXmkqu5e7EqqajuwHfw9OknD6r1HV1WHuvujwG3ApbOqHGL0TztnnNuVSdKK6BV0SU5LcvrMNLAJ2Der2u3A27qzr68Enqqqw33alaTF6Dt03QDclmRmXX9WVZ9P8i6AqroB2AlsBg4A32P6v2qXpGXh/4yQtJYN8j8jvDJCUvMMOknNM+gkNc+gk9Q8g05S8ww6Sc0z6CQ1z6CT1DyDTlLzDDpJzTPoJDXPoJPUPINOUvMMOknNM+gkNc+gk9Q8g05S8ww6Sc0z6CQ1z6CT1DyDTlLzDDpJzTPoJDVvyUGX5LwkX0zy9SQPJXn3hDqXJ3kqyd7udn2/7krS4q3rsexx4Heq6r4kpwN7kuyqqq/PqvelqrqyRzuS1MuS9+iq6nBV3ddNfxd4GDhnqI5J0lAGOUaX5Hzg5cC9E2a/Ksn9Se5I8tI51rEtye4ku4fokyTNSFX1W0HyfOBvgf9eVZ+dNe8ngB9V1TNJNgP/s6ouWMA6+3VKUiv2VNXGvivptUeX5FTgM8DNs0MOoKqerqpnuumdwKlJzurTpiQtVp+zrgE+ATxcVR+ZUueFXT2SXNq19+2ltilJS9HnrOu/AX4deDDJ3q7svwA/DVBVNwBvBX4zyXHgH4Grqu9YWZIWqfcxuuXgMTpJndU/RidJJwKDTlLzDDpJzTPoJDXPoJPUPINOUvMMOknNM+gkNc+gk9Q8g05S8ww6Sc0z6CQ1z6CT1DyDTlLzDDpJzTPoJDXPoJPUPINOUvMMOknNM+gkNc+gk9Q8g05S8ww6Sc3rFXRJrkjyjSQHklw3Yf4/T3JrN//eJOf3aU+SlmLJQZfkFOBPgDcBFwFXJ7loVrVrgO9U1c8BHwU+uNT2JGmp+uzRXQocqKpvVtUPgE8DW2bV2QLc1E3/H+D1SdKjTUlatD5Bdw7w+Njjg13ZxDpVdRx4CvipHm1K0qKtW+0OzEiyDdjWPfw+sG8Vu3MW8ORJ3P5a6MNqt78W+nCytw9w4RAr6RN0h4Dzxh6f25VNqnMwyTrgJ4FvT1pZVW0HtgMk2V1VG3v0rZeTvf210IfVbn8t9OFkb3+mD0Osp8/Q9WvABUlenOR5wFXA7bPq3A5s7abfCvxNVVWPNiVp0Za8R1dVx5NcC9wJnALsqKqHkrwf2F1VtwOfAD6V5ABwjFEYStKK6nWMrqp2AjtnlV0/Nv3/gF9Zwqq39+nXAE729mH1+7Da7cPq9+Fkbx8G6kMcSUpqnZeASWreqgXdal8+luS8JF9M8vUkDyV594Q6lyd5Ksne7nb9pHX16MNjSR7s1v2cs0sZ+aNuGzyQ5JKB279w7LntTfJ0kvfMqjPoNkiyI8nRJPvGys5MsivJ/u5+/ZRlt3Z19ifZOqlOjz78YZJHuu18W5Izpiw752vWo/33JTk0tp03T1l2zs9Nj/ZvHWv7sSR7pyw7xPOf+Nlb1vdBVa34jdHJi0eBlwDPA+4HLppV57eAG7rpq4BbB+7D2cAl3fTpwN9N6MPlwF8u43Z4DDhrjvmbgTuAAK8E7l3m1+QJ4GeWcxsArwUuAfaNlX0IuK6bvg744ITlzgS+2d2v76bXD9iHTcC6bvqDk/qwkNesR/vvA/7zAl6jOT83S21/1vwPA9cv4/Of+NlbzvfBau3RrfrlY1V1uKru66a/CzzMc6/sWG1bgE/WyD3AGUnOXqa2Xg88WlXfWqb1A1BVdzM6Az9u/LW+CXjLhEXfCOyqqmNV9R1gF3DFUH2oqrtqdPUOwD2Mvhe6LKZsg4VYyOemV/vdZ+xXgVuW0L+Ftj/ts7ds74PVCro1dflYNyx+OXDvhNmvSnJ/kjuSvHTgpgu4K8mejK4MmW0h22koVzH9zb2c2wBgQ1Ud7qafADZMqLOS2+KdjPakJ5nvNevj2m7ovGPKsG0ltsFrgCNVtX/K/EGf/6zP3rK9D076kxFJng98BnhPVT09a/Z9jIZyvwD8L+AvBm7+sqq6hNEvwPx2ktcOvP4FyegL328G/nzC7OXeBs9So/HJqn0VIMl7gePAzVOqLNdr9jHgZ4GLgcOMho+r4Wrm3psb7PnP9dkb+n2wWkG3mMvHyDyXjy1VklMZbeibq+qzs+dX1dNV9Uw3vRM4NclZQ7VfVYe6+6PAbYyGJuMWsp2G8Cbgvqo6MqGPy7oNOkdmhuTd/dEJdZZ9WyR5O3Al8GvdB+05FvCaLUlVHamqH1bVj4A/nbLeZd0G3efsl4Fb5+jnIM9/ymdv2d4HqxV0q375WHcs4hPAw1X1kSl1XjhzXDDJpYy21yBhm+S0JKfPTDM6GD77hwxuB96WkVcCT43t2g9p6l/x5dwGY8Zf663A5ybUuRPYlGR9N6zb1JUNIskVwO8Cb66q702ps5DXbKntjx97/aUp613I56aPNwCPVNXBKX0c5PnP8dlbvvdBn7MnPc+8bGZ0tuVR4L1d2fsZvdEA/gWjodQB4KvASwZu/zJGu8YPAHu722bgXcC7ujrXAg8xOrt1D/DqAdt/Sbfe+7s2ZrbBePth9OOmjwIPAhuX4XU4jVFw/eRY2bJtA0aBehj4J0bHV65hdOz1r4H9wBeAM7u6G4GPjy37zu79cAB4x8B9OMDo2M/Me2HmjP+LgJ1zvWYDtf+p7jV+gNEH/uzZ7U/73AzRfld+48zrPlZ3OZ7/tM/esr0PvDJCUvNO+pMRktpn0ElqnkEnqXkGnaTmGXSSmmfQSWqeQSepeQadpOb9f2CcU1YTI3GJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make environment\n",
    "maze = eu.gridworld(grid_params)\n",
    "maze.set_rwd([(int(grid_params['y_height']/2),int(grid_params['x_width']/2))])\n",
    "env = eu.gymworld(maze) # openAI-like wrapper \n",
    "\n",
    "#update agent params dictionary with layer sizes appropriate for environment \n",
    "agent_params = sg.gen_input(maze, agent_params)\n",
    "MF,opt = ac.make_agent(agent_params, freeze=False)\n",
    "gp.make_env_plots(maze,env=True)\n",
    "\n",
    "\n",
    "agent_params['cachelim'] = int(0.5*np.prod(maze.grid.shape))\n",
    "\n",
    "EC = ec.ep_mem(MF,agent_params['cachelim']) \n",
    "print(maze.rwd_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00075"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param_ in opt.param_groups:\n",
    "    print(param_['lr'])\n",
    "1.5*agent_params['eta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n"
     ]
    }
   ],
   "source": [
    "print(maze.grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function for runs with episodic mem and without -- take use_EC as a param\n",
    "# assume just for conv inputs \n",
    "def run_trials(run_dict, use_EC, **kwargs):\n",
    "    save_data  = kwargs.get('save', True)\n",
    "    NUM_TRIALS = run_dict['NUM_TRIALS']\n",
    "    NUM_EVENTS = run_dict['NUM_EVENTS']\n",
    "    \n",
    "    blocktime = time.time()\n",
    "    \n",
    "    if use_EC:\n",
    "        EC.reset_cache()\n",
    "        run_dict['total_loss'] =  [[],[]]\n",
    "        run_dict['total_reward'] = []\n",
    "        add_mem_dict = {} #dictionary of items which get put into memory cache\n",
    "        timestamp    = 0\n",
    "        ststs = []\n",
    "        tslr = 1\n",
    "        track_cs = [[],[]]\n",
    "        compare_policies = {}\n",
    "        rpe = np.zeros(maze.grid.shape)\n",
    "        reward = 0\n",
    "        \n",
    "        for trial in range(NUM_TRIALS):\n",
    "            trialstart_stamp = timestamp\n",
    "\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "\n",
    "            env.reset() \n",
    "            \n",
    "            state = torch.Tensor(sg.get_frame(maze))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "\n",
    "            for event in range(NUM_EVENTS):\n",
    "                #compute confidence in MFC\n",
    "                if event in [0,1]:\n",
    "                    MF_cs = EC.make_pvals(tslr,envelope=10)\n",
    "                else: \n",
    "                    MF_cs = EC.make_pvals(tslr,envelope=10, pol_id = pol_flag, mfc=MF_cs)\n",
    "                # pass state through EC module\n",
    "                \n",
    "                policy_, value_, lin_act_ = MF(state, temperature = 1)\n",
    "                lin_act = tuple(np.round(lin_act_.data[0].numpy(),4))\n",
    "                if event is not 0:\n",
    "                    ec_pol = torch.from_numpy(EC.recall_mem(lin_act, timestamp, env=150))\n",
    "                    candidate_policies = [policy_, ec_pol]\n",
    "                    pol_choice = np.random.choice([0,1], p=[MF_cs, 1-MF_cs])\n",
    "                    pol = candidate_policies[pol_choice]\n",
    "                    if pol_choice == 0:\n",
    "                        pol_flag = 'MF'\n",
    "                    else:\n",
    "                        pol_flag = 'EC'\n",
    "                    choice, policy, value = ac.select_ec_action(MF, policy_, value_, pol)\n",
    "                else:\n",
    "                    choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "                \n",
    "                add_mem_dict['state'] = maze.cur_state\n",
    "                #compute eligibility trace/rpe approximation\n",
    "                delta = reward + agent_params['gamma']*value - v_last  \n",
    "                rpe[maze.cur_state[1]][maze.cur_state[0]] = delta\n",
    "                \n",
    "                \n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "                if event is not 0:\n",
    "                    if reward == 1:\n",
    "                        tslr = 0\n",
    "                    else:\n",
    "                        tslr += 1\n",
    "                    \n",
    "                    track_cs[0].append(tslr)\n",
    "                    track_cs[1].append(MF_cs)\n",
    "                \n",
    "                MF.rewards.append(reward)\n",
    "                \n",
    "                \n",
    "                add_mem_dict['activity']  = lin_act\n",
    "                add_mem_dict['action']    = choice\n",
    "                add_mem_dict['delta']     = delta\n",
    "                add_mem_dict['timestamp'] = timestamp            \n",
    "                \n",
    "                EC.add_mem(add_mem_dict)#add event to memory cache\n",
    "                \n",
    "                # because we need to include batch size of 1 \n",
    "                state = torch.Tensor(sg.get_frame(maze))\n",
    "                reward_sum += reward\n",
    "\n",
    "                v_last = value\n",
    "                timestamp += 1\n",
    "\n",
    "            p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "            \n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                #run_dict['total_loss'][0].append(p_loss.data[0])\n",
    "                #run_dict['total_loss'][1].append(v_loss.data[0])\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "\n",
    "            if trial ==0 or trial%10==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()\n",
    "        return compare_policies, track_cs, rpe\n",
    "\n",
    "    else:\n",
    "        rpe = np.zeros(maze.grid.shape)\n",
    "        run_dict['total_loss'] =  [[],[]]\n",
    "        run_dict['total_reward'] = []\n",
    "        for trial in range(NUM_TRIALS):\n",
    "            reward_sum   = 0\n",
    "            v_last       = 0\n",
    "            track_deltas = []\n",
    "            track_spots  = []\n",
    "            visited_locs = []\n",
    "\n",
    "            env.reset() \n",
    "            state = torch.Tensor(sg.get_frame(maze))\n",
    "            MF.reinit_hid() #reinit recurrent hidden layers\n",
    "\n",
    "            for event in range(NUM_EVENTS):\n",
    "                policy_, value_ = MF(state, agent_params['temperature'])[0:2]\n",
    "                choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "\n",
    "                if event < NUM_EVENTS: \n",
    "                    next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "                MF.rewards.append(reward)\n",
    "                #compute eligibility trace/rpe approximation\n",
    "                delta = reward + agent_params['gamma']*value - v_last  \n",
    "                state = torch.Tensor(sg.get_frame(maze))\n",
    "                rpe[maze.cur_state[1]][maze.cur_state[0]] = delta\n",
    "                \n",
    "                reward_sum += reward\n",
    "                v_last = value\n",
    "\n",
    "            p_loss, v_loss = ac.finish_trial(MF,agent_params['gamma'],opt)\n",
    "\n",
    "            if save_data:\n",
    "                #value_map = ac.generate_values(maze,MF)\n",
    "                run_dict['total_loss'][0].append(p_loss.item())\n",
    "                run_dict['total_loss'][1].append(v_loss.item())\n",
    "                run_dict['total_reward'].append(reward_sum)\n",
    "                #run_dict['val_maps'].append(value_map.copy())\n",
    "                #run_dict['deltas'].append(track_deltas)\n",
    "                #run_dict['spots'].append(track_spots)\n",
    "                #run_dict['vls'].append(visited_locs)\n",
    "\n",
    "            if trial ==0 or trial%100==0 or trial == NUM_TRIALS-1:\n",
    "                print(\"[{0}]  Trial {1} TotRew = {2} ({3:.3f}s)\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial+1, reward_sum,time.time()-blocktime)) #print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime)) \n",
    "                blocktime = time.time()\n",
    "        return rpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:07:09]  Trial 1 TotRew = 0 (0.205s)\n",
      "[17:07:27]  Trial 100 TotRew = 0 (17.478s)\n"
     ]
    }
   ],
   "source": [
    "#compare_policies, track_cs, rpe = run_trials(run_dict, True)\n",
    "rpe = run_trials(run_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:14:21]  Trial 1 TotRew = 0 (0.181s)\n",
      "[17:14:41]  Trial 100 TotRew = 0 (19.744s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annik/.local/lib/python3.6/site-packages/matplotlib/figure.py:98: MatplotlibDeprecationWarning: \n",
      "Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  \"Adding an axes using the same arguments as a previous axes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:14:41]  Trial 1 TotRew = 0 (0.223s)\n",
      "[17:15:04]  Trial 100 TotRew = 0 (22.055s)\n",
      "[17:15:04]  Trial 1 TotRew = 1 (0.298s)\n",
      "[17:15:27]  Trial 100 TotRew = 1 (23.327s)\n",
      "[17:15:27]  Trial 1 TotRew = 0 (0.286s)\n",
      "[17:15:51]  Trial 100 TotRew = 0 (24.006s)\n",
      "[17:15:52]  Trial 1 TotRew = 1 (0.280s)\n",
      "[17:16:15]  Trial 100 TotRew = 1 (23.465s)\n",
      "[17:16:16]  Trial 1 TotRew = 0 (0.269s)\n",
      "[17:16:38]  Trial 100 TotRew = 0 (22.753s)\n",
      "[17:16:39]  Trial 1 TotRew = 0 (0.261s)\n",
      "[17:17:02]  Trial 100 TotRew = 1 (23.590s)\n",
      "[17:17:03]  Trial 1 TotRew = 0 (0.273s)\n",
      "[17:17:25]  Trial 100 TotRew = 0 (22.633s)\n",
      "[17:17:25]  Trial 1 TotRew = 0 (0.284s)\n",
      "[17:17:48]  Trial 100 TotRew = 0 (22.955s)\n",
      "[17:17:49]  Trial 1 TotRew = 0 (0.257s)\n",
      "[17:18:12]  Trial 100 TotRew = 0 (23.215s)\n",
      "[17:18:12]  Trial 1 TotRew = 0 (0.275s)\n",
      "[17:18:36]  Trial 100 TotRew = 0 (23.997s)\n",
      "[17:18:37]  Trial 1 TotRew = 0 (0.278s)\n",
      "[17:19:03]  Trial 100 TotRew = 0 (25.979s)\n",
      "[17:19:03]  Trial 1 TotRew = 0 (0.275s)\n",
      "[17:19:27]  Trial 100 TotRew = 0 (24.427s)\n",
      "[17:19:28]  Trial 1 TotRew = 0 (0.275s)\n",
      "[17:19:53]  Trial 100 TotRew = 0 (25.084s)\n",
      "[17:19:53]  Trial 1 TotRew = 0 (0.363s)\n",
      "[17:20:23]  Trial 100 TotRew = 0 (30.094s)\n",
      "[17:20:23]  Trial 1 TotRew = 0 (0.288s)\n",
      "[17:20:50]  Trial 100 TotRew = 0 (26.509s)\n",
      "[17:20:50]  Trial 1 TotRew = 0 (0.348s)\n",
      "[17:21:18]  Trial 100 TotRew = 0 (27.986s)\n",
      "[17:21:19]  Trial 1 TotRew = 0 (0.447s)\n",
      "[17:21:45]  Trial 100 TotRew = 0 (25.690s)\n",
      "[17:21:45]  Trial 1 TotRew = 0 (0.275s)\n",
      "[17:22:12]  Trial 100 TotRew = 0 (27.078s)\n",
      "[17:22:12]  Trial 1 TotRew = 0 (0.266s)\n",
      "[17:22:36]  Trial 100 TotRew = 0 (23.416s)\n",
      "[17:22:36]  Trial 1 TotRew = 0 (0.289s)\n",
      "[17:23:02]  Trial 100 TotRew = 0 (26.309s)\n",
      "[17:23:03]  Trial 1 TotRew = 0 (0.371s)\n",
      "[17:23:28]  Trial 100 TotRew = 0 (25.729s)\n",
      "[17:23:29]  Trial 1 TotRew = 0 (0.271s)\n",
      "[17:23:51]  Trial 100 TotRew = 0 (22.212s)\n",
      "[17:23:51]  Trial 1 TotRew = 0 (0.254s)\n",
      "[17:24:14]  Trial 100 TotRew = 1 (22.435s)\n",
      "[17:24:14]  Trial 1 TotRew = 0 (0.248s)\n",
      "[17:24:37]  Trial 100 TotRew = 0 (23.268s)\n",
      "[17:24:38]  Trial 1 TotRew = 2 (0.268s)\n",
      "[17:25:09]  Trial 100 TotRew = 0 (31.623s)\n",
      "[17:25:10]  Trial 1 TotRew = 2 (0.395s)\n",
      "[17:25:35]  Trial 100 TotRew = 0 (25.239s)\n",
      "[17:25:35]  Trial 1 TotRew = 0 (0.279s)\n",
      "[17:25:56]  Trial 100 TotRew = 0 (21.300s)\n",
      "[17:25:57]  Trial 1 TotRew = 2 (0.239s)\n",
      "[17:26:18]  Trial 100 TotRew = 0 (21.119s)\n",
      "[17:26:18]  Trial 1 TotRew = 0 (0.208s)\n",
      "[17:26:38]  Trial 100 TotRew = 0 (20.208s)\n",
      "[17:26:39]  Trial 1 TotRew = 0 (0.203s)\n",
      "[17:26:59]  Trial 100 TotRew = 0 (20.653s)\n",
      "[17:26:59]  Trial 1 TotRew = 0 (0.228s)\n",
      "[17:27:20]  Trial 100 TotRew = 0 (20.773s)\n",
      "[17:27:20]  Trial 1 TotRew = 0 (0.221s)\n",
      "[17:27:41]  Trial 100 TotRew = 0 (20.994s)\n",
      "[17:27:42]  Trial 1 TotRew = 0 (0.217s)\n",
      "[17:28:03]  Trial 100 TotRew = 0 (21.020s)\n",
      "[17:28:03]  Trial 1 TotRew = 0 (0.224s)\n",
      "[17:28:23]  Trial 100 TotRew = 0 (20.161s)\n",
      "[17:28:23]  Trial 1 TotRew = 0 (0.222s)\n",
      "[17:28:44]  Trial 100 TotRew = 0 (20.493s)\n",
      "[17:28:44]  Trial 1 TotRew = 0 (0.216s)\n",
      "[17:29:05]  Trial 100 TotRew = 0 (20.577s)\n",
      "[17:29:05]  Trial 1 TotRew = 0 (0.206s)\n",
      "[17:29:26]  Trial 100 TotRew = 0 (20.545s)\n",
      "[17:29:26]  Trial 1 TotRew = 0 (0.202s)\n",
      "[17:29:47]  Trial 100 TotRew = 0 (21.353s)\n",
      "[17:29:47]  Trial 1 TotRew = 0 (0.226s)\n",
      "[17:30:08]  Trial 100 TotRew = 0 (20.649s)\n",
      "[17:30:08]  Trial 1 TotRew = 0 (0.232s)\n",
      "[17:30:30]  Trial 100 TotRew = 0 (21.987s)\n",
      "[17:30:30]  Trial 1 TotRew = 0 (0.214s)\n",
      "[17:30:51]  Trial 100 TotRew = 0 (20.362s)\n",
      "[17:30:51]  Trial 1 TotRew = 0 (0.211s)\n",
      "[17:31:12]  Trial 100 TotRew = 0 (20.541s)\n",
      "[17:31:12]  Trial 1 TotRew = 0 (0.229s)\n",
      "[17:31:33]  Trial 100 TotRew = 0 (20.724s)\n",
      "[17:31:33]  Trial 1 TotRew = 0 (0.218s)\n",
      "[17:31:54]  Trial 100 TotRew = 0 (20.999s)\n",
      "[17:31:54]  Trial 1 TotRew = 0 (0.215s)\n",
      "[17:32:15]  Trial 100 TotRew = 0 (20.972s)\n",
      "[17:32:15]  Trial 1 TotRew = 0 (0.228s)\n",
      "[17:32:37]  Trial 100 TotRew = 0 (21.918s)\n",
      "[17:32:38]  Trial 1 TotRew = 1 (0.217s)\n",
      "[17:32:58]  Trial 100 TotRew = 0 (20.327s)\n",
      "[17:32:58]  Trial 1 TotRew = 0 (0.230s)\n",
      "[17:33:19]  Trial 100 TotRew = 0 (20.400s)\n",
      "[17:33:19]  Trial 1 TotRew = 0 (0.235s)\n",
      "[17:33:40]  Trial 100 TotRew = 0 (20.766s)\n",
      "[17:33:40]  Trial 1 TotRew = 0 (0.220s)\n",
      "[17:34:00]  Trial 100 TotRew = 0 (20.302s)\n",
      "[17:34:00]  Trial 1 TotRew = 0 (0.219s)\n",
      "[17:34:21]  Trial 100 TotRew = 0 (20.861s)\n",
      "[17:34:21]  Trial 1 TotRew = 0 (0.241s)\n",
      "[17:34:43]  Trial 100 TotRew = 0 (21.989s)\n",
      "[17:34:44]  Trial 1 TotRew = 0 (0.227s)\n",
      "[17:35:04]  Trial 100 TotRew = 1 (20.431s)\n",
      "[17:35:04]  Trial 1 TotRew = 0 (0.222s)\n",
      "[17:35:25]  Trial 100 TotRew = 0 (20.510s)\n",
      "[17:35:25]  Trial 1 TotRew = 0 (0.226s)\n",
      "[17:35:45]  Trial 100 TotRew = 1 (19.418s)\n",
      "[17:35:45]  Trial 1 TotRew = 0 (0.239s)\n",
      "[17:36:06]  Trial 100 TotRew = 0 (20.894s)\n",
      "[17:36:06]  Trial 1 TotRew = 0 (0.243s)\n",
      "[17:36:26]  Trial 100 TotRew = 0 (19.973s)\n",
      "[17:36:26]  Trial 1 TotRew = 0 (0.213s)\n",
      "[17:36:47]  Trial 100 TotRew = 0 (21.130s)\n",
      "[17:36:48]  Trial 1 TotRew = 0 (0.235s)\n"
     ]
    }
   ],
   "source": [
    "#### run a bunch of trials to get rwds to average \n",
    "for i in range(100):\n",
    "    MF,opt = ac.make_agent(agent_params, freeze=False)\n",
    "    \n",
    "    agent_params['cachelim'] = int(0.5*np.prod(maze.grid.shape))\n",
    "\n",
    "    EC = ec.ep_mem(MF,agent_params['cachelim']) \n",
    "    \n",
    "    rpe = run_trials(run_dict, False)\n",
    "    with open('early_learn_mf_only.csv', 'a+') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerows([run_dict['total_reward']])\n",
    "\n",
    "    writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(run_dict['total_reward'], label='r')\n",
    "#plt.plot(run_dict['total_loss'][0], label='p')\n",
    "#plt.plot(run_dict['total_loss'][1], label='v')\n",
    "plt.legend(bbox_to_anchor=(1.1,1.1))\n",
    "\n",
    "print(run_dict['total_reward'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early_learn_mf_only = run_dict['total_reward']\n",
    "#early_learn_w_ec = run_dict['total_reward']\n",
    "#late_learn_mf_only = run_dict['total_reward']\n",
    "#late_learn_w_ec = run_dict['total_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(run_dict['total_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, sharex = True, sharey=True)\n",
    "ax[0].plot(early_learning_w_ec, 'b', alpha =0.7, label = 'With EC')\n",
    "ax[0].plot(early_learn_mf_only, 'k--', alpha = 0.7, label=\"MF only\")\n",
    "\n",
    "ax[1].plot(late_learn_w_ec, 'r', label='With EC')\n",
    "ax[1].plot(late_learn_mf_only, 'k--', alpha = 0.7, label ='MF only')\n",
    "\n",
    "ax[0].legend(bbox_to_anchor = (1.0,.75))\n",
    "ax[1].legend(bbox_to_anchor = (1.0,.75))\n",
    "fig.savefig('compare.svg', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rpe) \n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "cmap = plt.cm.Spectral_r\n",
    "cNorm = colors.Normalize(vmin=0, vmax=1)\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap = cmap)\n",
    "\n",
    "ax1 = fig.add_axes([0.04, 0, 0.85, 0.85])\n",
    "axc = fig.add_axes([0.8, 0, 0.05, 0.85])\n",
    "\n",
    "cb1 = colorbar.ColorbarBase(axc, cmap = cmap, norm = cNorm)\n",
    "ax1.imshow(maze.grid, vmin=0, vmax=1, cmap='bone', interpolation='none')\n",
    "ax1.add_patch(patches.Circle(maze.rwd_loc[0], 0.35, fc='w'))\n",
    "\n",
    "for entry in compare_policies.keys():\n",
    "    x = entry[0]\n",
    "    y = entry[1]\n",
    "    \n",
    "    ec_policy = compare_policies[entry][1][0] - compare_policies[entry][0][0]\n",
    "    action = np.argmax(ec_policy)\n",
    "    prob = max(ec_policy)\n",
    "    \n",
    "    dx1, dy1, head_w, head_l = gp.make_arrows(action,prob)\n",
    "    if prob > 1/4:\n",
    "        if (dx1, dy1) == (0,0):\n",
    "            pass\n",
    "        else:\n",
    "            colorVal1 = scalarMap.to_rgba(prob)\n",
    "            ax1.arrow(x,y, dx1, dy1, head_width=0.3, head_length=0.2, color = colorVal1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "ax1.invert_yaxis()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = compare_policies[(7,5)][1]\n",
    "for i in range(len(x)):\n",
    "    plt.bar(np.arange(6)+(0.1*i), x[i], width = 0.1, alpha = 0.3)\n",
    "    plt.bar(np.arange(6), np.ones(6)*0.16666667, width =0.1, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(DND.cache_list))\n",
    "def cosine_sim(mem_dict, key, **kwargs):\n",
    "    similarity_threshold = kwargs.get('threshold', 0.9)\n",
    "\n",
    "    mem_cache = np.asarray(list(mem_dict.keys()))\n",
    "    print(mem_cache.shape)\n",
    "    entry = np.asarray(key)\n",
    "\n",
    "    mqt = np.dot(mem_cache, entry)\n",
    "    norm = np.linalg.norm(mem_cache, axis=1) * np.linalg.norm(entry)\n",
    "\n",
    "    cosine_similarity = mqt / norm\n",
    "\n",
    "    index = np.argmax(cosine_similarity)\n",
    "    similar_activity = mem_cache[index]\n",
    "    if max(cosine_similarity) >= similarity_threshold:\n",
    "        return similar_activity, index, max(cosine_similarity)\n",
    "\n",
    "    else:\n",
    "        # print('max memory similarity:', max(cosine_similarity))\n",
    "        return [], [], max(cosine_similarity)\n",
    "\n",
    "def make_pvals(p, **kwargs):\n",
    "    envelope = kwargs.get('envelope', 50)\n",
    "    return np.round(1 / np.cosh(p / envelope),8)\n",
    "\n",
    "def softmax(x, T=1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp((x - np.max(x))/T)\n",
    "    return e_x / e_x.sum(axis=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_key, ind, max_cs = cosine_sim(DND.cache_list, sts[0])\n",
    "print('sim', max_cs)\n",
    "memory = np.nan_to_num(DND.cache_list[tuple(state_key)][0])\n",
    "deltas = memory[:,0]\n",
    "print(deltas)\n",
    "rec_times = memory[:,1]\n",
    "print(\"rtimes\",rec_times)\n",
    "times        = abs(timestep - memory[:,1])\n",
    "print('tiems', times)\n",
    "pv =make_pvals(times)\n",
    "print(pv)\n",
    "\n",
    "mult = np.multiply(deltas, pv)\n",
    "print(softmax(max_cs*mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in sts[0:10]:\n",
    "    timestep = 1000\n",
    "    a = DND.recall_mem(key=i,timestep=timestep,env=100)\n",
    "    print(a,\"----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs =['r','g','b','c', 'y','pink']\n",
    "'''\n",
    "r = list(self.cache_list.keys())\n",
    "g = [t for e, t in self.cache_list.values()]\n",
    "b = lp = persistence_.index(min(persistence_))\n",
    "c = old_activity = cache_keys[lp]\n",
    "y = del self.cache_list[old_activity]\n",
    "'''\n",
    "plt.figure()\n",
    "for i in range(len(EC.stupid_df)):\n",
    "    xs = np.arange(len(EC.stupid_df[i]))\n",
    "    ys = EC.stupid_df[i]\n",
    "    \n",
    "    plt.scatter(xs, ys, c=cs[i], alpha=0.3)\n",
    "plt.ylim([-0.00002,0.00002])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(run_dict['total_reward'])\n",
    "plt.ylim([0,run_dict['NUM_EVENTS']])\n",
    " \n",
    "plt.figure(2)\n",
    "plt.plot(run_dict['total_loss'][0], label = 'pol')\n",
    "plt.plot(run_dict['total_loss'][1], label = 'val')\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "plt.close()\n",
    "#gp.print_value_maps(maze, run_dict['val_maps'], maps=0, val_range=(-1,50), save_dir=fig_savedir, title='Value Map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.torch.save(MF,agent_params['load_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
