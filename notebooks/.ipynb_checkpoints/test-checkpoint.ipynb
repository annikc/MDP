{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys \n",
    "sys.path.insert(0,'../rl_network'); import actorcritic as ac;  import stategen as sg\n",
    "sys.path.insert(0,'../environments/'); import gridworld as eu\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline \n",
    "fig_savedir = '../data/figures/'\n",
    "print_freq = 1/10\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment parameters \n",
    "y_height = 8\n",
    "x_width = 8\n",
    "walls = True\n",
    "if not walls:\n",
    "    y_height = y_height+2\n",
    "    x_width = x_width+2\n",
    "rho = 0\n",
    "maze_type = 'none'\n",
    "port_shift = 'none'\n",
    "\n",
    "#make environment\n",
    "maze = eu.gridworld([y_height, x_width], \n",
    "                    rho = rho, \n",
    "                    maze_type = maze_type, \n",
    "                    port_shift = port_shift, \n",
    "                    walls = walls)\n",
    "#maze.rwd_loc = [(int(y_new/2),int(x_new/2))]\n",
    "for i in maze.rwd_loc: \n",
    "    maze.orig_rwd_loc.append(i)\n",
    "\n",
    "\n",
    "\n",
    "state_type = 'pcs'\n",
    "\n",
    "if state_type == 'pcs':\n",
    "    # place cell parameters\n",
    "    num_pc = 1000\n",
    "    fwhm = 0.05\n",
    "    pcs = sg.PlaceCells(num_cells=num_pc, grid=maze, fwhm=fwhm)\n",
    "\n",
    "    #show environment\n",
    "    eu.make_env_plots(maze,env=True,pc_map=True,pcs=pcs, save=False)\n",
    "else: \n",
    "    eu.make_env_plots(maze,env=True)\n",
    "\n",
    "## test out gridworld wrapper. \n",
    "env = eu.gymworld(maze)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if state_type == 'conv':\n",
    "    num_channels = 3 \n",
    "    if maze.bound:\n",
    "        input_dims = (y_height+2,x_width+2,num_channels)\n",
    "    else: \n",
    "        input_dims = (y_height, x_width, num_channels)\n",
    "    hid_types = ['conv', 'pool', 'linear']\n",
    "    hid_dims = [(9,9,num_channels), (8,8,num_channels), 500]\n",
    "\n",
    "elif state_type == 'pcs':\n",
    "    input_dims = 1000\n",
    "    hid_types = ['linear']\n",
    "    hid_dims = [500]\n",
    "    \n",
    "action_dims = len(maze.actionlist)\n",
    "batch_size = 1\n",
    "\n",
    "NUM_EVENTS = 100\n",
    "NUM_TRIALS = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(ac)\n",
    "MF = ac.AC_Net(input_dims, action_dims, batch_size, hid_types, hid_dims)\n",
    "discount_factor = 0.98\n",
    "\n",
    "eta = 5e-4 #gradient descent learning rate\n",
    "opt = ac.optim.Adam(MF.parameters(), lr = eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = [[],[]]\n",
    "total_reward = []\n",
    "val_maps = []\n",
    "\n",
    "blocktime = time.time()\n",
    "for trial in range(NUM_TRIALS):\n",
    "    reward_sum = 0\n",
    "    if state_type == 'pcs':\n",
    "        get_pcs = pcs.activity(env.reset())\n",
    "        state = ac.Variable(ac.torch.FloatTensor(get_pcs))\n",
    "    elif state_type == 'conv':\n",
    "        env.reset()\n",
    "        # because we need to include batch size of 1 \n",
    "        frame = np.expand_dims(sg.get_frame(maze), axis=0)\n",
    "        state = ac.Variable(ac.torch.FloatTensor(frame))\n",
    "        \n",
    "    MF.reinit_hid()\n",
    "    for event in range(NUM_EVENTS):\n",
    "        policy_, value_ = MF(state)\n",
    "        choice, policy, value = ac.select_action(MF,policy_, value_)\n",
    "        if event < NUM_EVENTS: \n",
    "            next_state, reward, done, info = env.step(choice)\n",
    "\n",
    "        MF.rewards.append(reward)\n",
    "        if state_type == 'pcs':\n",
    "            state = ac.Variable(ac.torch.FloatTensor(pcs.activity(next_state)))       # update state\n",
    "        elif state_type == 'conv':\n",
    "            # because we need to include batch size of 1 \n",
    "            frame = np.expand_dims(sg.get_frame(maze), axis = 0)\n",
    "            state = ac.Variable(ac.torch.FloatTensor(frame))\n",
    "        reward_sum += reward\n",
    "\n",
    "    p_loss, v_loss = ac.finish_trial(MF, discount_factor,opt)\n",
    "    \n",
    "    total_loss[0].append(p_loss.data[0])\n",
    "    total_loss[1].append(v_loss.data[0])\n",
    "    total_reward.append(reward_sum)\n",
    "    if state_type == 'pcs':\n",
    "        value_map = ac.generate_values(maze,MF,pcs=pcs)\n",
    "    else:\n",
    "        value_map = ac.generate_values(maze,MF)\n",
    "    val_maps.append(value_map.copy())\n",
    "    \n",
    "    #if trial%10 == 0:\n",
    "    #    eu.save_value_map(value_map.copy(), maze, trial)\n",
    "    \n",
    "    if trial%(print_freq*NUM_TRIALS)==0 or trial == NUM_TRIALS-1: \n",
    "        print(\"[{0}]  Trial {1} total reward = {2} (Avg {3:.3f})\".format(time.strftime(\"%H:%M:%S\", time.localtime()), trial, reward_sum, float(reward_sum)/float(NUM_EVENTS)), \"Block took {0:.3f}\".format(time.time()-blocktime))\n",
    "        blocktime = time.time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_loss[0], label='p')\n",
    "plt.plot(total_loss[1], label='v')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(total_reward, label='r', color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(eu)\n",
    "eu.print_value_maps(maze,\n",
    "                    val_maps,\n",
    "                    maps='all', #list(np.arange(841, 894)), \n",
    "                    val_range=(-1,50),\n",
    "                    save_dir=fig_savedir,\n",
    "                    title='Value Map') ### see individual map with kwarg maps=X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store_data = {}\n",
    "store_data[state_type] = [total_loss, total_reward, val_maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
