{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "sys.path.insert(0,'../environments/'); import gw\n",
    "sys.path.insert(0,'../environments/'); import gridworld_plotting as gp\n",
    "sys.path.insert(0,'../rl_network/'); import ac\n",
    "sys.path.insert(0,'../memory/'); import episodic as ec\n",
    "\n",
    "import experiment as expt\n",
    "from experiment import run_expt, run_mem_expt\n",
    "#####\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_expt(NUM_TRIALS, NUM_EVENTS, env, agent, data, **kwargs):\n",
    "    print_freq = kwargs.get('printfreq', 0.1)\n",
    "\n",
    "    get_samples = kwargs.get('get_samples', False)\n",
    "    if get_samples:\n",
    "        sample_observations = env.get_sample_obs()\n",
    "\n",
    "    around_reward = kwargs.get('around_reward', True)\n",
    "    start_radius = kwargs.get('radius', 5)\n",
    "\n",
    "    start_locs = kwargs.get('room', None)\n",
    "\n",
    "    t = time.time()\n",
    "    data['start_count'] = np.zeros(env.shape)\n",
    "    for trial in range(NUM_TRIALS):\n",
    "        # reset environment, reinitialize agent in environment\n",
    "        env.resetEnvironment(around_rwd=around_reward, radius=start_radius)\n",
    "\n",
    "        if start_locs is not None:\n",
    "            # st = np.random.choice(len(start_locs))\n",
    "            st = start_locs[trial]\n",
    "            env.set_state(env.twoD2oneD(st))\n",
    "\n",
    "        loc1 = env.oneD2twoD(env.state)\n",
    "        data['start_count'][loc1[0], loc1[1]] += 1\n",
    "        # env.set_state(env.twoD2oneD((19,19)))\n",
    "        # clear hidden layer cache if using lstm or gru cells\n",
    "        agent.reinit_hid()\n",
    "        reward_sum = 0\n",
    "\n",
    "        for event in range(NUM_EVENTS):\n",
    "            # get state observation\n",
    "            observation = torch.Tensor(np.expand_dims(env.get_observation(), axis=0))\n",
    "\n",
    "            # pass observation through network\n",
    "            # policy_, value_ = agent(observation)\n",
    "            policy_, value_, lin_acts_ = agent(observation, lin_act=[4,5])\n",
    "            phi = lin_acts_[0].data[0].detach()\n",
    "            psi = lin_acts_[1].data[0].detach()\n",
    "            \n",
    "            agent.saved_phi.append(phi)\n",
    "            agent.saved_psi.append(psi)\n",
    "\n",
    "            # select action from policy\n",
    "            choice = agent.select_action(policy_, value_)\n",
    "            action = env.action_list[choice][0]\n",
    "\n",
    "            # take a step in the environment\n",
    "\n",
    "            s_1d, reward, isdone = env.move(action)\n",
    "\n",
    "            agent.saved_rewards.append(reward)\n",
    "            reward_sum += reward\n",
    "            ###optional\n",
    "            # sar.append(env.state, action, reward) ## tracking oneD states\n",
    "            if isdone:\n",
    "                break\n",
    "\n",
    "        p_loss, v_loss ,q_loss = agent.finish_trial()\n",
    "        data['trial_length'].append(event)\n",
    "        data['total_reward'].append(reward_sum)\n",
    "        data['loss'][0].append(p_loss.item())\n",
    "        data['loss'][1].append(v_loss.item())\n",
    "        data['trials_run_to_date'] += 1\n",
    "        if get_samples:\n",
    "            pol_grid, val_grid = sample_PV(sample_observations, env, agent)\n",
    "            data['pol_tracking'].append(pol_grid)\n",
    "            data['val_tracking'].append(val_grid)\n",
    "            data['t'].append(trial)\n",
    "\n",
    "        if trial == 0 or trial % int(print_freq * NUM_TRIALS) == 0 or trial == NUM_TRIALS - 1:\n",
    "            print(f\"{trial}: start at {loc1} {reward_sum} ({time.time() - t}s)\")\n",
    "\n",
    "            # num_states = len(env.useable)\n",
    "            # cos_sim = np.zeros((num_states,num_states))\n",
    "\n",
    "            # observations = env.get_sample_obs()\n",
    "            # p,v,lin_acts4 = agent(torch.Tensor(observations[0]), lin_act=4)\n",
    "\n",
    "            # LA4 = lin_acts4.data.numpy()\n",
    "\n",
    "            # cs4 = cosine_similarity(LA4,LA4)\n",
    "\n",
    "            # plt.figure(0)\n",
    "            # plt.pcolor(cs4)\n",
    "            # plt.show()\n",
    "            t = time.time()\n",
    "\n",
    "        if around_reward and trial > 0 and trial == int(\n",
    "                NUM_TRIALS / 2):  # np.mean(data['trial_length'][-20:])< 2*start_radius:\n",
    "            print(trial)\n",
    "            # around_reward = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(gw)\n",
    "rerun = False\n",
    "\n",
    "rows, columns = 10,10\n",
    "env_type = None\n",
    "penalty = -0.01 \n",
    "\n",
    "NUM_TRIALS = 3\n",
    "NUM_EVENTS = 100\n",
    "\n",
    "\n",
    "if not rerun:\n",
    "    reward_location = (3,3)\n",
    "    env = gw.GridWorld(rows=rows,cols=columns,\n",
    "                       env_type=env_type,\n",
    "                       rewards = {reward_location:1, (5,7):-10},#rewards={(int(rows/2),int(columns/2)):1},#\n",
    "                       step_penalization=penalty,\n",
    "                       rho=0.0,\n",
    "                       rewarded_action = None)\n",
    "    \n",
    "    agent_params = {\n",
    "        'load_model':  False,\n",
    "        'load_dir':    f'../data/outputs/gridworld/openfield{rows}{columns}.pt',\n",
    "        'freeze_w':    False,\n",
    "\n",
    "        'input_dims':  env.observation.shape,\n",
    "        'action_dims': len(env.action_list),\n",
    "        'hidden_types':['conv','pool','conv', 'pool', 'linear','linear'],\n",
    "        'hidden_dims': [None, None, None, None, 100, 200],\n",
    "\n",
    "        'rfsize':      5,\n",
    "        'stride':      1,\n",
    "        'padding':     1,\n",
    "        'dilation':    1,\n",
    "\n",
    "        'gamma':       0.98,\n",
    "        'eta':         5e-4,\n",
    "\n",
    "        'use_EC':      True,\n",
    "        'EC':          {},\n",
    "        'cachelim':    300,\n",
    "        'mem_temp':    1\n",
    "        }\n",
    "    agent = ac.make_agent(agent_params)\n",
    "    episodic_memory = ec.ep_mem(agent, 300)\n",
    "    data = {'total_reward': [],\n",
    "            'loss': [[],[]],\n",
    "            'trial_length': [],\n",
    "            'trials_run_to_date':0,\n",
    "            'pol_tracking':[],\n",
    "            'val_tracking':[],\n",
    "            'ec_tracking': [],\n",
    "            't': [],\n",
    "            'mfcs':[]\n",
    "           }\n",
    "\n",
    "else:\n",
    "    reward_location = (16,3)\n",
    "    env = gw.GridWorld(rows=rows,cols=columns,\n",
    "                       env_type=env_type,\n",
    "                       rewards = {reward_location:1},#rewards={(int(rows/2),int(columns/2)):1},#\n",
    "                       step_penalization=penalty,\n",
    "                       rho=0.0,\n",
    "                       rewarded_action = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(gw)\n",
    "fig = gw.plotWorld(env,current_state=True,scale=0.35, states=True) \n",
    "rooms = {'1': [], '2':[], '3':[], '4':[]}\n",
    "for i in range(rows):\n",
    "    for j in range(columns):\n",
    "        if (i,j) in env.obstacles_list:\n",
    "            pass\n",
    "        else:\n",
    "            if i < 5:\n",
    "                if j <5:\n",
    "                    rooms['1'].append((i,j))\n",
    "                if j >5:\n",
    "                    rooms['2'].append((i,j))\n",
    "                if j ==5: \n",
    "                    rooms['1'].append((i,j))\n",
    "                    rooms['2'].append((i,j))\n",
    "            if i >5:\n",
    "                if j <5:\n",
    "                    rooms['3'].append((i,j))\n",
    "                if j >5:\n",
    "                    rooms['4'].append((i,j))\n",
    "                if j ==5: \n",
    "                    rooms['3'].append((i,j))\n",
    "                    rooms['4'].append((i,j))\n",
    "\n",
    "            if i == 5:\n",
    "                if j<5:\n",
    "                    rooms['1'].append((i,j))\n",
    "                    rooms['3'].append((i,j))\n",
    "                if j>5: \n",
    "                    rooms['2'].append((i,j))\n",
    "                    rooms['4'].append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "sample_observations = env.get_sample_obs()\n",
    "print(int(NUM_TRIALS/len(env.useable)))\n",
    "shuf = int(NUM_TRIALS/len(env.useable))*(env.useable)\n",
    "random.shuffle(shuf)\n",
    "\n",
    "run_expt(NUM_TRIALS, NUM_EVENTS, env, agent, data, get_samples=True, printfreq = 1, around_reward=True)\n",
    "pol_grid, val_grid = expt.sample_PV(sample_observations, env,agent)\n",
    "gp.plot_polmap(env, pol_grid, threshold = 0.25)\n",
    "gp.plot_valmap(env, val_grid, v_range = [-1,1])\n",
    "\n",
    "plt.imshow(data['start_count'])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, sharex=True)\n",
    "#ax[0].plot(data['trial_length'])\n",
    "ax[0].plot(gp.running_mean(data['total_reward'],1))\n",
    "ax[1].plot(data['loss'][0], label='p')\n",
    "ax[1].plot(data['loss'][1], label='v')\n",
    "ax[1].legend(loc=0)\n",
    "ax[0].set_ylim([-10.,1])\n",
    "plt.show()\n",
    "sample_observations = env.get_sample_obs()\n",
    "pol_grid, val_grid = expt.sample_PV(sample_observations, env,agent)\n",
    "valmin = np.floor(np.min(val_grid))\n",
    "valmax = np.ceil(np.max(val_grid))\n",
    "\n",
    "gp.plot_polmap(env, pol_grid, threshold = 0.25, save=False, directory = './Blake Meeting/', title='p_after reward switch 7500', filetype='svg')\n",
    "gp.plot_valmap(env, val_grid, v_range = [-0.5,1], save=False, directory = './Blake Meeting/', title='v_after reward switch 7500', filetype='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(ec)\n",
    "episodic_memory = ec.ep_mem(agent, 300)\n",
    "run_mem_expt(NUM_TRIALS, NUM_EVENTS, env, agent, episodic_memory, data, get_samples=True, printfreq=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "observations = env.get_sample_obs()\n",
    "states = observations[1]\n",
    "c = np.zeros(len(states))\n",
    "for ind, x in enumerate(states):\n",
    "    if x in rooms['1']:\n",
    "        c[ind] = 1\n",
    "    elif x in rooms['2']:\n",
    "        c[ind] =2\n",
    "    elif x in rooms['3']:\n",
    "        c[ind] = 3\n",
    "    elif x in rooms['4']:\n",
    "        c[ind] = 4\n",
    "p,v,lin_acts4 = agent(torch.Tensor(observations[0]), lin_act=4)\n",
    "\n",
    "LA4 = lin_acts4.data.numpy()\n",
    "\n",
    "k = pca.fit_transform(LA4)\n",
    "plt.scatter(k[:,0],k[:,1],c=c, cmap =plt.cm.get_cmap('Accent_r',5))\n",
    "plt.colorbar(ticks=range(4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_states = len(env.useable)\n",
    "cos_sim = np.zeros((num_states,num_states))\n",
    "\n",
    "observations = env.get_sample_obs()\n",
    "p,v,lin_acts4 = agent(torch.Tensor(observations[0]), lin_act=4)\n",
    "p,v,lin_acts5 = agent(torch.Tensor(observations[0]), lin_act=5)\n",
    "\n",
    "LA4 = lin_acts4.data.numpy()\n",
    "LA5 = lin_acts5.data.numpy()\n",
    "\n",
    "cs4 = cosine_similarity(LA4,LA4)\n",
    "cs5 = cosine_similarity(LA5,LA5)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.pcolor(cs4)\n",
    "plt.figure(1)\n",
    "plt.pcolor(cs5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "index =  52\n",
    "ind_state = env.oneD2twoD(index)\n",
    "cos_map4 = cs4[index,:]\n",
    "\n",
    "def euc(x,y):\n",
    "    return math.sqrt(sum([(a - b) ** 2 for a, b in zip(x, y)]))\n",
    "\n",
    "index_map4 = np.zeros((rows,columns))\n",
    "euc4 = [[],[]]\n",
    "for ind, c in enumerate(cos_map4):\n",
    "    coord = env.oneD2twoD(ind)\n",
    "    index_map4[coord[0],coord[1]] = c\n",
    "    euc4[0].append(euc(ind_state,coord))\n",
    "    euc4[1].append(c)\n",
    "    \n",
    "cos_map5 = cs5[index,:]\n",
    "euc5 = [[],[]]\n",
    "index_map5 = np.zeros((rows,columns))\n",
    "for ind, c in enumerate(cos_map5):\n",
    "    coord = env.oneD2twoD(ind)\n",
    "    index_map5[coord[0],coord[1]] = c\n",
    "    \n",
    "    euc5[0].append(euc(ind_state,coord))\n",
    "    euc5[1].append(c)\n",
    "    \n",
    "gp.plot_valmap(env,index_map4, rwds= [env.oneD2twoD(index)],title='Similarity in Layer 4 Representations')\n",
    "gp.plot_valmap(env,index_map5, rwds= [env.oneD2twoD(index)],title='Similarity in Layer 5 Representations')\n",
    "\n",
    "plt.scatter(euc4[0],euc4[1],label = f'R={np.round(pearsonr(euc4[0], euc4[1])[0],4)}')\n",
    "plt.scatter(euc5[0],euc5[1],label = f'R={np.round(pearsonr(euc5[0], euc5[1])[0],4)}')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr, _ = pearsonr(data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_env = ec.calc_env(halfmax = 20)\n",
    "mem_temp = 0.3\n",
    "state = torch.Tensor([env.get_observation(agtlocation=(9,16))])\n",
    "p_, v_, l_ = agent(state, lin_act=2)\n",
    "lin_act = tuple(np.round(l_.data[0].numpy(), 4))\n",
    "track = []\n",
    "\n",
    "for i in range(0,sum(data['trial_length']),500):\n",
    "    memory_policy = torch.from_numpy(episodic_memory.recall_mem(lin_act, \n",
    "                                                               i, \n",
    "                                                               env=recency_env, mem_temp=mem_temp))\n",
    "    track.append(memory_policy.data.numpy())\n",
    "print(env.action_list)\n",
    "print(memory_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mat = np.array(list[episodic_memory.cache_list.keys()])\n",
    "true_key, sim = episodic_memory.cosine_sim(lin_act)\n",
    "entry = episodic_memory.cache_list[true_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(track[0])):\n",
    "    plt.plot(track[i], label=env.action_list[i])\n",
    "plt.xlim([0,10])\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.savefig('./Blake Meeting/Total_reward.svg', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.obstacles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['env'] = env.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(data, open('../Blake Meeting/simple_network/test.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savestring = f'../data/outputs/gridworld/4rooms{rows}{columns}.pt'\n",
    "ac.torch.save(agent,savestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ac)\n",
    "for i in range(0,1):\n",
    "    abcd, efgh = ac.mem_snapshot(env, \n",
    "                           episodic_memory, \n",
    "                           trial_timestamp = 3, \n",
    "                           decay           = ec.calc_env(halfmax = 2000), \n",
    "                           mem_temp        = 1,\n",
    "                                get_vals=True)#agent_params['mem_temp'])\n",
    "    gp.plot_polmap(env, abcd,save=False, threshold = 0.22, title='ec_pol_new_rwd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_grid = np.zeros(env.shape, dtype=[('D', 'f8'), ('U', 'f8'), ('R', 'f8'), ('L', 'f8'), ('J', 'f8'), ('P', 'f8')])\n",
    "for i in range(rows):\n",
    "    for j in range(columns):\n",
    "        obs = torch.Tensor(np.expand_dims(env.get_observation(agtlocation= (i,j)), axis=0))\n",
    "        p,v, lin_act = agent(obs, lin_act=2)\n",
    "        h = tuple(np.round(lin_act.data[0].numpy(),4))\n",
    "        h_mem, sim = episodic_memory.cosine_sim(h)\n",
    "        \n",
    "        infos = episodic_memory.cache_list[h_mem]\n",
    "        \n",
    "        \n",
    "        memory = infos[0]\n",
    "        t = infos[1]\n",
    "        state = infos[2]\n",
    "        \n",
    "        decay = 20\n",
    "        p_val = np.round( 1/ np.cosh((len(data['trial_length'])-t)/decay), 8)\n",
    "        \n",
    "        p_prime = np.nan_to_num(memory[:,0])\n",
    "        policy = tuple(ec.softmax(sim*p_prime))\n",
    "        print(f'({i},{j}) {sim} with {state}:\\n{p_prime}\\n{policy}\\n======')\n",
    "        if sim < 0.8:\n",
    "            pol_grid[i,j] = tuple(ec.softmax([1,1,1,1,1,1]))\n",
    "        else:\n",
    "            pol_grid[i,j] = policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data['trial_length']))\n",
    "gp.plot_polmap(env, pol_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j = (10,2)\n",
    "print(pol_grid[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=2)\n",
    "for x in env.action_list:\n",
    "    gp.plot_valmap(env, efgh[x[0]], v_range = [0,1], title=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in list(episodic_memory.cache_list.values()):\n",
    "    print(f'=====\\n{x[2]}\\n{x[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((3,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.state, oneD2twoD(env.state, env.shape))\n",
    "fig = plotWorld(env,current_state=False)\n",
    "\n",
    "num_steps = 10\n",
    "moves = True\n",
    "if moves:\n",
    "    for _ in range(num_steps):\n",
    "        if _ == 0:\n",
    "            action_choice = \"\"\n",
    "        start_at = oneD2twoD(env.state, env.shape)\n",
    "        # draw state\n",
    "        agent_y, agent_x = oneD2twoD(env.state, env.shape)\n",
    "        agent_dot = plt.Circle((agent_x + .5, agent_y + .5), 0.35, fc='b')\n",
    "        fig.gca().add_artist(agent_dot)\n",
    "\n",
    "        # select action\n",
    "        action_choice = random_policy(env)\n",
    "        env.move(action_choice)\n",
    "\n",
    "        print(f\"{start_at}:{action_choice} --> {oneD2twoD(env.state, env.shape)}\")\n",
    "        text = plt.gcf().text(0.1, 0.9, f'Agent Chooses: {action_choice}', fontsize=14)\n",
    "\n",
    "        plt.pause(1)\n",
    "        plt.draw()\n",
    "        dx1, dy1, head_w, head_l = make_arrows(env.action_dict[action_choice])\n",
    "        if action_choice == 'P':\n",
    "            arrow = plt.text(agent_x+0.49, agent_y+0.6, \"*\", {'color': 'white', 'fontsize': 24, 'ha': 'center', 'va': 'center'})\n",
    "        else:\n",
    "            arrow = plt.arrow(agent_x+0.5, agent_y+0.5, dx1, dy1, head_width=0.3, head_length=0.2, color=\"cyan\")\n",
    "\n",
    "\n",
    "        plt.pause(.5)\n",
    "        plt.draw()\n",
    "\n",
    "        if _ < num_steps-1:\n",
    "            agent_dot.remove()\n",
    "            arrow.remove()\n",
    "            text.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
